{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7560cd6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Iterable, List, Callable\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import Dataset, Features, ClassLabel, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, Trainer, TrainingArguments\n",
    ")\n",
    "from inspect import signature\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import defaultdict\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728dbd3",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a011d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "_EMAIL_RE = re.compile(r'\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b')\n",
    "_HTML_RE = re.compile(r'<[^>]+>')  \n",
    "_NEWPAGE_RE = re.compile(r'<\\s*NEWPAGE\\s*>', flags=re.IGNORECASE)\n",
    "_CTRL_RE = re.compile(r'[\\u0000-\\u001F\\u007F]')\n",
    "_MULTISPACE_RE = re.compile(r'\\s+')\n",
    "_DANGLING_PUNCT_RE = re.compile(r'\\s+([.,;:!?])')\n",
    "_WEIRD_CHARS_RE = re.compile(r'[^\\w\\s\\-\\’\\'.,;:!?()“”\"…–—°€£$%&/@]+', flags=re.UNICODE)\n",
    "\n",
    "def normalize_unicode(text: str, form: str = \"NFC\") -> str:\n",
    "    return unicodedata.normalize(form, text)\n",
    "\n",
    "def strip_control_chars(text: str) -> str:\n",
    "    return _CTRL_RE.sub(' ', text)\n",
    "\n",
    "def replace_smart_quotes(text: str) -> str:\n",
    "    replacements = {\n",
    "        \"“\":\"\\\"\", \"”\":\"\\\"\", \"‘\":\"'\", \"’\":\"'\",\n",
    "        \"–\":\"-\", \"—\":\"-\", \"…\":\"...\", \"•\":\"*\",\n",
    "        \"¬\":\"-\", \"ﬁ\":\"fi\", \"ﬂ\":\"fl\", \"½\":\"1/2\", \"¼\":\"1/4\", \"¾\":\"3/4\"\n",
    "    }\n",
    "    for k,v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def dehyphenate_linebreaks(text: str) -> str:\n",
    "    return re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "def basic_whitespace(text: str) -> str:\n",
    "    text = _DANGLING_PUNCT_RE.sub(r'\\1', text)       \n",
    "    text = _MULTISPACE_RE.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_urls_emails(text: str) -> str:\n",
    "    text = _URL_RE.sub(' ', text)\n",
    "    text = _EMAIL_RE.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def strip_html_like(text: str) -> str:\n",
    "    return _HTML_RE.sub(' ', text)\n",
    "\n",
    "def remove_garbage(text: str, keep_charset:str=\"broad\") -> str:\n",
    "    \"\"\"\n",
    "    keep_charset:\n",
    "      - \"broad\": keep letters, digits, underscores, whitespace, modest punctuation\n",
    "      - \"letters_only\": keep letters (incl. accented) + spaces + basic punctuation\n",
    "    \"\"\"\n",
    "    if keep_charset == \"broad\":\n",
    "        return _WEIRD_CHARS_RE.sub(' ', text)\n",
    "    else:\n",
    "        return re.sub(r\"[^A-Za-zÀ-ÖØ-öø-ÿ\\s'.,;:!?-]\", \" \", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8831cd5",
   "metadata": {},
   "source": [
    "### Light Cleaning for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9297248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_roberta(text: str) -> str:\n",
    "    x = text\n",
    "    x = normalize_unicode(x, \"NFC\")\n",
    "    x = replace_smart_quotes(x)\n",
    "    x = dehyphenate_linebreaks(x)\n",
    "    x = _NEWPAGE_RE.sub(' [NEWPAGE] ', x)          \n",
    "    x = strip_html_like(x)\n",
    "    x = remove_urls_emails(x)\n",
    "    x = strip_control_chars(x)\n",
    "    x = remove_garbage(x, keep_charset=\"broad\")\n",
    "    x = basic_whitespace(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12a4c5",
   "metadata": {},
   "source": [
    "### Heavy Cleaning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0737ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_svm(text: str, lowercase:bool=True, ascii_fold:bool=True) -> str:\n",
    "    x = text\n",
    "    x = normalize_unicode(x, \"NFKC\")\n",
    "    x = replace_smart_quotes(x)\n",
    "    x = dehyphenate_linebreaks(x)\n",
    "    x = _NEWPAGE_RE.sub(' ', x)                       \n",
    "    x = strip_html_like(x)\n",
    "    x = remove_urls_emails(x)\n",
    "    x = strip_control_chars(x)\n",
    "    x = remove_garbage(x, keep_charset=\"letters_only\")\n",
    "    if lowercase:\n",
    "        x = x.lower()\n",
    "    if ascii_fold:\n",
    "        x = unicodedata.normalize(\"NFKD\", x).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    x = basic_whitespace(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_clean(texts: Iterable[str], fn: Callable[[str], str]) -> List[str]:\n",
    "    return [fn(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54115f84",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be12550",
   "metadata": {},
   "source": [
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_df = pd.read_csv(r\"..\\Data\\Art_exhibitions_gale.csv\")\n",
    "books_df = pd.read_csv(r\"..\\Data\\Books_gale.csv\")\n",
    "concerts_df = pd.read_csv(r\"..\\Data\\Concerts_gale.csv\")\n",
    "dance_df = pd.read_csv(r\"..\\Data\\Dance_gale.csv\")\n",
    "operas_df = pd.read_csv(r\"..\\Data\\Operas_gale.csv\")\n",
    "poetry_df = pd.read_csv(r\"..\\Data\\Poetry_gale.csv\")\n",
    "theater_df = pd.read_csv(r\"..\\Data\\Theater_gale.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080bcca",
   "metadata": {},
   "source": [
    "### Merge All Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e52244",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {\n",
    "    \"opera\":   operas_df,\n",
    "    \"theater\": theater_df,\n",
    "    \"book\":    books_df,\n",
    "    \"art\":     art_df,\n",
    "    \"concert\": concerts_df,\n",
    "    \"dance\":   dance_df,\n",
    "    \"poetry\":  poetry_df,\n",
    "}\n",
    "\n",
    "frames = []\n",
    "for label, df in sources.items():\n",
    "    assert \"Full_text\" in df.columns, f\"{label} is missing 'Full_text'\"\n",
    "    tmp = df[[\"Full_text\"]].copy()\n",
    "    tmp[\"label\"] = label\n",
    "    frames.append(tmp)\n",
    "\n",
    "big_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "big_df = big_df.dropna(subset=[\"Full_text\"]).drop_duplicates(subset=[\"Full_text\", \"label\"]).reset_index(drop=True)\n",
    "print(\"Class balance:\\n\", big_df[\"label\"].value_counts())\n",
    "\n",
    "big_df[\"text_roberta\"] = batch_clean(big_df[\"Full_text\"], clean_for_roberta)\n",
    "big_df[\"text_svm\"]     = batch_clean(big_df[\"Full_text\"], clean_for_svm)\n",
    "\n",
    "big_df.to_parquet(\"big_cleaned.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f41b7",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert {\"Full_text\",\"label\"}.issubset(big_df.columns)\n",
    "big_df = big_df.dropna(subset=[\"Full_text\"]).reset_index(drop=True)\n",
    "\n",
    "big_df[\"text_roberta\"] = batch_clean(big_df[\"Full_text\"], clean_for_roberta)\n",
    "big_df[\"text_svm\"]     = batch_clean(big_df[\"Full_text\"], clean_for_svm)\n",
    "\n",
    "print(big_df[[\"label\",\"Full_text\",\"text_roberta\",\"text_svm\"]].head(2))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    big_df[\"text_svm\"], big_df[\"label\"],\n",
    "    test_size=0.2, random_state=42, stratify=big_df[\"label\"]\n",
    ")\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    (\"feats\", FeatureUnion(transformer_list=[\n",
    "        (\"word_tfidf\", TfidfVectorizer(\n",
    "            ngram_range=(1,2), min_df=2, max_df=0.9,\n",
    "            stop_words=\"english\", sublinear_tf=True\n",
    "        )),\n",
    "        (\"char_tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char\", ngram_range=(3,5),\n",
    "            min_df=2, sublinear_tf=True\n",
    "        ))\n",
    "    ])),\n",
    "    (\"clf\", LinearSVC(C=1.0))\n",
    "])\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "pred = svm_pipeline.predict(X_val)\n",
    "print(classification_report(y_val, pred, digits=3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e22155",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_names = sorted(big_df[\"label\"].unique())\n",
    "# label2id = {l:i for i,l in enumerate(label_names)}\n",
    "# id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# hf_df = big_df[[\"text_roberta\",\"label\"]].rename(columns={\"text_roberta\":\"text\"}).copy()\n",
    "# hf_df[\"labels\"] = hf_df[\"label\"].map(label2id)\n",
    "\n",
    "# features = Features({\"text\": Value(\"string\"), \"labels\": ClassLabel(names=label_names)})\n",
    "# ds = Dataset.from_pandas(hf_df[[\"text\",\"labels\"]], preserve_index=False, features=features)\n",
    "# ds = ds.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\n",
    "\n",
    "# MODEL = \"roberta-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# def tok(batch):\n",
    "#     return tokenizer(batch[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "# tokenized = ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "# collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL, num_labels=len(label_names), id2label=id2label, label2id=label2id\n",
    "# )\n",
    "\n",
    "# init_params = signature(TrainingArguments.__init__).parameters\n",
    "# has = lambda k: k in init_params\n",
    "\n",
    "# STRATEGY = \"steps\"  \n",
    "# args_kwargs = dict(\n",
    "#     output_dir=\"roberta_out\",\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=3,\n",
    "# )\n",
    "\n",
    "# if has(\"per_device_train_batch_size\"): args_kwargs[\"per_device_train_batch_size\"] = 16\n",
    "# if has(\"per_device_eval_batch_size\"):  args_kwargs[\"per_device_eval_batch_size\"]  = 32\n",
    "\n",
    "# if has(\"logging_steps\"): args_kwargs[\"logging_steps\"] = 50\n",
    "\n",
    "# if STRATEGY == \"steps\":\n",
    "#     if has(\"eval_steps\"): args_kwargs[\"eval_steps\"] = 500\n",
    "#     if has(\"save_steps\"): args_kwargs[\"save_steps\"] = 1000  \n",
    "#     if has(\"save_strategy\"): args_kwargs[\"save_strategy\"] = \"steps\"\n",
    "#     if has(\"evaluation_strategy\"): args_kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "#     elif has(\"eval_strategy\"): args_kwargs[\"eval_strategy\"] = \"steps\"\n",
    "#     elif has(\"evaluate_during_training\"): args_kwargs[\"evaluate_during_training\"] = True\n",
    "# else:  \n",
    "#     if has(\"save_strategy\"): args_kwargs[\"save_strategy\"] = \"epoch\"\n",
    "#     if has(\"evaluation_strategy\"): args_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "#     elif has(\"eval_strategy\"): args_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "#     elif has(\"evaluate_during_training\"): args_kwargs[\"evaluate_during_training\"] = True\n",
    "\n",
    "# enable_load_best = has(\"load_best_model_at_end\") and (\n",
    "#     has(\"evaluation_strategy\") or has(\"eval_strategy\") or has(\"evaluate_during_training\")\n",
    "# )\n",
    "# if enable_load_best:\n",
    "#     args_kwargs[\"load_best_model_at_end\"] = True\n",
    "#     if has(\"metric_for_best_model\"): args_kwargs[\"metric_for_best_model\"] = \"eval_loss\"\n",
    "#     if has(\"save_total_limit\"): args_kwargs[\"save_total_limit\"] = 2\n",
    "\n",
    "# args = TrainingArguments(**args_kwargs)\n",
    "# print(\"Resolved TrainingArguments:\", args)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=tokenized[\"train\"],\n",
    "#     eval_dataset=tokenized[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=collator\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# try:\n",
    "#     eval_res = trainer.evaluate()\n",
    "#     print(\"Eval:\", eval_res)\n",
    "# except TypeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6278d2c",
   "metadata": {},
   "source": [
    "## SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd18bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_margin(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    return svm_pipeline.decision_function(cleaned)  \n",
    "masker = shap.maskers.Text()\n",
    "\n",
    "text_explainer = shap.Explainer(model_margin, masker)\n",
    "\n",
    "raw_examples = big_df[\"Full_text\"].sample(3, random_state=0).tolist()\n",
    "\n",
    "text_explanations = text_explainer(raw_examples)  \n",
    "\n",
    "margins = model_margin(raw_examples)\n",
    "pred_idx = margins.argmax(axis=1)  \n",
    "\n",
    "for i, raw in enumerate(raw_examples):\n",
    "    cls = pred_idx[i]\n",
    "    shap.plots.text(text_explanations[i, :, cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a457e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "union = svm_pipeline.named_steps[\"feats\"]\n",
    "clf   = svm_pipeline.named_steps[\"clf\"]\n",
    "\n",
    "bg_texts = big_df[\"text_svm\"].sample(200, random_state=42).tolist()\n",
    "X_bg = union.transform(bg_texts)\n",
    "\n",
    "lin_explainer = shap.LinearExplainer(clf, X_bg)\n",
    "\n",
    "val_texts = big_df[\"text_svm\"].sample(200, random_state=0).tolist()\n",
    "X_val = union.transform(val_texts)\n",
    "\n",
    "\n",
    "shap_vals_list = lin_explainer.shap_values(X_val)  \n",
    "expected_vals  = lin_explainer.expected_value     \n",
    "\n",
    "word_names = svm_pipeline.named_steps[\"feats\"].transformer_list[0][1].get_feature_names_out()\n",
    "char_names  = svm_pipeline.named_steps[\"feats\"].transformer_list[1][1].get_feature_names_out()\n",
    "feature_names = np.concatenate([word_names, char_names])\n",
    "\n",
    "n_classes = shap_vals_list[0].shape[1] if shap_vals_list[0].ndim == 2 else len(shap_vals_list)\n",
    "class_labels = np.array(sorted(big_df[\"label\"].unique().tolist())) \n",
    "def top_global_shap(class_i, topk=20):\n",
    "    sv = shap_vals_list[class_i]              \n",
    "    mean_abs = np.abs(sv).mean(axis=0)       \n",
    "    top_idx = np.argsort(-mean_abs)[:topk]\n",
    "    return pd.DataFrame({\n",
    "        \"feature\": feature_names[top_idx],\n",
    "        \"mean(|SHAP|)\": mean_abs[top_idx]\n",
    "    })\n",
    "\n",
    "for i, cls in enumerate(class_labels):\n",
    "    print(f\"\\n=== Top features for class: {cls} ===\")\n",
    "    display(top_global_shap(i, topk=15))\n",
    "\n",
    "i = 0\n",
    "x_row = X_val[i]\n",
    "raw_text = val_texts[i]\n",
    "pred_margin = clf.decision_function(x_row) \n",
    "pred_class = int(np.argmax(pred_margin))\n",
    "\n",
    "row_sv = shap_vals_list[pred_class][i].toarray().ravel() if hasattr(shap_vals_list[pred_class], \"toarray\") \\\n",
    "         else shap_vals_list[pred_class][i]\n",
    "\n",
    "pos_idx = np.argsort(-row_sv)[:15]\n",
    "neg_idx = np.argsort(row_sv)[:15]\n",
    "\n",
    "local_df = pd.DataFrame({\n",
    "    \"feature\": np.r_[feature_names[pos_idx], feature_names[neg_idx]],\n",
    "    \"shap_value\": np.r_[row_sv[pos_idx], row_sv[neg_idx]]\n",
    "})\n",
    "print(f\"\\nRaw text:\\n{raw_text[:800]}...\\n\")\n",
    "print(f\"Predicted class: {class_labels[pred_class]}\")\n",
    "print(\"\\nTop local contributors (+):\")\n",
    "display(local_df.iloc[:15])\n",
    "print(\"\\nTop local contributors (−):\")\n",
    "display(local_df.iloc[15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ef4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import matplotlib as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1\n",
    "dark_style = {\n",
    "    'figure.facecolor': '#122b38',\n",
    "    'axes.facecolor': '#122b38',\n",
    "    'savefig.facecolor':'#122b38',\n",
    "    'axes.grid': True,\n",
    "    'axes.grid.which': 'both',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'grid.color': 'white',\n",
    "    'grid.linewidth': '0.3',\n",
    "    'text.color': '0.9',\n",
    "    'axes.labelcolor': '0.9',\n",
    "    'xtick.color': '0.9',\n",
    "    'ytick.color': '0.9',\n",
    "    'font.size': 12 }\n",
    "plt.rcParams.update(dark_style)\n",
    "rcParams['figure.figsize'] = (18, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"theater\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'theater'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6880a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"book\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'book'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"concert\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'concert'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"dance\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'dance'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"opera\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'opera'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"poetry\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'poetry'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls = np.where(class_labels == \"art\")[0][0]\n",
    "shap_vals = shap_vals_list[target_cls]\n",
    "\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[-10:] \n",
    "\n",
    "X_val_top = X_val[:, top_idx]\n",
    "shap_vals_top = shap_vals[:, top_idx]\n",
    "feature_names_top = np.array(feature_names)[top_idx]\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_top,\n",
    "    features=X_val_top,\n",
    "    feature_names=feature_names_top,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", color=\"white\", fontsize=12)\n",
    "ax.set_ylabel(\"\", color=\"white\")\n",
    "ax.set_title(\"Top 5 SHAP Features for 'art'\", color=\"white\", fontsize=14)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "for coll in ax.collections:\n",
    "    coll.set_facecolor(\"lightblue\")\n",
    "    coll.set_edgecolor(\"lightblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c4b8d",
   "metadata": {},
   "source": [
    "### LIME3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_proba(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    margins = svm_pipeline.decision_function(cleaned)  \n",
    "    return softmax(margins, axis=1)\n",
    "\n",
    "lime_explainer = LimeTextExplainer(\n",
    "    class_names=class_labels,\n",
    "    split_expression=r\"\\W+\",\n",
    "    bow=True\n",
    ")\n",
    "\n",
    "target_class_name = \"theater\"\n",
    "target_class_idx = np.where(class_labels == target_class_name)[0][0]\n",
    "\n",
    "word_importance = defaultdict(list)\n",
    "\n",
    "n_samples_to_explain = 50\n",
    "num_features = 20\n",
    "\n",
    "for text in val_texts[:n_samples_to_explain]:\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text_instance=text,\n",
    "        classifier_fn=lime_predict_proba,\n",
    "        labels=[target_class_idx],\n",
    "        num_features=num_features\n",
    "    )\n",
    "\n",
    "    for word, weight in explanation.as_list(label=target_class_idx):\n",
    "        word_importance[word].append(weight)\n",
    "\n",
    "lime_word_scores = {\n",
    "    word: np.mean(np.abs(weights))\n",
    "    for word, weights in word_importance.items()\n",
    "}\n",
    "\n",
    "top_k = 10\n",
    "top_words = sorted(\n",
    "    lime_word_scores.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} LIME words for class '{target_class_name}':\\n\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfa472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_proba(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    margins = svm_pipeline.decision_function(cleaned)  \n",
    "    return softmax(margins, axis=1)\n",
    "\n",
    "lime_explainer = LimeTextExplainer(\n",
    "    class_names=class_labels,\n",
    "    split_expression=r\"\\W+\",\n",
    "    bow=True\n",
    ")\n",
    "\n",
    "target_class_name = \"art\"\n",
    "target_class_idx = np.where(class_labels == target_class_name)[0][0]\n",
    "\n",
    "word_importance = defaultdict(list)\n",
    "\n",
    "n_samples_to_explain = 50\n",
    "num_features = 20\n",
    "\n",
    "for text in val_texts[:n_samples_to_explain]:\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text_instance=text,\n",
    "        classifier_fn=lime_predict_proba,\n",
    "        labels=[target_class_idx],\n",
    "        num_features=num_features\n",
    "    )\n",
    "\n",
    "    for word, weight in explanation.as_list(label=target_class_idx):\n",
    "        word_importance[word].append(weight)\n",
    "\n",
    "lime_word_scores = {\n",
    "    word: np.mean(np.abs(weights))\n",
    "    for word, weights in word_importance.items()\n",
    "}\n",
    "\n",
    "top_k = 10\n",
    "top_words = sorted(\n",
    "    lime_word_scores.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} LIME words for class '{target_class_name}':\\n\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c08446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_proba(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    margins = svm_pipeline.decision_function(cleaned)  \n",
    "    return softmax(margins, axis=1)\n",
    "\n",
    "lime_explainer = LimeTextExplainer(\n",
    "    class_names=class_labels,\n",
    "    split_expression=r\"\\W+\",\n",
    "    bow=True\n",
    ")\n",
    "\n",
    "target_class_name = \"concert\"\n",
    "target_class_idx = np.where(class_labels == target_class_name)[0][0]\n",
    "\n",
    "word_importance = defaultdict(list)\n",
    "\n",
    "n_samples_to_explain = 50\n",
    "num_features = 20\n",
    "\n",
    "for text in val_texts[:n_samples_to_explain]:\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text_instance=text,\n",
    "        classifier_fn=lime_predict_proba,\n",
    "        labels=[target_class_idx],\n",
    "        num_features=num_features\n",
    "    )\n",
    "\n",
    "    for word, weight in explanation.as_list(label=target_class_idx):\n",
    "        word_importance[word].append(weight)\n",
    "\n",
    "lime_word_scores = {\n",
    "    word: np.mean(np.abs(weights))\n",
    "    for word, weights in word_importance.items()\n",
    "}\n",
    "\n",
    "top_k = 10\n",
    "top_words = sorted(\n",
    "    lime_word_scores.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} LIME words for class '{target_class_name}':\\n\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word:<15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_proba(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    margins = svm_pipeline.decision_function(cleaned)  \n",
    "    return softmax(margins, axis=1)\n",
    "\n",
    "lime_explainer = LimeTextExplainer(\n",
    "    class_names=class_labels,\n",
    "    split_expression=r\"\\W+\",\n",
    "    bow=True\n",
    ")\n",
    "\n",
    "target_class_name = \"dance\"\n",
    "target_class_idx = np.where(class_labels == target_class_name)[0][0]\n",
    "\n",
    "word_importance = defaultdict(list)\n",
    "\n",
    "n_samples_to_explain = 50\n",
    "num_features = 20\n",
    "\n",
    "for text in val_texts[:n_samples_to_explain]:\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text_instance=text,\n",
    "        classifier_fn=lime_predict_proba,\n",
    "        labels=[target_class_idx],\n",
    "        num_features=num_features\n",
    "    )\n",
    "\n",
    "    for word, weight in explanation.as_list(label=target_class_idx):\n",
    "        word_importance[word].append(weight)\n",
    "\n",
    "lime_word_scores = {\n",
    "    word: np.mean(np.abs(weights))\n",
    "    for word, weights in word_importance.items()\n",
    "}\n",
    "\n",
    "top_k = 10\n",
    "top_words = sorted(\n",
    "    lime_word_scores.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} LIME words for class '{target_class_name}':\\n\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_proba(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    margins = svm_pipeline.decision_function(cleaned)  \n",
    "    return softmax(margins, axis=1)\n",
    "\n",
    "lime_explainer = LimeTextExplainer(\n",
    "    class_names=class_labels,\n",
    "    split_expression=r\"\\W+\",\n",
    "    bow=True\n",
    ")\n",
    "\n",
    "target_class_name = \"opera\"\n",
    "target_class_idx = np.where(class_labels == target_class_name)[0][0]\n",
    "\n",
    "word_importance = defaultdict(list)\n",
    "\n",
    "n_samples_to_explain = 50\n",
    "num_features = 20\n",
    "\n",
    "for text in val_texts[:n_samples_to_explain]:\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text_instance=text,\n",
    "        classifier_fn=lime_predict_proba,\n",
    "        labels=[target_class_idx],\n",
    "        num_features=num_features\n",
    "    )\n",
    "\n",
    "    for word, weight in explanation.as_list(label=target_class_idx):\n",
    "        word_importance[word].append(weight)\n",
    "\n",
    "lime_word_scores = {\n",
    "    word: np.mean(np.abs(weights))\n",
    "    for word, weights in word_importance.items()\n",
    "}\n",
    "\n",
    "top_k = 10\n",
    "top_words = sorted(\n",
    "    lime_word_scores.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} LIME words for class '{target_class_name}':\\n\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_proba(raw_texts):\n",
    "    cleaned = [clean_for_svm(t) for t in raw_texts]\n",
    "    margins = svm_pipeline.decision_function(cleaned)  \n",
    "    return softmax(margins, axis=1)\n",
    "\n",
    "lime_explainer = LimeTextExplainer(\n",
    "    class_names=class_labels,\n",
    "    split_expression=r\"\\W+\",\n",
    "    bow=True\n",
    ")\n",
    "\n",
    "target_class_name = \"poetry\"\n",
    "target_class_idx = np.where(class_labels == target_class_name)[0][0]\n",
    "\n",
    "word_importance = defaultdict(list)\n",
    "\n",
    "n_samples_to_explain = 50\n",
    "num_features = 20\n",
    "\n",
    "for text in val_texts[:n_samples_to_explain]:\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        text_instance=text,\n",
    "        classifier_fn=lime_predict_proba,\n",
    "        labels=[target_class_idx],\n",
    "        num_features=num_features\n",
    "    )\n",
    "\n",
    "    for word, weight in explanation.as_list(label=target_class_idx):\n",
    "        word_importance[word].append(weight)\n",
    "\n",
    "lime_word_scores = {\n",
    "    word: np.mean(np.abs(weights))\n",
    "    for word, weights in word_importance.items()\n",
    "}\n",
    "\n",
    "top_k = 10\n",
    "top_words = sorted(\n",
    "    lime_word_scores.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} LIME words for class '{target_class_name}':\\n\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
