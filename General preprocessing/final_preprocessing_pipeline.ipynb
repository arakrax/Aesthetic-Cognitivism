{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac6f2b2",
   "metadata": {},
   "source": [
    "# OCR Post-Processing & Text Restoration Pipeline\n",
    "\n",
    "This documentation provides a technical overview of the `final_preprocessing_pipeline.ipynb`, which is designed to clean, standardise, and restore historical text data degraded by Optical Character Recognition (OCR) errors. The pipeline focuses on art reviews and employs a multi-stage approach combining regex heuristics, Named Entity Recognition (NER), and symmetric spelling correction.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment & Dependencies\n",
    "The script relies on several key libraries for NLP and data processing:\n",
    "* **Data Handling**: `pandas`, `datasets` (Hugging Face), and `pathlib`.\n",
    "* **NLP Models**: `transformers` (utilising the `dslim/bert-large-NER` model), `torch`, and `symspellpy`.\n",
    "* **Visualisation**: `matplotlib` for performance tracking.\n",
    "* **Utilities**: `re` for pattern matching and `pickle` for wordlist management.\n",
    "\n",
    "## 2. Pipeline Architecture\n",
    "The `pipeline_driver()` function orchestrates three primary stages of processing:\n",
    "\n",
    "| Stage | Function | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Regex Cleaning** | `clean_text_piece_batched` | Corrects systematic OCR misreads and removes metadata or \"junk\" characters. |\n",
    "| **2. NER & Spelling** | `capitalization_and_correction_batched` | Protects proper nouns and applies length-based spellchecking. |\n",
    "| **3. Polishing** | `final_polishing_batched` | Standardises honorifics and fixes punctuation/currency spacing. |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Detailed Component Breakdown\n",
    "\n",
    "### A. Heuristic OCR Patching\n",
    "Before statistical correction, `apply_common_ocr_patches` and `apply_aggressive_ocr_patches` target high-frequency failures:\n",
    "* **Common Substitutions**: e.g., `alvays` $\\rightarrow$ `always`, `thle` $\\rightarrow$ `the`, and `vith` $\\rightarrow$ `with`.\n",
    "* **Systematic Fixes**: Resolves character confusion such as `rn` being read as `m`, or `wv/vw` being read as `w`.\n",
    "These patterns have been identified by inspecting an non-dictionary words in the full uncorrected corpus.\n",
    "\n",
    "### B. NER Shielding\n",
    "To prevent the spellchecker from erroneously altering rare surnames or historical locations, the pipeline implements a \"shielding\" mechanism:\n",
    "1. **Extraction**: The BERT NER model identifies entities (PER, ORG, LOC).\n",
    "2. **Merging**: `merge_entities_generic` joins fragmented tokens (e.g., \"B.\", \"B.\", \"C.\") into unified entities.\n",
    "3. **Protection**: Words within these entity boundaries are checked against a specialized `ENTITY_WORDLIST` or ignored by the general spellchecker.\n",
    "\n",
    "### C. Staircase Edit Distance\n",
    "The `get_staircase_edit_distance` function assigns an edit budget based on word length to minimize over-correction:\n",
    "* For word length $L < 4$: 1 edit allowed.\n",
    "* For $4 \\le L < 6$: 2 edits allowed.\n",
    "* For $6 \\le L < 10$: 3 edits allowed.\n",
    "* For $10 \\le L < 15$: 4 edits allowed.\n",
    "* For $L \\ge 15$: 6 edits allowed.\n",
    "\n",
    "### D. Digit-to-Letter Correction\n",
    "The `find_best_digit_correction` function addresses cases where numbers were substituted for letters (e.g., \"M0re\" for \"More\"). It maps digits to visually similar letters (e.g., $0 \\rightarrow o, d, q$) and checks the resulting candidates against the frequency dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Standardisation & Polishing\n",
    "The final stage ensures consistency, specifically following British English conventions:\n",
    "* **Honorifics**: `standardize_honorifics` manages titles like Mr, Mrs, and Dr, with an option for the modern British \"no-dot\" style.\n",
    "* **Spacing**: `reapply_punctuation_spacing` corrects errors like \"word ' s\" $\\rightarrow$ \"word's\" and standardises currency formatting (e.g., \"£ 100\" $\\rightarrow$ \"£100\").\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Evaluation & Metrics\n",
    "The `evaluate_pipeline_oov` function assesses the pipeline's effectiveness:\n",
    "* **OOV Reduction**: Tracks the percentage of Out-Of-Vocabulary words before and after processing.\n",
    "* **Historical Grouping**: Groups results into 50-year periods to analyze performance across different eras of print quality.\n",
    "* **Visualisation**: Generates dual-axis plots comparing document volume with mean improvement percentages.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Execution Example\n",
    "To run the pipeline in a Jupyter Notebook cell:\n",
    "\n",
    "```python\n",
    "# 1. Run the main processing driver\n",
    "wordlist, output_folder = pipeline_driver()\n",
    "\n",
    "# 2. Evaluate performance on the output\n",
    "stats, eval_path = evaluate_pipeline_oov(wordlist, output_folder)\n",
    "\n",
    "# 3. Generate summary plots\n",
    "plot_merged_performance(stats, eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023ad1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import product\n",
    "import pickle\n",
    "import shutil\n",
    "import textwrap\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccae922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories and file paths\n",
    "DICTIONARY_DATA_DIR = './Dictionary_data/'\n",
    "\n",
    "ART_WORDLIST_NAME = 'artist_wordlist.pkl' \n",
    "NER_WORDLIST_NAME = 'ner_wordlist.pkl'\n",
    "\n",
    "GUTENBERG_WORDS_PATH = 'project_gutenberg_word_count.txt'\n",
    "GUTENBERG_BIGRAMS_PATH = 'project_gutenberg_bigrams.txt'\n",
    "\n",
    "INPUT_PATH = './Data'\n",
    "OUTPUT_PATH = './Cleaned_data/' + os.path.basename(os.path.normpath(INPUT_PATH)) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d438c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading NER model: dslim/bert-large-NER...\n",
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "INFO:__main__:NER model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setup basic logging to see warnings\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Regex to match words (including hyphenated and apostrophized words)\n",
    "WORD_REGEX = re.compile(r\"\\b[a-zA-Z'0-9-]+\\b\")\n",
    "\n",
    "# Check for CUDA and set the device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# --- Load NER Model and Tokenizer ---\n",
    "model_name_ner = \"dslim/bert-large-NER\"\n",
    "try:\n",
    "    log.info(f\"Loading NER model: {model_name_ner}...\")\n",
    "    # Using aggregation_strategy=\"simple\" helps merge B-TAG I-TAG sequences\n",
    "    ner_pipeline = pipeline(\"ner\", model=model_name_ner, tokenizer=model_name_ner, aggregation_strategy=\"simple\", device=device)\n",
    "    log.info(\"NER model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    log.error(f\"Failed to load NER model: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc57eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'art' words loaded: 33188\n",
      "Total 'ner' words loaded: 14874\n",
      "The 'ENTITY' wordlist consist of 45111 words.\n"
     ]
    }
   ],
   "source": [
    "# Reading wordlists from files and creating master\n",
    "with open(DICTIONARY_DATA_DIR + ART_WORDLIST_NAME, 'rb') as file:\n",
    "    ART_WORDLIST = pickle.load(file)\n",
    "print(\"Total 'art' words loaded:\", len(ART_WORDLIST))\n",
    "\n",
    "with open(DICTIONARY_DATA_DIR + NER_WORDLIST_NAME, 'rb') as file:\n",
    "    NER_WORDLIST = pickle.load(file)\n",
    "print(\"Total 'ner' words loaded:\", len(NER_WORDLIST))\n",
    "\n",
    "ENTITY_WORDLIST = ART_WORDLIST.union(NER_WORDLIST)\n",
    "print(f\"The 'ENTITY' wordlist consist of {len(ENTITY_WORDLIST)} words.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3385cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CSV file into a HuggingFace dataset\n",
    "def load_csv_file(file_path):\n",
    "    try:\n",
    "        ds = load_dataset('csv', data_files=file_path)\n",
    "        file = Path(file_path)\n",
    "\n",
    "        return {'name': file.name, \n",
    "                'data': ds,\n",
    "                'length': ds.get('train').num_rows # ds is a dict like dataset while train is an arrow dataset\n",
    "                }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Caught error when loading csv file from path: {file_path} \\n\\n {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d703a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spellchecker(master_words=True, artist_set=True, global_max_edits=6):\n",
    "\n",
    "    log.info(f\"Setting up SymSpellPy...\")\n",
    "    symspell = SymSpell(max_dictionary_edit_distance=global_max_edits, prefix_length=7)\n",
    "    master_words_added = set()\n",
    "    art_words_added = set()\n",
    "\n",
    "    # Load Project Gutenberg dictionary\n",
    "    if master_words:\n",
    "        symspell.load_dictionary(DICTIONARY_DATA_DIR + GUTENBERG_WORDS_PATH, term_index=0, count_index=1, separator='\\t')\n",
    "        symspell.load_bigram_dictionary(DICTIONARY_DATA_DIR + GUTENBERG_BIGRAMS_PATH, term_index=0, count_index=1, separator='\\t') \n",
    "        master_words_added = set(symspell.words.keys())\n",
    "\n",
    "    # Add artist and NER words with high frequency\n",
    "    if artist_set: \n",
    "        try:\n",
    "            # Use the max frequency from the existing dictionary to set a high frequency for art words\n",
    "            # this ensures they are prioritised in suggestions\n",
    "            art_word_count = (max(symspell.words.values())*10) or 1000000000 \n",
    "        except ValueError:\n",
    "            art_word_count = 1000000000\n",
    "        for word in ENTITY_WORDLIST:\n",
    "            # Replace the current frequency with a higher to\n",
    "            # emphasise the genre importance\n",
    "            symspell.create_dictionary_entry(word, art_word_count)\n",
    "        art_words_added = ENTITY_WORDLIST\n",
    "            \n",
    "    log.info(f\"Added {len(art_words_added)} words from ENTITY wordlist to SymSpell.\")    \n",
    "    log.info(f\"Added {len(master_words_added)} words from Project Gutenberg library to SymSpell.\")\n",
    "    log.info(f\"SymSpell setup complete. Total unique terms: {symspell.word_count}\")\n",
    "    return symspell, master_words_added, art_words_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2d5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_common_ocr_patches(text):\n",
    "    # Fix common title misreads\n",
    "    text = re.sub(r'\\bMar\\.\\s', 'Mr. ', text)\n",
    "    text = re.sub(r'\\bAlr\\.\\s', 'Mr. ', text) \n",
    "    \n",
    "    # Common miss-spellings based on frequence analysis of \n",
    "    # unidentified words from the whole dataset.\n",
    "    text = re.sub(r'\\balvays\\b', 'always', text)\n",
    "\n",
    "    text = re.sub(r'\\banld\\b', 'and', text)\n",
    "    text = re.sub(r'\\bnnd\\b', 'and', text)\n",
    "    text = re.sub(r'\\bsnd\\b', 'and', text)\n",
    "    text = re.sub(r'\\bahd\\b', 'and', text)\n",
    "    text = re.sub(r'\\banv\\b', 'and', text)\n",
    "\n",
    "    text = re.sub(r'\\bcen\\b', 'been', text)\n",
    "    text = re.sub(r'\\becn\\b', 'been', text)\n",
    "\n",
    "    text = re.sub(r'\\bflrst\\b', 'first', text)\n",
    "    text = re.sub(r'\\blirst\\b', 'first', text)\n",
    "\n",
    "    text = re.sub(r'\\birom\\b', 'from', text)\n",
    "    text = re.sub(r'\\bfiom\\b', 'from', text)\n",
    "    text = re.sub(r'\\bfronm\\b', 'from', text)\n",
    "    text = re.sub(r'\\btrom\\b', 'from', text)\n",
    "\n",
    "    text = re.sub(r'\\bhavc\\b', 'have', text)\n",
    "\n",
    "    text = re.sub(r'\\bhiis\\b', 'his', text)\n",
    "\n",
    "    text = re.sub(r'\\bmladame\\b', 'madame', text)\n",
    "    text = re.sub(r'\\bmiadame\\b', 'madame', text)\n",
    "\n",
    "    text = re.sub(r'\\bmanv\\b', 'many', text)\n",
    "\n",
    "    text = re.sub(r'\\bmav\\b', 'may', text)\n",
    "    text = re.sub(r'\\bmnay\\b', 'may', text)\n",
    "\n",
    "    text = re.sub(r'\\bmnore\\b', 'more', text)\n",
    "    text = re.sub(r'\\bnmore\\b', 'more', text)\n",
    "\n",
    "    text = re.sub(r'\\bmnost\\b', 'most', text)\n",
    "\n",
    "    text = re.sub(r'\\bmedtner\\b', 'mother', text)\n",
    "\n",
    "    text = re.sub(r'\\bmnuch\\b', 'much', text)\n",
    "\n",
    "    text = re.sub(r'\\bmlusic\\b', 'music', text)\n",
    "    text = re.sub(r'\\bmnsic\\b', 'music', text)\n",
    "\n",
    "    text = re.sub(r'\\bonlv\\b', 'only', text)\n",
    "\n",
    "    text = re.sub(r'\\bplav\\b', 'play', text)\n",
    "\n",
    "    text = re.sub(r'\\bplaved\\b', 'played', text)\n",
    "\n",
    "    text = re.sub(r'\\bprogrammc\\b', 'programme', text)\n",
    "\n",
    "    text = re.sub(r'\\bsomc\\b', 'some', text)\n",
    "\n",
    "    text = re.sub(r'\\bstvle\\b', 'style', text)\n",
    "\n",
    "    text = re.sub(r'\\btbat\\b', 'that', text)\n",
    "    text = re.sub(r'\\bthlat\\b', 'that', text)\n",
    "    text = re.sub(r'\\bthiat\\b', 'that', text)\n",
    "    text = re.sub(r'\\btlhat\\b', 'that', text)\n",
    "\n",
    "    text = re.sub(r'\\bthc\\b', 'the', text)\n",
    "    text = re.sub(r'\\btbe\\b', 'the', text)\n",
    "    text = re.sub(r'\\bthle\\b', 'the', text)\n",
    "    text = re.sub(r'\\blhe\\b', 'the', text)\n",
    "    text = re.sub(r'\\bt\\'he\\b', 'the', text)\n",
    "    text = re.sub(r'\\btthe\\b', 'the', text)\n",
    "    text = re.sub(r'\\bthr\\b', 'the', text)\n",
    "    text = re.sub(r'\\bfhe\\b', 'the', text)\n",
    "\n",
    "    text = re.sub(r'\\bteatro\\b', 'theatre', text)\n",
    "    text = re.sub(r'\\btbeatre\\b', 'theatre', text)\n",
    "    text = re.sub(r'\\bthcatre\\b', 'theatre', text)\n",
    "\n",
    "    text = re.sub(r'\\bthemn\\b', 'them', text)\n",
    "    text = re.sub(r'\\bthenm\\b', 'them', text)\n",
    "\n",
    "    text = re.sub(r'\\bthel\\b', 'they', text)\n",
    "\n",
    "    text = re.sub(r'\\bthoso\\b', 'those', text)\n",
    "\n",
    "    text = re.sub(r'\\btimc\\b', 'time', text)\n",
    "\n",
    "    text = re.sub(r'\\btvo\\b', 'two', text)\n",
    "\n",
    "    text = re.sub(r'\\bvcry\\b', 'very', text)\n",
    "\n",
    "    text = re.sub(r'wv', 'w', text)\n",
    "    text = re.sub(r'vw', 'w', text)\n",
    "\n",
    "    text = re.sub(r'\\bwvas\\b', 'was', text)\n",
    "    text = re.sub(r'\\bwias\\b', 'was', text)\n",
    "\n",
    "    text = re.sub(r'\\bwerc\\b', 'were', text)\n",
    "\n",
    "    text = re.sub(r'\\bwlhat\\b', 'what', text)\n",
    "\n",
    "    text = re.sub(r'\\bvhen\\b', 'when', text)\n",
    "    text = re.sub(r'\\bwlhen\\b', 'when', text)\n",
    "    text = re.sub(r'\\bwben\\b', 'when', text)\n",
    "\n",
    "    text = re.sub(r'\\bwvhich\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwbich\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhicb\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhieh\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhichl\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhiclh\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhlich\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhicl\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwhiich\\b', 'which', text)\n",
    "    text = re.sub(r'\\bwrhich\\b', 'which', text)\n",
    "\n",
    "    text = re.sub(r'\\bwlho\\b', 'who', text)\n",
    "    text = re.sub(r'\\bvho\\b', 'who', text)\n",
    "    text = re.sub(r'\\bwbo\\b', 'who', text)\n",
    "    text = re.sub(r'\\bwhlo\\b', 'who', text)\n",
    "    text = re.sub(r'\\bwhio\\b', 'who', text)\n",
    "\n",
    "    text = re.sub(r'\\bwtih\\b', 'with', text)\n",
    "    text = re.sub(r'\\bwvith\\b', 'with', text)\n",
    "    text = re.sub(r'\\bvith\\b', 'with', text)\n",
    "    text = re.sub(r'\\bwitb\\b', 'with', text)\n",
    "    text = re.sub(r'\\bwithl\\b', 'with', text)\n",
    "    text = re.sub(r'\\bwitl\\b', 'with', text)\n",
    "\n",
    "    text = re.sub(r'\\bvould\\b', 'would', text)\n",
    "\n",
    "    text = re.sub(r'\\bvears\\b', 'years', text)\n",
    "\n",
    "    text = re.sub(r'\\bvou\\b', 'you', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d544a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aggressive_ocr_patches(text):\n",
    "    \"\"\"\n",
    "    Applies regex-based fixes for systematic OCR errors (wv->w, rn->m)\n",
    "    and high-frequency specific word errors found in the dataset.\n",
    "    \"\"\"\n",
    "    # --- 1. Systematic 'w' and 'm' Fixes ---\n",
    "    # Fix \"wv\" or \"vw\" anywhere (e.g. \"twvo\", \"wvas\")\n",
    "    text = re.sub(r'wv|vw', 'w', text, flags=re.IGNORECASE)\n",
    "    # Fix \"wl\" at start of word (e.g. \"wlhich\", \"wlho\")\n",
    "    text = re.sub(r'\\bwl', 'w', text, flags=re.IGNORECASE)\n",
    "    # Fix \"w\" being read as \"v\" in specific common words\n",
    "    text = re.sub(r'\\bv(hich|ith|hen|here|ho|hose|ould)\\b', r'w\\1', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Fix \"mn\" or \"nm\" at start (e.g. \"mnore\")\n",
    "    text = re.sub(r'\\b[mn]m', 'm', text, flags=re.IGNORECASE)\n",
    "    # Fix \"rn\" -> \"m\" at start (e.g. \"rnore\")\n",
    "    text = re.sub(r'\\brn(?=[aeiou])', 'm', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # --- 2. 'The' / 'That' variants ---\n",
    "    text = re.sub(r'\\bt[bkn]e\\b', 'the', text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r'\\bt[li]he\\b', 'the', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b[li]he\\b', 'the', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bt[li](at|is|ose)\\b', r'th\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # --- 3. High-Frequency Specific Fixes (Safe List) ---\n",
    "    # Common misreads of \"and\"\n",
    "    text = re.sub(r'\\ba[ni][li]d\\b', 'and', text, flags=re.IGNORECASE) \n",
    "    # Honorifics\n",
    "    text = re.sub(r'\\bml(iss|rs|r)\\b', r'M\\1', text, flags=re.IGNORECASE) # Mliss -> Miss\n",
    "    # Common words\n",
    "    text = re.sub(r'\\bMar\\.\\s', 'Mr. ', text)  # Fixes Mar. \n",
    "    text = re.sub(r'\\bAlr\\.\\s', 'Mr. ', text)  # Fixes Alr. \n",
    "    text = re.sub(r'\\bMlr\\.\\s', 'Mr. ', text)  # Fixes Mlr. \n",
    "    text = re.sub(r'\\b[fl]irst\\b', 'first', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bhiis\\b', 'his', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\balvays\\b', 'always', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bprogrammc\\b', 'programme', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2901982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_piece_batched(batch, text_column=\"Full_text\"):\n",
    "    input_texts = batch[text_column]\n",
    "    cleaned_texts = []\n",
    "\n",
    "    # --- Regex Patterns ---\n",
    "    backslash_re = re.compile(r'\\\\')\n",
    "    newpage_re = re.compile(r'<NEWPAGE>')\n",
    "    junk_char_re = re.compile(r'[^a-zA-Z0-9\\s\\.,;:?!£\\'\"()-]')\n",
    "    double_single_quote_re = re.compile(r\"''\")\n",
    "    dehyphen_eol_re = re.compile(r'([a-zA-Z])-\\s*\\n\\s*([a-zA-Z])')\n",
    "    internal_hyphen_re = re.compile(r'-')\n",
    "    newline_re = re.compile(r'\\n')\n",
    "    \n",
    "    # Currency/Context\n",
    "    currency_l_dot_re = re.compile(r'\\s+l\\.(?=\\s+|\\d)')\n",
    "    num_l00_re = re.compile(r'\\sl00(?=\\s|[,.])')\n",
    "    num_O0_re = re.compile(r'\\sO0(?=\\s|[,.])')\n",
    "\n",
    "    # Spacing\n",
    "    space_before_punct_re = re.compile(r'\\s+([,\\.;:?!])')\n",
    "    space_after_punct_re = re.compile(r'([,\\.;:?!])\\s*')\n",
    "    space_around_brackets_open_re = re.compile(r'\\s*\\(\\s*')\n",
    "    space_around_brackets_close_re = re.compile(r'\\s*\\)\\s*')\n",
    "    multi_space_re = re.compile(r'\\s+')\n",
    "\n",
    "    for text in input_texts:\n",
    "        if not text:\n",
    "            cleaned_texts.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # 1. Metadata & Junk\n",
    "        text = backslash_re.sub('', text)\n",
    "        text = newpage_re.sub('', text)\n",
    "        text = junk_char_re.sub(' ', text)\n",
    "        text = double_single_quote_re.sub('\"', text)\n",
    "\n",
    "        # 2. Aggressive Regex Patches (Systematic Fixes)\n",
    "        text = apply_common_ocr_patches(text)\n",
    "        text = apply_aggressive_ocr_patches(text)\n",
    "\n",
    "        # 3. De-hyphenation\n",
    "        new_text = text\n",
    "        while True:\n",
    "            processed_text = dehyphen_eol_re.sub(r'\\1\\2', new_text)\n",
    "            if processed_text == new_text: break\n",
    "            new_text = processed_text\n",
    "        text = new_text\n",
    "\n",
    "        # Remove internal hyphens (non-EOL noise like 'in-the' -> 'in the')\n",
    "        text = internal_hyphen_re.sub(' ', text)\n",
    "\n",
    "        # 4. Join Lines\n",
    "        text = newline_re.sub(' ', text)\n",
    "\n",
    "        # 5. Currency & Context\n",
    "        text = currency_l_dot_re.sub(' £', text)\n",
    "        text = num_l00_re.sub(' 100', text)\n",
    "        text = num_O0_re.sub(' 00', text)\n",
    "\n",
    "        # 6. Pre-Correction Spacing \n",
    "        text = space_before_punct_re.sub(r'\\1', text)\n",
    "        text = space_after_punct_re.sub(r'\\1 ', text)\n",
    "        text = space_around_brackets_open_re.sub(' (', text)\n",
    "        text = space_around_brackets_close_re.sub(') ', text)\n",
    "        text = multi_space_re.sub(' ', text)\n",
    "\n",
    "        # 8. Final Cleanup\n",
    "        text = multi_space_re.sub(' ', text)\n",
    "        cleaned_texts.append(text.strip())\n",
    "\n",
    "    return {'Regex_cleaned_text': cleaned_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94861466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities_generic(ner_results, text, entity_types_to_merge={'PER', 'ORG', 'LOC'}):\n",
    "    \"\"\"\n",
    "    Merges adjacent, fragmented entities (PER, ORG, LOC by default)\n",
    "    from a NER model's output if separated by common connectors or whitespace.\n",
    "    Handles initials specifically for PER entities.\n",
    "\n",
    "    Args:\n",
    "        ner_results (list): The raw output from the NER pipeline (ideally\n",
    "                            with an aggregation strategy like \"simple\" already applied).\n",
    "        text (str): The original text that was analyzed.\n",
    "        entity_types_to_merge (set): A set of entity group labels to consider for merging\n",
    "                                     (e.g., {'PER', 'ORG', 'LOC'}).\n",
    "\n",
    "    Returns:\n",
    "        list: A new list of entities with fragmented ones merged.\n",
    "    \"\"\"\n",
    "    if not ner_results:\n",
    "        return []\n",
    "\n",
    "    merged_results = []\n",
    "    i = 0\n",
    "    while i < len(ner_results):\n",
    "        current_entity = ner_results[i]\n",
    "\n",
    "        # Check if the current entity is one we want to merge and if there's a next entity\n",
    "        if current_entity['entity_group'] in entity_types_to_merge and i + 1 < len(ner_results):\n",
    "            next_entity = ner_results[i+1]\n",
    "\n",
    "            # --- MERGING LOGIC ---\n",
    "            # Condition 1: Check if the next entity is of the SAME type\n",
    "            if next_entity['entity_group'] == current_entity['entity_group']:\n",
    "                # Get the text between the two entities\n",
    "                start = current_entity['end']\n",
    "                end = next_entity['start']\n",
    "                text_between = text[start:end]\n",
    "                stripped_text_between = text_between.strip()\n",
    "\n",
    "                should_merge = False\n",
    "                # Merge based on simple separators ('and', '&', space/nothing)\n",
    "                if stripped_text_between in ['and', '&', '']:\n",
    "                    should_merge = True\n",
    "\n",
    "                # Merge initials specifically for PER entities\n",
    "                elif current_entity['entity_group'] == 'PER' and \\\n",
    "                     current_entity['word'].endswith('.') and \\\n",
    "                     len(current_entity['word'].strip('.')) <= 1 and \\\n",
    "                     stripped_text_between == '':\n",
    "                    # Ensure space is added between initials\n",
    "                     text_between = \" \"\n",
    "                     should_merge = True\n",
    "\n",
    "                if should_merge:\n",
    "                    # Combine words, average scores, update indices\n",
    "                    # Add space if original separation was just whitespace but not empty\n",
    "                    if stripped_text_between == '' and text_between != '':\n",
    "                        new_word = current_entity['word'] + ' ' + next_entity['word']\n",
    "                    else:\n",
    "                        new_word = current_entity['word'] + text_between + next_entity['word']\n",
    "\n",
    "                    new_score = (current_entity['score'] + next_entity['score']) / 2\n",
    "\n",
    "                    # Create a new merged entity dictionary\n",
    "                    merged_entity = {\n",
    "                        'entity_group': current_entity['entity_group'],\n",
    "                        'word': new_word.replace(\" ##\", \"\"), # Clean up sub-word tokens if necessary\n",
    "                        'score': new_score,\n",
    "                        'start': current_entity['start'],\n",
    "                        'end': next_entity['end']\n",
    "                    }\n",
    "\n",
    "                    # Replace the current entity with the merged one for the next iteration/append step\n",
    "                    current_entity = merged_entity\n",
    "                    # Increment 'i' to skip the next entity that we just merged\n",
    "                    i += 1\n",
    "\n",
    "        # Append the current (potentially merged) entity to the results\n",
    "        merged_results.append(current_entity)\n",
    "        i += 1 # Move to the next entity\n",
    "\n",
    "    # --- Final Filtering ---\n",
    "    # Keep only the desired entity types and apply basic length filtering\n",
    "    # Allows single initials ending with '.' for PER, otherwise length > 1\n",
    "    final_results = [\n",
    "        ent for ent in merged_results\n",
    "        if ent['entity_group'] in entity_types_to_merge and \\\n",
    "           (len(ent['word'].strip()) > 1 or \\\n",
    "            (ent['entity_group'] == 'PER' and ent['word'].strip().endswith('.') and len(ent['word'].strip()) <= 2)\n",
    "           )\n",
    "    ]\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95be452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_capitalization(text, ner_entities):\n",
    "    \"\"\"\n",
    "    Applies sentence case and capitalizes recognized entities,\n",
    "    handling common abbreviations/acronyms as special cases (keeping them uppercase).\n",
    "    \"\"\"\n",
    "    if not text or text.isspace():\n",
    "        return text\n",
    "\n",
    "    # Convert text to list of characters for easier modification\n",
    "    text_chars = list(text.lower())\n",
    "\n",
    "    # --- Apply Sentence Capitalization ---\n",
    "    capitalize_next = True \n",
    "    for i, char in enumerate(text_chars):\n",
    "        if capitalize_next and char.isalpha():\n",
    "            text_chars[i] = char.upper()\n",
    "            capitalize_next = False\n",
    "        elif char in '.?!' and i + 1 < len(text_chars) and text_chars[i+1].isspace():\n",
    "             capitalize_next = True\n",
    "        elif not char.isspace():\n",
    "            capitalize_next = False\n",
    "\n",
    "    # Ensure first letter is capitalized even if text starts with non-alpha\n",
    "    for i, char in enumerate(text_chars):\n",
    "        if char.isalpha():\n",
    "            text_chars[i] = char.upper()\n",
    "            break\n",
    "\n",
    "    # --- Lock Existing Title Case Words ---\n",
    "    # If a word is already title case in the original text,\n",
    "    # preserve its casing if for example it is not\n",
    "    # captred by NER.\n",
    "    for match in WORD_REGEX.finditer(text):\n",
    "        start, end = match.start(), match.end()\n",
    "\n",
    "        if match.group(0).istitle():\n",
    "            text_chars[start:end] = list(match.group(0))\n",
    "\n",
    "    # --- Apply Entity Capitalization ---\n",
    "    # Sort reversed to handle indices safely if modifications were to change length\n",
    "    sorted_entities = sorted(ner_entities, key=lambda x: x['start'], reverse=True)\n",
    "    modified_indices = set()\n",
    "\n",
    "    known_abbreviations = {\n",
    "        'llc', 'ltd', 'inc', 'co', 'plc', 'corp',\n",
    "        'ra', 'mp', 'esq', 't5', 'kbe', 'cbe', 'obe', 'mbe', \n",
    "        'mph', 'kph', 'm', 'km', 'hh', 'kph', 'mph', 'kg', \n",
    "        'lb', 'oz', 'in', 'ft', 'yd', 'am', 'pm', 'cm', \n",
    "        'fps', 'mm', 'ns', 'ms'\n",
    "    }\n",
    "    initials_pattern = re.compile(r'^([A-Z]\\.\\s?)+$')\n",
    "\n",
    "    # Process each entity for capitalization.\n",
    "    # Iterate in reverse order to avoid index shifting issues.\n",
    "    # These overwrite the sentence case applied earlier.\n",
    "    for entity in sorted_entities:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "\n",
    "        # If the entity is followed by 's or ', extend the end boundary \n",
    "        # to ensure the possessive suffix matches the entity's casing.\n",
    "        if end < len(text_chars) and text_chars[end] == \"'\":\n",
    "            if end + 1 < len(text_chars) and text_chars[end+1] == \"s\":\n",
    "                end += 2  # Expand span to include 's\n",
    "            else:\n",
    "                end += 1\n",
    "    \n",
    "        # --- Index Boundary Safety Check ---\n",
    "        if start < 0 or end > len(text_chars) or start >= end:\n",
    "            log.warning(f\"Skipping entity '{entity.get('word')}' due to out-of-bounds indices: [{start}:{end}]\")\n",
    "            continue\n",
    "\n",
    "        entity_group = entity['entity_group']\n",
    "        original_word = entity['word'] \n",
    "\n",
    "        if any(idx in modified_indices for idx in range(start, end)):\n",
    "            continue\n",
    "\n",
    "        current_span_list = text_chars[start:end]\n",
    "        current_span_lower = \"\".join(current_span_list) \n",
    "\n",
    "        # --- Capitalization Rules ---\n",
    "        capitalized_span = \"\"\n",
    "        check_word = current_span_lower.rstrip('.,;:?!')\n",
    "        \n",
    "        if check_word in known_abbreviations:\n",
    "            capitalized_span = current_span_lower.upper()\n",
    "        elif initials_pattern.match(original_word):\n",
    "             capitalized_span = current_span_lower.upper()\n",
    "        elif entity_group in ('ORG', 'LOC') and len(original_word) > 0:\n",
    "             caps_count = sum(1 for char in original_word if char.isupper())\n",
    "             if caps_count / len(original_word) >= 0.4: \n",
    "                capitalized_span = current_span_lower.upper()\n",
    "        \n",
    "        # Default fallback\n",
    "        if not capitalized_span:\n",
    "            capitalized_span = current_span_lower.title()\n",
    "\n",
    "        # --- Force Length Alignment ---\n",
    "        # This prevents the 'Length mismatch' warning by ensuring the replacement\n",
    "        # fits exactly into the original character slice.\n",
    "        expected_len = end - start\n",
    "        if len(capitalized_span) != expected_len:\n",
    "            if len(capitalized_span) > expected_len:\n",
    "                capitalized_span = capitalized_span[:expected_len]\n",
    "            else:\n",
    "                capitalized_span = capitalized_span.ljust(expected_len)\n",
    "\n",
    "        text_chars[start:end] = list(capitalized_span)\n",
    "        modified_indices.update(range(start, end))\n",
    "\n",
    "    return \"\".join(text_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe21dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_garbage(word, alpha_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Identifies tokens likely to be OCR noise.\n",
    "    Returns True if the token should be ignored by the spellchecker.\n",
    "    \"\"\"\n",
    "    # Do not filter short tokens or dates\n",
    "    if len(word) < 4 or word.isdigit():\n",
    "        return False\n",
    "        \n",
    "    letters = sum(c.isalpha() for c in word)\n",
    "    alpha_ratio = letters / len(word)\n",
    "    \n",
    "    # High ratio of symbols/numbers usually indicates a failed OCR read\n",
    "    if alpha_ratio < alpha_threshold:\n",
    "        return True\n",
    "        \n",
    "    # Check for vowel presence in alphabetic tokens longer than 3 chars\n",
    "    if letters > 3:\n",
    "        has_vowel = any(c in 'aåäeëêiïîoöôuüûyÿŷ' for c in word.lower())\n",
    "        if not has_vowel:\n",
    "            return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "942031f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_staircase_edit_distance(word_len):\n",
    "    if word_len < 4:\n",
    "        return 1  \n",
    "    elif word_len < 6:\n",
    "        return 2\n",
    "    elif word_len < 10:\n",
    "        return 3\n",
    "    elif word_len < 15:\n",
    "        return 4\n",
    "    else:\n",
    "        return 6 # Only long words get a budget of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b8ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the most common OCR digit-to-letter confusion map\n",
    "# We keep this conservative to limit the number of combinations generated.\n",
    "OCR_DIGIT_MAP = {\n",
    "    # 0 (Zero): Visually similar to O, D, and Q.\n",
    "    '0': ['o', 'd', 'q'], \n",
    "\n",
    "    # 1 (One): Visually similar to I, L, T, A, and sometimes J.\n",
    "    '1': ['i', 'l', 't', 'a', 'j'],\n",
    "\n",
    "    # 2 (Two): Visually similar to Z, R, E, and sometimes N.\n",
    "    '2': ['z', 'r', 'e', 'n'],\n",
    "\n",
    "    # 3 (Three): Very common substitute for E, A, I, and sometimes B/P (mirrored).\n",
    "    '3': ['e', 'a', 'i', 'b', 'p'],\n",
    "\n",
    "    # 4 (Four): Visually similar to A, H, T, and sometimes Y (inverted).\n",
    "    '4': ['a', 'h', 't', 'y'],\n",
    "\n",
    "    # 5 (Five): Visually similar to S and sometimes I.\n",
    "    '5': ['s', 'i'],\n",
    "\n",
    "    # 6 (Six): Very common substitute for G, B, and sometimes E (inverted 9).\n",
    "    '6': ['g', 'b', 'e'], \n",
    "\n",
    "    # 7 (Seven): Most commonly substituted for T, L, or I.\n",
    "    '7': ['t', 'l', 'i'],\n",
    "\n",
    "    # 8 (Eight): Visually similar to B, O, and sometimes G/S.\n",
    "    '8': ['b', 'o', 'g', 's'],\n",
    "\n",
    "    # 9 (Nine): Most often substituted for F or G.\n",
    "    '9': ['f', 'g']\n",
    "}\n",
    "\n",
    "DIGIT_CHECK_REGEX = re.compile(r'\\d')\n",
    "\n",
    "\n",
    "def generate_candidates(word):\n",
    "    \"\"\"\n",
    "    Generates all possible word candidates by replacing digits with mapped letters.\n",
    "    \"\"\"\n",
    "    # Prepare the word structure: list of possible characters for each position\n",
    "    word_parts = []\n",
    "    for char in word:\n",
    "        # If the character is a mapped digit, use its substitutes; otherwise, use the character itself.\n",
    "        word_parts.append(OCR_DIGIT_MAP.get(char, [char]))\n",
    "            \n",
    "    # Use itertools.product to generate all possible combinations efficiently\n",
    "    # The list comprehension at the end joins the characters back into strings.\n",
    "    candidates = [\"\".join(p) for p in product(*word_parts)]\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def find_best_digit_correction(word, symspell_instance):\n",
    "    \"\"\"\n",
    "    Generates candidates and finds the one with an exact match (distance 0)\n",
    "    and the highest frequency in the SymSpell dictionary.\n",
    "    \"\"\"\n",
    "    candidates = generate_candidates(word.lower())\n",
    "    best_suggestion = None\n",
    "    best_count = -1\n",
    "    \n",
    "    # Allow some flexibility with a max_edit_distance=2 since a lot of the words are misspelled.\n",
    "    for candidate in candidates:\n",
    "        suggestions = symspell_instance.lookup(\n",
    "            candidate, \n",
    "            Verbosity.CLOSEST, # Use CLOSEST to check frequency if distance is 1\n",
    "            max_edit_distance=2, \n",
    "            include_unknown=False # Must exist in the dictionary\n",
    "        )\n",
    "        \n",
    "        if suggestions:\n",
    "            suggestion = suggestions[0]\n",
    "            \n",
    "            # Prioritize the suggestion with the highest frequency count\n",
    "            if suggestion.count > best_count:\n",
    "                best_count = suggestion.count\n",
    "                best_suggestion = suggestion.term\n",
    "                \n",
    "    return best_suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2517ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_case(original_word, new_word):\n",
    "    \"\"\"Matches the case of the new word to the original word.\"\"\"\n",
    "    if original_word.isupper():\n",
    "        return new_word.upper()\n",
    "    if original_word.istitle():\n",
    "        return new_word.title()\n",
    "    if original_word[0].isupper() and (len(original_word) == 1 or original_word[1:].islower()):\n",
    "        return new_word.title()\n",
    "    return new_word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6055c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling_safe(text, ner_entities, ner_symspell=None, symspell=None,\n",
    "                          master_word_set=None, \n",
    "                          segmentation_ratio=6):\n",
    "    \"\"\"\n",
    "    Corrects misspellings using a staircase edit-distance model and NER shielding.\n",
    "    Maintains digit-to-letter checks and possessive logic.\n",
    "    \"\"\"\n",
    "    if ner_symspell is None or symspell is None or master_word_set is None or not text:\n",
    "        return text\n",
    "\n",
    "    entity_indices = set()\n",
    "    for ent in ner_entities:\n",
    "        entity_indices.update(range(ent['start'], ent['end']))\n",
    "\n",
    "    corrected_parts = []\n",
    "    last_index = 0\n",
    "    \n",
    "    for match in WORD_REGEX.finditer(text):\n",
    "        word = match.group(0)\n",
    "        word_lower = word.lower()\n",
    "        match_start = match.start()\n",
    "        match_end = match.end()\n",
    "\n",
    "        # --- ALPHA DENSITY CHECK ---\n",
    "        if is_garbage(word):\n",
    "            corrected_parts.append(text[last_index:match_start])\n",
    "            corrected_parts.append(word)\n",
    "            last_index = match_end\n",
    "            continue\n",
    "        \n",
    "        corrected_word = word \n",
    "        is_known = False\n",
    "        is_possessive = False\n",
    "        \n",
    "        # --- 1. Possessive Check ---\n",
    "        if word_lower.endswith(\"'s\"):\n",
    "            root_word = word_lower[:-2]\n",
    "            is_possessive = True\n",
    "        elif word_lower.endswith(\"'\"):\n",
    "            root_word = word_lower[:-1]\n",
    "            is_possessive = True\n",
    "        \n",
    "        if is_possessive:\n",
    "            if root_word in master_word_set:\n",
    "                is_known = True\n",
    "        else:\n",
    "            if word_lower in master_word_set:\n",
    "                is_known = True\n",
    "\n",
    "        # --- 2. Staircase Budget Assignment ---\n",
    "        # Calculate the budget once per word based on its length.\n",
    "        staircase_budget = get_staircase_edit_distance(len(word))\n",
    "\n",
    "        if not is_known:\n",
    "            # --- 3. High-Priority Digit-to-Letter Correction ---\n",
    "            if DIGIT_CHECK_REGEX.search(word):\n",
    "                digit_corrected = find_best_digit_correction(word, symspell)\n",
    "                if digit_corrected:\n",
    "                    corrected_word = preserve_case(word, digit_corrected)\n",
    "                    # Update word_lower for subsequent steps if digit-corrected\n",
    "                    word_lower = corrected_word.lower()\n",
    "\n",
    "            # --- 4. NER Shielded Correction ---\n",
    "            # If word is still uncorrected and is shielded by NER\n",
    "            is_shielded = any(idx in entity_indices for idx in range(match_start, match_end))\n",
    "            \n",
    "            if corrected_word == word and is_shielded:\n",
    "                # Use a very strict budget for names (max 1 edit) to prevent\n",
    "                # changing one person's name into another's.\n",
    "                suggestions = ner_symspell.lookup(word, Verbosity.CLOSEST, max_edit_distance=1)\n",
    "                if suggestions:\n",
    "                    corrected_word = preserve_case(word, suggestions[0].term)\n",
    "\n",
    "            # --- 5. General SymSpell Correction (Unshielded) ---\n",
    "            if corrected_word == word and not is_shielded and staircase_budget > 0:\n",
    "                # Try standard lookup first using the staircase budget\n",
    "                suggestions = symspell.lookup(\n",
    "                    word, \n",
    "                    Verbosity.CLOSEST, \n",
    "                    max_edit_distance=staircase_budget,\n",
    "                    include_unknown=False\n",
    "                )\n",
    "                \n",
    "                if suggestions:\n",
    "                    corrected_word = preserve_case(word, suggestions[0].term)\n",
    "                \n",
    "                # --- 6. Word Segmentation (Fallback for long tokens) ---\n",
    "                elif len(word) > 10: \n",
    "                    # A length-based ratio is used but capped at the staircase budget\n",
    "                    dynamic_max_edits = min(staircase_budget, (len(word) // segmentation_ratio))\n",
    "                    if dynamic_max_edits < 1: dynamic_max_edits = 1\n",
    "\n",
    "                    try:\n",
    "                        segment_suggestion = symspell.word_segmentation(\n",
    "                            word,\n",
    "                            max_edit_distance=dynamic_max_edits\n",
    "                        )\n",
    "                        if segment_suggestion.corrected_string and \\\n",
    "                           segment_suggestion.corrected_string.lower() != word_lower:\n",
    "                            corrected_word = preserve_case(word, segment_suggestion.corrected_string)\n",
    "                    except Exception as e:\n",
    "                        log.warning(f\"Segmentation failed for '{word}': {e}\")\n",
    "\n",
    "\n",
    "        # --- 7. Reassemble ---\n",
    "        corrected_parts.append(text[last_index:match_start])\n",
    "        corrected_parts.append(corrected_word)\n",
    "        last_index = match_end\n",
    "\n",
    "    corrected_parts.append(text[last_index:])\n",
    "    return \"\".join(corrected_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "235bbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE! This function is not currently a part of the pipeline_driver\n",
    "# if it is used some experimentation is needed to see how much\n",
    "# freedom the symspell instance should have. It easily over-corrects.\n",
    "# it can be integrated into the 'capitalization_and_correction_batched'\n",
    "# as a last step:\n",
    "# final_segmented_text = fix_targeted_segmentation(\n",
    "#    spell_checked_text_v2, \n",
    "#    merged_entities, \n",
    "#    symspell\n",
    "# )\n",
    "#\n",
    "#final_texts.append(final_segmented_text)\n",
    "#\n",
    "#\n",
    "\n",
    "def fix_targeted_segmentation(text, ner_entities, symspell, max_edit_distance=4):\n",
    "    \"\"\"\n",
    "    Finds and corrects *only* specific, targeted segmentation errors\n",
    "    (like \"wa ter\") using regex and SymSpellPy, AND SKIPPING matches\n",
    "    that overlap with NER entities.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to correct (output of 1-to-1 correction).\n",
    "        ner_entities (list): The list of merged/filtered NER entities for this text.\n",
    "        symspell (SymSpell): The pre-configured SymSpellPy instance.\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with targeted segmentation corrections.\n",
    "    \"\"\"\n",
    "    if symspell is None or not text:\n",
    "        return text\n",
    "\n",
    "    # --- Step 1: Create the \"shield\" set of all indices ---\n",
    "    entity_indices = set()\n",
    "    for ent in ner_entities:\n",
    "        entity_indices.update(range(ent['start'], ent['end']))\n",
    "\n",
    "    # Define the patterns (excluding the risky p3)\n",
    "    p1 = r\"\\b([a-zA-Z']{1,2})\\s([a-zA-Z']{3,})\\b\"\n",
    "    p2 = r\"\\b([a-zA-Z']{3,})\\s([a-zA-Z']{1,2})\\b\"\n",
    "    candidate_pattern = re.compile(f\"({p1})|({p2})\", flags=re.IGNORECASE)\n",
    "    \n",
    "    matches = list(candidate_pattern.finditer(text))\n",
    "    corrected_text = text\n",
    "    \n",
    "    for match in reversed(matches):\n",
    "        original_phrase = match.group(0)\n",
    "        start_index = match.start()\n",
    "        end_index = match.end()\n",
    "\n",
    "        # --- Step 2: The Shield Check ---\n",
    "        # Check if *any* part of this match overlaps with the NER shield\n",
    "        is_shielded = False\n",
    "        for i in range(start_index, end_index):\n",
    "            if i in entity_indices:\n",
    "                is_shielded = True\n",
    "                break\n",
    "        \n",
    "        if is_shielded:\n",
    "            continue # This match is part of a NER entity, do not correct.\n",
    "\n",
    "        # --- Step 3: The Decider (SymSpellPy) ---\n",
    "        # (This part is only reached if the phrase is NOT shielded)\n",
    "        try:\n",
    "            suggestions = symspell.lookup_compound(\n",
    "                original_phrase,\n",
    "                max_edit_distance=max_edit_distance,\n",
    "                transfer_casing=True\n",
    "            )\n",
    "            \n",
    "            if suggestions:\n",
    "                best_suggestion = suggestions[0].term\n",
    "                if best_suggestion != original_phrase and \" \" not in best_suggestion:\n",
    "                    corrected_text = (\n",
    "                        corrected_text[:start_index] + \n",
    "                        best_suggestion + \n",
    "                        corrected_text[end_index:]\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during lookup_compound on phrase '{original_phrase}': {e}\")\n",
    "            # Continue without correcting this phrase\n",
    "\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39c44453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalization_and_correction_batched(batch, text_column=\"Regex_cleaned_text\", ner_symspell=None, symspell=None, master_word_set=ENTITY_WORDLIST, run=\"\"):\n",
    "    \"\"\"\n",
    "    Stage 2: NER, Entity Merging, and Capitalization.\n",
    "    \"\"\"\n",
    "    if ner_pipeline is None:\n",
    "        log.error(\"NER pipeline not loaded.\")\n",
    "        return {f\"Corrected_{text_column}\": batch[text_column]}\n",
    "\n",
    "    input_texts = batch[text_column]\n",
    "    final_texts = []\n",
    "    all_merged_entities = []\n",
    "\n",
    "    # Run NER\n",
    "    try:\n",
    "        all_ner_raw_results = ner_pipeline(input_texts)\n",
    "    except Exception as e:\n",
    "        log.error(f\"NER failed: {e}\")\n",
    "        return {f\"Corrected_{text_column}\": input_texts}\n",
    "\n",
    "    for i, text in enumerate(input_texts):\n",
    "        if not text or text.isspace():\n",
    "             final_texts.append(text)\n",
    "             all_merged_entities.append([])\n",
    "             continue\n",
    "\n",
    "        ner_results = all_ner_raw_results[i]\n",
    "\n",
    "        # 1. Merge Entities\n",
    "        merged_entities = merge_entities_generic(ner_results, text)\n",
    "        \n",
    "        # 2. Apply Capitalization\n",
    "        capitalized_text = apply_capitalization(text, merged_entities)\n",
    "        \n",
    "        # 3. Spell check\n",
    "        spell_checked_text_v1 = correct_spelling_safe(capitalized_text,\n",
    "                                                   merged_entities,\n",
    "                                                   ner_symspell,\n",
    "                                                   symspell,\n",
    "                                                   master_word_set)\n",
    "        \n",
    "        # 4. Spell check again to enable correction on previously merged words\n",
    "        spell_checked_text_v2 = correct_spelling_safe(spell_checked_text_v1,\n",
    "                                                   merged_entities,\n",
    "                                                   ner_symspell,\n",
    "                                                   symspell,\n",
    "                                                   master_word_set)\n",
    "        \n",
    "        final_texts.append(spell_checked_text_v2)\n",
    "        all_merged_entities.append(merged_entities)\n",
    "\n",
    "    return {f\"Symspell_corrected_text_v{run}\": final_texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86cfc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_honorifics(text, style='dot'):\n",
    "    \"\"\"\n",
    "    Standardizes common honorifics (Mr, Mrs, Sir, etc.) to a consistent\n",
    "    capitalization and punctuation style, running as a final polish.\n",
    "    \n",
    "    This function should be applied *after* all NER and spelling corrections.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "        style (str): \n",
    "            'dot'   -> Enforces a dot: 'Mr.', 'Mrs.', 'Dr.', 'Esq.'\n",
    "            'no_dot' -> Enforces no dot: 'Mr', 'Mrs', 'Dr', 'Esq' \n",
    "                         (Closer to modern British English style)\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with standardized honorifics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # --- Define Replacements ---\n",
    "    \n",
    "    # These titles are always capitalized but never take a dot.\n",
    "    # These are run first.\n",
    "    no_dot_titles = {\n",
    "        r'\\bsir\\b': 'Sir',\n",
    "        r'\\bdame\\b': 'Dame',\n",
    "        r'\\blord\\b': 'Lord',\n",
    "        r'\\blady\\b': 'Lady',\n",
    "        r'\\bmiss\\b': 'Miss', # Miss is a full word, not an abbreviation\n",
    "    }\n",
    "\n",
    "    # These are abbreviations, and their punctuation depends on the style.\n",
    "    # The regex \\.? matches if a dot is present or not, standardizing both.\n",
    "    if style == 'dot':\n",
    "        style_dependent_titles = {\n",
    "            r'\\bmr\\.?\\b': 'Mr.',\n",
    "            r'\\bmrs\\.?\\b': 'Mrs.',\n",
    "            r'\\bms\\.?\\b': 'Ms.',\n",
    "            r'\\bdr\\.?\\b': 'Dr.',\n",
    "            r'\\brev\\.?\\b': 'Rev.',\n",
    "            r'\\bprof\\.?\\b': 'Prof.',\n",
    "            r'\\bcapt\\.?\\b': 'Capt.',\n",
    "            r'\\bcol\\.?\\b': 'Col.',\n",
    "            r'\\bgen\\.?\\b': 'Gen.',\n",
    "            r'\\besq\\.?\\b': 'Esq.',\n",
    "            r'\\bwm\\.?\\b': 'Wm.', # For William\n",
    "        }\n",
    "    else: # style == 'no_dot' (Modern British)\n",
    "        style_dependent_titles = {\n",
    "            r'\\bmr\\.?\\b': 'Mr',\n",
    "            r'\\bmrs\\.?\\b': 'Mrs',\n",
    "            r'\\bms\\.?\\b': 'Ms',\n",
    "            r'\\bdr\\.?\\b': 'Dr',\n",
    "            r'\\brev\\.?\\b': 'Rev',\n",
    "            r'\\bprof\\.?\\b': 'Prof',\n",
    "            r'\\bcapt\\.?\\b': 'Capt',\n",
    "            r'\\bcol\\.?\\b': 'Col',\n",
    "            r'\\bgen\\.?\\b': 'Gen',\n",
    "            r'\\besq\\.?\\b': 'Esq', # For consistency, the dot is removed\n",
    "            r'\\bwm\\.?\\b': 'Wm', \n",
    "        }\n",
    "\n",
    "    # Apply the replacements, starting with the non-dotted ones.\n",
    "    # re.IGNORECASE is used to catch all variants (e.g., 'mr', 'Mr', 'MR').\n",
    "    \n",
    "    for pattern, replacement in no_dot_titles.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "        \n",
    "    for pattern, replacement in style_dependent_titles.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53c5edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reapply_punctuation_spacing(text):\n",
    "    \"\"\"\n",
    "    Standardises spacing around punctuation, preserves acronyms, \n",
    "    and heals split initials.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # 1. Heal Split Initials: \"B. B. C.\" -> \"B.B.C.\"\n",
    "    # Matches a single letter + dot + space, followed by another single letter + dot\n",
    "    text = re.sub(r'\\b([a-zA-Z])\\.\\s+(?=([a-zA-Z]\\.))', r'\\1.', text)\n",
    "\n",
    "    # 2. Standardise Possessives: \"word ' s\" -> \"word's\"\n",
    "    text = re.sub(r'(\\w)\\s*\\'\\s*s\\b', r\"\\1's\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3. Standardise Plural Possessives: \"artists ' \" -> \"artists' \"\n",
    "    # Ensures a space follows to avoid joining the next word\n",
    "    text = re.sub(r\"(\\w)\\s*'\\s*(?!s\\b)\", r\"\\1' \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. Standardise space BEFORE punctuation (Remove it)\n",
    "    text = re.sub(r'\\s+([,\\.;:?!])', r'\\1', text)\n",
    "\n",
    "    # 5. Standardise space AFTER punctuation (Ensure it)\n",
    "    # 5a. Commas, colons, semicolons, and exclamation/question marks\n",
    "    text = re.sub(r'([,;:?!])([a-zA-Z0-9])', r'\\1 \\2', text)\n",
    "    \n",
    "    # 5b. Initial-aware dots: Adds space only if NOT an initial\n",
    "    # If the dot is preceded by a single letter (an initial), do not add a space.\n",
    "    # If the dot is preceded by a full word, add a space.\n",
    "    text = re.sub(r'(?<!\\b[a-zA-Z])\\.([a-zA-Z0-9])', r'. \\1', text)\n",
    "\n",
    "    # 6. Clean up spaces around brackets\n",
    "    text = re.sub(r'\\s*\\(\\s*', ' (', text)\n",
    "    text = re.sub(r'\\s*\\)\\s*', ') ', text)\n",
    "\n",
    "    # 7. Final cleanup of multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 8. Fix spaces in common currencies\n",
    "    # 8a. Symbol + Number: \"$ 100\", \"£ 40. 2\", \"€ 5\"\n",
    "    text = re.sub(\n",
    "        r'([$£€])\\s*(\\d+(?:,\\d+)*)(?:\\s*[\\.]\\s*(\\d+))?',\n",
    "        lambda m: f\"{m.group(1)}{m.group(2)}{'.' + m.group(3) if m.group(3) else ''}\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # 8b. Number + Symbol: \"100 $\", \"2,500 €\", \"5 £\"\n",
    "    text = re.sub(\n",
    "        r'(\\d+(?:,\\d+)*)(?:\\s*[\\.]\\s*(\\d+))?\\s*([$£€])',\n",
    "        lambda m: f\"{m.group(1)}{'.' + m.group(2) if m.group(2) else ''}{m.group(3)}\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # Fix spaces in times\n",
    "    # 9a. Heal fragmented times: \"10 . 30\" or \"10 : 30\" -> \"10:30\" / \"10.30\"\n",
    "    # This handles the colon found in modern reviews and the dot common in older ones.\n",
    "    text = re.sub(r'(\\b\\d{1,2})\\s*([.:])\\s*(\\d{2})\\b', r'\\1\\2\\3', text)\n",
    "\n",
    "    # 9b. Standardise a.m. and p.m.\n",
    "    # Matches digits + optional space + a/p + optional dots + m + optional dots.\n",
    "    # Output is standardised to \"a.m.\" or \"p.m.\" for dictionary consistency.\n",
    "    text = re.sub(r'(\\d+)\\s*([ap])\\.?\\s*m\\.?\\b', r'\\1 \\2.m.', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 10. Standardise Units (m, kg, mph, etc.)\n",
    "    # Ensures a single space between digits and units to prevent OOV \"word+unit\" tokens.\n",
    "    units = r'\\b(m|km|hh|kph|mph|kg|lb|oz|in|ft|yd|cm|mm|fps)\\b'\n",
    "    text = re.sub(rf'(\\d+)\\s*({units})', r'\\1 \\2', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9eb4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_polishing_batched(batch, text_column=\"MLM_Corrected_text\", style='dot'):\n",
    "    \"\"\"\n",
    "    Wrapper function to apply honorific standardization to a\n",
    "    dataset batch.\n",
    "    \"\"\"\n",
    "    input_texts = batch[text_column]\n",
    "    \n",
    "    polished_texts = [\n",
    "        standardize_honorifics(text, style=style) for text in input_texts\n",
    "        ]\n",
    "    \n",
    "    polished_texts = [\n",
    "        reapply_punctuation_spacing(text) for text in polished_texts\n",
    "        ]\n",
    "\n",
    "    # Return in a new column to preserve the previous step\n",
    "    return {f\"Final_Polished_Text\": polished_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3747cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_record(record):\n",
    "    \"\"\"\n",
    "    Displays a Hugging Face dataset record with metadata followed by two\n",
    "    text fields (Full_text, Final_Polished_Text) in side-by-side\n",
    "    columns in the terminal.\n",
    "    \n",
    "    This function works with both dictionaries and pandas Series\n",
    "    (e.g., a row from a DataFrame like df.loc[0]).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Display Metadata ---\n",
    "    print(\"-\" * 80)\n",
    "    print(\"METADATA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Define metadata keys to extract and display\n",
    "    metadata_keys = [\n",
    "        'Author', 'Title', 'Publication', 'Date', 'Place', 'URL'\n",
    "    ]\n",
    "    \n",
    "    for key in metadata_keys:\n",
    "        # Use .get() to safely access keys, providing 'N/A' if key is missing\n",
    "        value = record.get(key, 'N/A')\n",
    "        print(f\"{key+':':<15} {value}\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # --- 2. Prepare Columnar Text ---\n",
    "    \n",
    "    # Get text fields\n",
    "    full_text = record.get('Full_text', '')\n",
    "    capitalized_text = record.get('Final_Polished_Text', '')\n",
    "\n",
    "    # Get terminal width to make columns responsive\n",
    "    try:\n",
    "        terminal_width = shutil.get_terminal_size().columns\n",
    "    except OSError:\n",
    "        # Fallback if not in a real terminal (e.g., CI/CD)\n",
    "        terminal_width = 120\n",
    "\n",
    "    # Define spacing: 3 columns + 2 separators (\" | \")\n",
    "    # We give a little extra buffer for the separators\n",
    "    padding = 4 \n",
    "    col_width = (terminal_width - padding) // 2\n",
    "    \n",
    "    if col_width < 10:\n",
    "        print(\"Terminal is too narrow to display columns effectively.\")\n",
    "        print(\"\\nFull_text:\\n\", full_text)\n",
    "        print(\"\\Final_Polished_Text:\\n\", capitalized_text)\n",
    "        return\n",
    "\n",
    "    # Wrap text for each column\n",
    "    wrapped_full = textwrap.wrap(full_text, width=col_width)\n",
    "    wrapped_capitalized = textwrap.wrap(capitalized_text, width=col_width)\n",
    "\n",
    "    # Find the maximum number of lines needed\n",
    "    max_lines = max(len(wrapped_full), len(wrapped_capitalized))\n",
    "\n",
    "    # --- 3. Print Headers and Rows ---\n",
    "    \n",
    "    # Create header strings, left-aligned and truncated if necessary\n",
    "    header_full = \"Full_text\".ljust(col_width)\n",
    "    header_capitalized = \"Final_Polished_Text\".ljust(col_width)\n",
    "\n",
    "    print(f\"{header_full} | {header_capitalized}\")\n",
    "    print(f\"{'-' * col_width} | {'-' * col_width}\")\n",
    "\n",
    "    # Print each row\n",
    "    for i in range(max_lines):\n",
    "        # Get the line for each column, or an empty string if text is shorter\n",
    "        line_full = wrapped_full[i] if i < len(wrapped_full) else \"\"\n",
    "        line_capitalized = wrapped_capitalized[i] if i < len(wrapped_capitalized) else \"\"\n",
    "        \n",
    "        # Print the formatted row, ensuring each part adheres to the column width\n",
    "        print(f\"{line_full.ljust(col_width)} | {line_capitalized.ljust(col_width)}\")\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b90314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_driver():\n",
    "    \"\"\"\n",
    "    Main driver function to run the full pre-processing pipeline.\n",
    "\n",
    "    - Finds all .csv files in a specified input directory.\n",
    "    - Sequentially applies the four main processing stages:\n",
    "      1. Initial text cleaning (`clean_text_piece_batched`).\n",
    "      2. NER-driven correction and capitalization (`capitalization_and_correction_batched`).\n",
    "      3. Honorific standardization (`final_polishing_batched`).\n",
    "      4. Final punctuation spacing cleanup (`final_spacing_batched`).\n",
    "    - Saves the fully processed data to new .csv files in an output directory.\n",
    "    - Returns the DataFrame from the last processed file for inspection.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # Define the folder where your raw .csv files are located\n",
    "    input_folder = Path(INPUT_PATH)  \n",
    "    # Define the folder where processed files will be saved\n",
    "    output_folder = Path(OUTPUT_PATH)\n",
    "    \n",
    "    # --- Setup ---\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    log.info(f\"Input folder: '{input_folder}'\")\n",
    "    log.info(f\"Output folder: '{output_folder}'\")\n",
    "\n",
    "    # Setup SymSpellPy with the master word set\n",
    "    general_symspell_checker, master_words_added, art_words_added = setup_spellchecker(master_words=True, artist_set=True)\n",
    "    # Create a separate SymSpell instance for NER-shielded corrections\n",
    "    ner_symspell_checker, _, _ = setup_spellchecker(master_words=False, artist_set=True)\n",
    "    all_words = master_words_added | art_words_added\n",
    "\n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = list(input_folder.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        log.warning(f\"No CSV files found in '{input_folder}'. Exiting.\")\n",
    "        return\n",
    "\n",
    "    log.info(f\"Found {len(csv_files)} CSV file(s) to process: {[f.name for f in csv_files]}\")\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    for file_path in csv_files:\n",
    "        log.info(f\"\\n{'='*20} Processing file: {file_path.name} {'='*20}\")\n",
    "        \n",
    "        # 1. Load the data using the helper function\n",
    "        loaded_data = load_csv_file(str(file_path))\n",
    "        if not loaded_data:\n",
    "            log.error(f\"Failed to load {file_path.name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        raw_dataset = loaded_data['data']\n",
    "        log.info(f\"Loaded {loaded_data['length']} rows.\")\n",
    "\n",
    "        # --- 2. RUN THE FULL PIPELINE ---\n",
    "        log.info(\"Starting text cleaning pipeline...\")\n",
    "\n",
    "        log.info(\"Step 1/3: Initial regex cleaning and pattern correction...\")\n",
    "        regex_cleaned_dataset = raw_dataset['train'].map(\n",
    "            clean_text_piece_batched, \n",
    "            batched=True,\n",
    "            fn_kwargs={'text_column': 'Full_text'}\n",
    "        )\n",
    "\n",
    "        log.info(\"Step 2/3: NER, capitalisation and Symspell corrections...\")\n",
    "        symspell_corrected_dataset_v1 = regex_cleaned_dataset.map(\n",
    "            capitalization_and_correction_batched, \n",
    "            batched=True,\n",
    "            batch_size=8,\n",
    "            fn_kwargs={'text_column': 'Regex_cleaned_text', \n",
    "                       'ner_symspell': ner_symspell_checker,\n",
    "                       'symspell': general_symspell_checker, \n",
    "                       'master_word_set': all_words,\n",
    "                       'run': 1}\n",
    "        )\n",
    "\n",
    "        log.info(\"Step 3/3: Final polishing...\")\n",
    "        final_dataset = symspell_corrected_dataset_v1.map(\n",
    "            final_polishing_batched,\n",
    "            batched=True,\n",
    "            fn_kwargs={\n",
    "                'text_column': 'Symspell_corrected_text_v1',\n",
    "                'style': 'no_dot' \n",
    "            }\n",
    "        )\n",
    "\n",
    "        log.info(\"Pipeline complete. Final text is in 'Final_Polished_Text'.\")\n",
    "\n",
    "        # 4. Save the results to a new CSV file\n",
    "        log.info(\"Saving processed data...\")\n",
    "        output_filename = f\"{file_path.stem}_processed.csv\"\n",
    "        output_path = output_folder / output_filename\n",
    "        \n",
    "\n",
    "        # Convert the final dataset to a pandas DataFrame to save as CSV\n",
    "        final_df = final_dataset.to_pandas()\n",
    "        \n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        log.info(f\"Successfully saved processed data to '{output_path}'\")\n",
    "\n",
    "    log.info(f\"\\n{'='*20} Pipeline finished. {'='*20}\")\n",
    "\n",
    "    return all_words, output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f48a86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_oov_corrections_batched(batch, text_column, corrected_text_column, master_word_set):    \n",
    "    \n",
    "    oov_scores = list()\n",
    "\n",
    "    for original_text, corrected_text in zip(batch[text_column], batch[corrected_text_column]):\n",
    "        if not original_text or not corrected_text:\n",
    "            continue\n",
    "\n",
    "        original_words = WORD_REGEX.findall(original_text.lower())\n",
    "        corrected_words = WORD_REGEX.findall(corrected_text.lower())\n",
    "\n",
    "        original_oov = sum(1 for word in original_words if word not in master_word_set)\n",
    "        corrected_oov = sum(1 for word in corrected_words if word not in master_word_set)\n",
    "\n",
    "        # Calculate OOV reduction percentage\n",
    "        # Deal with division by zero if there were no OOVs originally\n",
    "        if original_oov == 0:\n",
    "            oov_scores.append(0)\n",
    "        else:\n",
    "            oov_difference_percentage = (original_oov - corrected_oov) / original_oov \n",
    "            oov_scores.append(oov_difference_percentage)\n",
    "\n",
    "    return {'OOV_Correction_Improvement_Percentage': oov_scores}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9403b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_oov(wordlist, output_folder):\n",
    "    \"\"\"\n",
    "    Evaluates the pipeline's effectiveness on out-of-vocabulary (OOV) words\n",
    "    by comparing pre- and post-correction texts against a master word list.\n",
    "    Outputs statistics on corrections made to OOV words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine number of CPU cores to use (leaving 2 free)\n",
    "    num_cores = max(1, os.cpu_count() - 2) \n",
    "\n",
    "    # Define the folder where processed files will be saved\n",
    "    # The input path is the same as output path from pipeline_driver\n",
    "    # The output path will save the evaluation results in a separate folder\n",
    "    input_folder = Path(output_folder)\n",
    "    output_folder = Path(input_folder) / \"evaluation_results\"\n",
    "    \n",
    "    # --- Setup ---\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    log.info(f\"Input folder: '{input_folder}'\")\n",
    "    log.info(f\"Output folder: '{output_folder}'\")\n",
    "\n",
    "    # If no wordlist provided, setup SymSpellPy to get the master word set\n",
    "    if not wordlist:\n",
    "        # Setup SymSpellPy with the master word set\n",
    "        # Same setup as in pipeline_driver\n",
    "        _, master_words_added, art_words_added = setup_spellchecker(master_words=True, artist_set=True)\n",
    "        wordlist = master_words_added | art_words_added\n",
    "\n",
    "    # This list will hold smaller dataframes for final reduction\n",
    "    all_metrics_storage = []\n",
    "    negative_metric_storage = []\n",
    "    periods_min_texts = dict()\n",
    "    periods_max_texts = dict()\n",
    "\n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = list(input_folder.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        log.warning(f\"No CSV files found in '{input_folder}'. Exiting.\")\n",
    "        return\n",
    "\n",
    "    log.info(f\"Found {len(csv_files)} CSV file(s) to process: {[f.name for f in csv_files]}\")\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    for file_path in csv_files:\n",
    "        log.info(f\"\\n{'='*20} Processing file: {file_path.name} {'='*20}\")\n",
    "        \n",
    "        # 1. Load the data using the helper function\n",
    "        loaded_data = load_csv_file(str(file_path))\n",
    "        if not loaded_data:\n",
    "            log.error(f\"Failed to load {file_path.name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        raw_dataset = loaded_data['data']\n",
    "        log.info(f\"Loaded {loaded_data['length']} rows.\")\n",
    "\n",
    "        log.info(\"Evaluating OOV correction effectiveness...\")\n",
    "        oov_dataset = raw_dataset['train'].map(\n",
    "            evaluate_oov_corrections_batched,\n",
    "            batched=True,\n",
    "            num_proc=num_cores,\n",
    "            batch_size=500,\n",
    "            fn_kwargs={\n",
    "                'text_column': 'Full_text',\n",
    "                'corrected_text_column': 'Final_Polished_Text',\n",
    "                'master_word_set': wordlist\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        oov_df = oov_dataset.to_pandas()\n",
    "\n",
    "\n",
    "        metrics_df = oov_df[['Date', 'Publication', 'OOV_Correction_Improvement_Percentage']].copy()\n",
    "        metrics_df.loc[metrics_df['Date'] == \"The Times\", 'Date'] = metrics_df['Publication']\n",
    "        # Extraction logic for the year\n",
    "        # Converts \"2008-Nov-22\" -> 2008\n",
    "        metrics_df['Year'] = pd.to_datetime(metrics_df['Date'], errors='coerce').dt.year\n",
    "        \n",
    "        # If any rows failed datetime conversion, try to grab the first 4 chars (YYYY)\n",
    "        mask = metrics_df['Year'].isna()\n",
    "        metrics_df.loc[mask, 'Year'] = metrics_df.loc[mask, 'Date'].str.extract(r'(\\d{4})')[0].astype(float)\n",
    "\n",
    "        metrics_df['Period'] = (metrics_df['Year'] // 50) * 50\n",
    "\n",
    "        metrics_df.drop(columns=['Publication', 'Date', 'Year'], inplace=True)\n",
    "        all_metrics_storage.append(metrics_df)\n",
    "\n",
    "        # Isolate all rows with a negative improvement score\n",
    "        negative_impact_df = oov_df[oov_df['OOV_Correction_Improvement_Percentage'] < 0].copy()\n",
    "        negative_metric_storage.append(negative_impact_df)\n",
    "\n",
    "\n",
    "        # Determine min/max oov for each period\n",
    "        for period in metrics_df['Period'].unique():\n",
    "            period_data = metrics_df[metrics_df['Period'] == period]\n",
    "            min_idx = period_data['OOV_Correction_Improvement_Percentage'].idxmin()\n",
    "            max_idx = period_data['OOV_Correction_Improvement_Percentage'].idxmax()\n",
    "            min_oov = period_data['OOV_Correction_Improvement_Percentage'].min()\n",
    "            max_oov = period_data['OOV_Correction_Improvement_Percentage'].max()\n",
    "\n",
    "            # Check if period already exists in the dictionary\n",
    "            if period in periods_min_texts:\n",
    "                min_score, original_text, cleaned_text = periods_min_texts[period]\n",
    "                if min_oov < min_score:\n",
    "                    periods_min_texts[period] = (min_oov, oov_df.loc[min_idx, 'Full_text'], oov_df.loc[min_idx, 'Final_Polished_Text'])\n",
    "            else:\n",
    "                periods_min_texts[period] = (min_oov, oov_df.loc[min_idx, 'Full_text'], oov_df.loc[min_idx, 'Final_Polished_Text'])\n",
    "\n",
    "            if period in periods_max_texts:\n",
    "                max_score, original_text, cleaned_text = periods_max_texts[period]\n",
    "                if max_oov > max_score:\n",
    "                    periods_max_texts[period] = (max_oov, oov_df.loc[max_idx, 'Full_text'], oov_df.loc[max_idx, 'Final_Polished_Text'])\n",
    "            else:\n",
    "                periods_max_texts[period] = (max_oov, oov_df.loc[max_idx, 'Full_text'], oov_df.loc[max_idx, 'Final_Polished_Text'])\n",
    "\n",
    "\n",
    "\n",
    "    # Reduce all collected metrics into a single DataFrame\n",
    "    negative_metric_storage_df = pd.concat(negative_metric_storage, ignore_index=True)\n",
    "    negative_metric_storage_df.to_csv(output_folder / \"negative_oov_impact_records.csv\", index=False)\n",
    "\n",
    "    log.info(\"Aggregating results across all files...\")\n",
    "    master_df = pd.concat(all_metrics_storage, ignore_index=True)\n",
    "\n",
    "    # Groupby acts as the 'reduce' function\n",
    "    summary_stats = master_df.groupby('Period')['OOV_Correction_Improvement_Percentage'].agg(\n",
    "        ['count', 'mean', 'median', 'std']\n",
    "    ).reset_index()\n",
    "\n",
    "    log.info(\"Final summary statistics computed.\")\n",
    "    # Save and Print Summary\n",
    "    summary_stats.to_csv(output_folder / \"global_oov_summary.csv\", index=False)\n",
    "    log.info(\"Saved global summary statistics to 'global_oov_summary.csv'.\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for period in sorted(periods_min_texts.keys()):\n",
    "        min_score, original_text_min, cleaned_text_min = periods_min_texts[period]\n",
    "        max_score, original_text_max, cleaned_text_max = periods_max_texts[period]\n",
    "\n",
    "        rows.append({\n",
    "            'Period': period,\n",
    "            'Min_OOV_Improvement': min_score,\n",
    "            'Original_Text_Min': original_text_min,\n",
    "            'Cleaned_Text_Min': cleaned_text_min,\n",
    "            'Max_OOV_Improvement': max_score,\n",
    "            'Original_Text_Max': original_text_max,\n",
    "            'Cleaned_Text_Max': cleaned_text_max,\n",
    "        })\n",
    "\n",
    "        print(f\"\\n--- Period: {period} ---\")\n",
    "        print(f\"Min OOV Improvement: {min_score:.4f}\")\n",
    "        \n",
    "        display_record({\n",
    "            'Full_text': original_text_min,\n",
    "            'Final_Polished_Text': cleaned_text_min,\n",
    "        })\n",
    "\n",
    "        print(f\"\\nMax OOV Improvement: {max_score:.4f}\")\n",
    "        display_record({\n",
    "            'Full_text': original_text_max,\n",
    "            'Final_Polished_Text': cleaned_text_max,\n",
    "        })\n",
    "\n",
    "    period_df = pd.DataFrame(rows)\n",
    "\n",
    "    # 4. Save to CSV\n",
    "    period_df.to_csv(output_folder / \"extreme_oov_cases_per_period.csv\", index=False)\n",
    "\n",
    "    return summary_stats, output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bc2d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oov_performance(summary_df, output_folder):\n",
    "    \"\"\"\n",
    "    Creates two plots: \n",
    "    1. A line plot showing Mean/Median OOV improvement with standard deviation.\n",
    "    2. A bar chart showing the volume of data processed per period.\n",
    "    \"\"\"\n",
    "    # Plot 1: Improvement Metrics\n",
    "    plt.errorbar(\n",
    "        summary_df['Period'], \n",
    "        summary_df['mean'], \n",
    "        yerr=summary_df['std'], \n",
    "        fmt='-o', \n",
    "        label='Mean Improvement (%)', \n",
    "        capsize=5, \n",
    "        color='#1f77b4'\n",
    "    )\n",
    "    plt.plot(\n",
    "        summary_df['Period'], \n",
    "        summary_df['median'], \n",
    "        '-s', \n",
    "        label='Median Improvement (%)', \n",
    "        alpha=0.7, \n",
    "        color='#ff7f0e'\n",
    "    )\n",
    "    \n",
    "    plt.title('OOV Correction Improvement by Historical Period')\n",
    "    plt.xlabel('Period (Start Year)')\n",
    "    plt.ylabel('Improvement Percentage (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    img_dir = Path(output_folder) / 'images'\n",
    "    img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(img_dir / 'oov_improvement_trends.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Processing Volume\n",
    "    plt.bar(\n",
    "        summary_df['Period'], \n",
    "        summary_df['count'], \n",
    "        width=40, \n",
    "        color='#aec7e8', \n",
    "        edgecolor='#1f77b4', \n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.title('Review Volume per 50-Year Period')\n",
    "    plt.xlabel('Period (Start Year)')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.xticks(summary_df['Period'])\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_dir / 'data_volume_per_period.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e70ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_merged_performance(summary_df, output_folder):\n",
    "    \"\"\"\n",
    "    Creates a single dual-axis plot showing both OOV improvement trends\n",
    "    and data volume per period.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # --- Plot 1 (Primary Axis - Left): Improvement Percentage ---\n",
    "    # Plot error bars for Mean OOV Improvement\n",
    "    line1 = ax1.errorbar(\n",
    "        summary_df['Period'],\n",
    "        summary_df['mean'],\n",
    "        yerr=summary_df['std'],\n",
    "        fmt='-o',\n",
    "        label='Mean OOV Improvement (%)',\n",
    "        capsize=5,\n",
    "        color='#1f77b4',  # Standard blue\n",
    "        zorder=2  # Ensure lines are on top of bars\n",
    "    )\n",
    "    \n",
    "    ax1.set_xlabel('Period (Start Year)', fontsize=12)\n",
    "    ax1.set_ylabel('Improvement Percentage (%)', fontsize=12, color='#1f77b4')\n",
    "    ax1.tick_params(axis='y', labelcolor='#1f77b4')\n",
    "    # Set x-ticks to match the periods for clarity\n",
    "    ax1.set_xticks(summary_df['Period'])\n",
    "    # Add a light grid based on the primary axis\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Plot 2 (Secondary Axis - Right): Data Volume ---\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot bars for Document Count\n",
    "    bar1 = ax2.bar(\n",
    "        summary_df['Period'],\n",
    "        summary_df['count'],\n",
    "        width=35,  # Adjust width for visibility\n",
    "        color='#aec7e8',  # Lighter blue\n",
    "        edgecolor='#1f77b4',\n",
    "        alpha=0.6,\n",
    "        label='Document Count',\n",
    "        zorder=1  # Ensure bars are behind lines\n",
    "    )\n",
    "    \n",
    "    ax2.set_ylabel('Number of Documents', fontsize=12, color='navy')\n",
    "    ax2.tick_params(axis='y', labelcolor='navy')\n",
    "\n",
    "    # --- Formatting ---\n",
    "    plt.title('Pipeline Effectiveness and Data Volume by Historical Period', fontsize=14)\n",
    "\n",
    "    # Combine legends from both axes into a single legend box\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    bars, bar_labels = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + bars, labels + bar_labels, loc='upper right')\n",
    "\n",
    "    img_dir = Path(output_folder) / 'images'\n",
    "    img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_dir / 'merged_oov_volume_plot.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b91b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_latex(summary_df, output_folder, filename='oov_results.tex'):\n",
    "    \"\"\"\n",
    "    Generates a professionally formatted LaTeX table.\n",
    "    \"\"\"\n",
    "    # Create a copy and rename columns for the table header\n",
    "    latex_df = summary_df.copy()\n",
    "    latex_df.columns = ['Period', 'Count', 'Mean Imp. (%)', 'Median Imp. (%)', 'Std Dev']\n",
    "    \n",
    "    latex_str = latex_df.to_latex(\n",
    "        index=False, \n",
    "        caption='Summary of OOV Correction Improvement across Historical Art Reviews',\n",
    "        label='tab:oov_pipeline_summary',\n",
    "        column_format='lcccc',\n",
    "        float_format=\"%.2f\"\n",
    "    )\n",
    "    \n",
    "    with open(Path(output_folder) / filename, 'w') as f:\n",
    "        f.write(latex_str)\n",
    "    \n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2603613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input folder: '1-sample-test'\n",
      "INFO:__main__:Output folder: 'Cleaned_data/1-sample-test'\n",
      "INFO:__main__:Setting up SymSpellPy...\n",
      "INFO:__main__:Added 45111 words from ENTITY wordlist to SymSpell.\n",
      "INFO:__main__:Added 1337241 words from Project Gutenberg library to SymSpell.\n",
      "INFO:__main__:SymSpell setup complete. Total unique terms: 1363716\n",
      "INFO:__main__:Setting up SymSpellPy...\n",
      "INFO:__main__:Added 45111 words from ENTITY wordlist to SymSpell.\n",
      "INFO:__main__:Added 0 words from Project Gutenberg library to SymSpell.\n",
      "INFO:__main__:SymSpell setup complete. Total unique terms: 45111\n",
      "INFO:__main__:Found 1 CSV file(s) to process: ['test_1_sample copy.csv']\n",
      "INFO:__main__:\n",
      "==================== Processing file: test_1_sample copy.csv ====================\n",
      "INFO:__main__:Loaded 76 rows.\n",
      "INFO:__main__:Starting text cleaning pipeline...\n",
      "INFO:__main__:Step 1/3: Initial regex cleaning and pattern correction...\n",
      "INFO:__main__:Step 2/3: NER, capitalisation and Symspell corrections...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4502228dfbb480e93464e3c5a625a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 3/3: Final polishing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983009b77cf94c0b854fbe895cea62e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pipeline complete. Final text is in 'Final_Polished_Text'.\n",
      "INFO:__main__:Saving processed data...\n",
      "INFO:__main__:Successfully saved processed data to 'Cleaned_data/1-sample-test/test_1_sample copy_processed.csv'\n",
      "INFO:__main__:\n",
      "==================== Pipeline finished. ====================\n"
     ]
    }
   ],
   "source": [
    "# --- Execute the main function ---\n",
    "# This block ensures the code runs when the script is executed\n",
    "wordlist, output_folder = pipeline_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afdc78ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input folder: 'Cleaned_data/Data'\n",
      "INFO:__main__:Output folder: 'Cleaned_data/Data/evaluation_results'\n",
      "INFO:__main__:Found 7 CSV file(s) to process: ['Operas_gale_processed.csv', 'Poetry_gale_processed.csv', 'Books_gale_processed.csv', 'Theater_gale_processed.csv', 'Art_exhibitions_gale_processed.csv', 'Concerts_gale_processed.csv', 'Dance_gale_processed.csv']\n",
      "INFO:__main__:\n",
      "==================== Processing file: Operas_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463a5949e6904d16888b75b94d03a0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 972 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4615866079244ca586aca41fe7bd7455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "==================== Processing file: Poetry_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e646cccc6d4472a70316764bc2e42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 629 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84e4240a15b4f0a8dd53bc726362358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/629 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "==================== Processing file: Books_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dc5cae273d43caa37bf52da280b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 2283 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c7e4f82846471d8fdbc71d2b545abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "==================== Processing file: Theater_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b098410859b041e5ac4040091e5e3e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 18036 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc3908d8fbf4642bf99bef8c236731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18036 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "==================== Processing file: Art_exhibitions_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54146829a623486d84399be9ecd97a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 2651 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245728f2531c4bc0bad9b06d9f5412af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "==================== Processing file: Concerts_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf144f7a83a482681fb715065f3a594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 21907 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1066848ff87549e9a8f80df0f1a886ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/21907 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "==================== Processing file: Dance_gale_processed.csv ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189f23596a05441d828a4727e1f5311b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 3766 rows.\n",
      "INFO:__main__:Evaluating OOV correction effectiveness...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4be8944df404f2bba515f2ddea9a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Aggregating results across all files...\n",
      "INFO:__main__:Final summary statistics computed.\n",
      "INFO:__main__:Saved global summary statistics to 'global_oov_summary.csv'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Period: 1750 ---\n",
      "Min OOV Improvement: 0.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "The Beth Theatre closed on Saturday    | The Beth Theatre closed on saturday   \n",
      "la#, after what is termed a profitable | la, after what is termed a profitable \n",
      "season. Friday evening Mrs. Martyr and | season. Friday evening Mrs. Martyr and\n",
      "Mr, I)Arley commenced their regular    | Mr, I) Arley commenced their regular  \n",
      "engagements at Vauxhall, their         | engagements at Vauxhall, their        \n",
      "performance at the Theatre having      | performance at the Theatre having     \n",
      "prevented their doing it sooner. We    | prevented their doing it sooner. We   \n",
      "are authorized to acquaint the Public* | are authorized to acquaint the public \n",
      "that the- exhibition of the Sortie of  | that the exhibition of the sortie of  \n",
      "Gibraltar* painted by Mr. Trumbull,    | Gibraltar painted by Mr. Trumbull,    \n",
      "will close at Spring Gardens, on       | will close at Spring Gardens, on      \n",
      "Saturday the 17th of June. _ His       | saturday the 17th of june. His majesty\n",
      "Majesty has confirmed the appointment  | has confirmed the appointment of      \n",
      "of General Tord Amherft, to be         | general Tord Amherft, to be           \n",
      "President 'of the Court Martiali which | president'of the Court martiali which \n",
      "is to fit at the HorseGuards on Col.   | is to fit at the Horseguards on Col.  \n",
      "John Debbelgge of the Corps of         | John Debbelgge of the Corps Of        \n",
      "Engineers, in consequcnce'of a         | Engineers, in consequence a complaint \n",
      "complaint prefer red by the Mailer     | prefer red by the mailer general Of   \n",
      "General of the Ordnance,               | The Ordnance,                         \n",
      "================================================================================\n",
      "\n",
      "Max OOV Improvement: 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "THEATRE- o pece upon the stage ever    | Theatre pece upon the stage ever      \n",
      "displayed a more perfect or more       | displayed a more perfect or more      \n",
      "various display of comic powers? than  | various display of comic powers? Than \n",
      "that most excellent comedy, The Merr*  | that most excellent comedy, the merr  \n",
      "Wives of Windsor, as performed at      | wives of windsor, as performed at     \n",
      "Covent Garden.. If the fintst writing  | Covent Garden.. If the finest writing \n",
      "and best acting combined could retain  | and best acting combined could retain \n",
      "the powers of attraction after         | the powers of attraction after        \n",
      "repeated exhibitions, tins drama would | repeated exhibitions, tins drama would\n",
      "ever produce overflowing houses ; but  | ever produce overflowing houses; but  \n",
      "Novelty, though led by Folly and       | novelty, though led by Folly and      \n",
      "Impudence., has a temporary influence, | impudeNce., has a temporary influence,\n",
      "and draws the unthinking to attend     | and draws the unthinking to attend    \n",
      "those exhibi- tions which have lately  | those exhibitions which have lately   \n",
      "disgraced- the Theatre Royal in the    | disgraced the Theatre Royal in the    \n",
      "Haymarket, ami which stigmatize the    | Haymarket, ami which stigmatize the   \n",
      "taste of this town. At Drury-Lane, the | taste of this town. At Drury Lane, the\n",
      "announcing of Mrs. Liddons in          | announcing of Mrs. Liddons in         \n",
      "Rosalind, her first appearance in      | rosalind, her first appearance in     \n",
      "comedy, bn the London stage since ilie | comedy, bn the London stage since ilie\n",
      "grew into fame, drew together as       | grew into fame, drew together as      \n",
      "crowded and splendid an audience, as   | crowded and splendid an audience, as  \n",
      "ever appeared in a theatre.—It would   | ever appeared in a theatre. It would  \n",
      "be uncandid to fay this unusual        | be uncandid to fay this unusual       \n",
      "attendance, at this season, was solely | attendance, at this season, was solely\n",
      "owing to curiosity. The established    | owing to curiosity. The established   \n",
      "professional reputation of Mrs.        | professional reputation of Mrs.       \n",
      "Siddons must ever ensure-her a full    | Siddons must ever ensure her a full   \n",
      "benefit. She has claims upon, the      | benefit. She has claims upon, the     \n",
      "gratitude of the public for the enter* | gratitude of the public for the enter \n",
      "taiument she has ifforded. We come now | tainment she has afforded. We come now\n",
      "to examine her- performance of         | to examine her performance of         \n",
      "Rosalind, and will execute the task in | rosalind, and will execute the task in\n",
      "very few words. .Firsti -then, we do   | very few words.. Birsti then, we do   \n",
      "not think that nature has formed her   | not think that nature has formed her  \n",
      "of-those materials which in real life  | of those materials which in real life \n",
      "would constitute' such a character as  | would constitute' such a character as \n",
      "Rosaline/, of course\"the adlrcfs is    | Rosaline, of course\" the allofs is too\n",
      "too apparent; and though the fiction   | apparent; and though the fiction be   \n",
      "be highly finished by art* the polish  | highly finished by art the polish is  \n",
      "is evidently-not the work of nature.   | evidently not the work of nature.     \n",
      "================================================================================\n",
      "\n",
      "--- Period: 1800 ---\n",
      "Min OOV Improvement: 0.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "CO FEN i -GA RD EN THEATRE. The Lord   | Co fen i ga rd en theatre. The Lord of\n",
      "of the Manor, the pleasing production  | the manor, the pleasing production of!\n",
      "of ! Burgoyne’s pen, was performed at  | Burgoyne s pen, was performed at this \n",
      "this Theatre yesterday evening. The    | theatre yesterday evening. The        \n",
      "continued illness of Miss Ste!vbns     | continued illness of Miss Ste! VbNs   \n",
      "still deprives us of the gratification | still deprives us of the gratification\n",
      "of hearing her—a loss irreparable to   | of hearing her a loss irreparable to  \n",
      "the English opera. Mrs. Sterling was   | the english opera. Mrs. Sterling was  \n",
      "the Sophia. Miss Matthews played       | the Sophia. Miss Matthews played      \n",
      "Annette, which she rendered lively and | Annette, which she rendered lively and\n",
      "interesting—and to the songs of which  | interesting and to the songs of which \n",
      "she did great justice. Fawcett gave    | she did great justice. Fawcett gave   \n",
      "the odd benevolence of Sir John        | the odd benevolence of Sir John       \n",
      "Contrast with the rich humour which he | Contrast with the rich humour which he\n",
      "has unceasingly at command. 'Sinclair  | has unceasingly at command.'Sinclair  \n",
      "sung with taste and sweetness. We wish | sung with taste and sweetness. We wish\n",
      "some substitute could be found for Mr. | some substitute could be found for Mr.\n",
      "Taylor, ■ in Rashly. This gentleman    | Taylor, in rashly. This gentleman has \n",
      "has a good sound voice । enough—but it | a good sound voice enough but it wants\n",
      "wants the mellowness and softness      | the mellowness and softness requisite \n",
      "requisite in the two beautiful songs “ | in the two beautiful songs encompassed\n",
      "Encompassed in an angel’s frame,”—and  | in an angel s frame, and \" when first \n",
      "\" When first this humble roof I        | this humble roof i knew, nor was be   \n",
      "knew,”—nor was be capable es singing   | capable es singing up to Sinclair in  \n",
      "up to Sinclair in the fine trio of the | the fine trio of the 2d act; besides  \n",
      "2d act; besides all which, the         | all which, the character of RAshLy,   \n",
      "character of Rashly, although there    | although there may not be much of it, \n",
      "may not be much of it, is tinged with  | is tinged with a species of melancholy\n",
      "a species of melancholy sensibility,   | sensibility, which was not by any     \n",
      "which was not by any means apparent in | means apparent in the air, the tone,  \n",
      "the air, the tone, or manner of Mr.    | or manner of Mr. Taylor. Jones was    \n",
      "Taylor. Jones was exquisite in Young   | exquisite in young contrast; if not   \n",
      "Contrast ; if not original, it was at  | original, it was at least from himself\n",
      "least from himself that lie co- nied.  | that lie copied. His fops are famous  \n",
      "His fops are famous fora family        | fora family likeness, and let them    \n",
      "likeness, and let them wear what       | wear what disguise they will, we soon \n",
      "disguise they will, we soon unmask the | unmask the face, and figure, and      \n",
      "face, and figure, and lounging dialect | lounging dialect of Mr. Jones.        \n",
      "of Mr. Jones. Simmons, in Æfre. Doll   | Simmons, in fre. Doll Flagon, was not \n",
      "Flagon, was not deficient in the       | deficient in the graceful delicacy,   \n",
      "graceful delicacy, which has ever      | which has ever characterised that     \n",
      "characterised that bewitching female.' | bewitching female. '                  \n",
      "================================================================================\n",
      "\n",
      "Max OOV Improvement: 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "THEATRE. D RURT-LANE. \" Their          | Theatre. D curt lane. \" their         \n",
      "Majesties, accompanied by three        | majesties, accompanied by three       \n",
      "Princesses, honoured this Theatre last | princesses, honoured this theatre last\n",
      "night with their presence^.to tec the  | night with their presence. To tec the \n",
      "Comedy of The Rivals and the Dramatic  | comedy of the rivals and the dramatic \n",
      "Romance of .Blue-Beard. The liveliest  | romance of. Blue beard. The liveliest \n",
      "demonstrations of loyalty and          | demonstrations of loyalty and         \n",
      "affection uere expressed on the        | affection uere expressed on the       \n",
      "occasion by-a splendid and crowded     | occasion by a splendid and crowded    \n",
      "audiences The performance was          | audiences the performance was         \n",
      "respectably executed; but particular   | respectably executed; but particular  \n",
      "encomium is due to tire highly-        | encomium is due to tire highly        \n",
      "finished, acting of King- in Sir       | finished, acting of King in Sir       \n",
      "Anthony Absolute. The Captain Absolute | Anthony absolute. The captain AbsolUte\n",
      "of Powell is a cliassc-and spirited    | of Powell is a class and spirited     \n",
      "delineation; and Bannister, in the     | delineation; and Bannister, in the    \n",
      "whimsical part of Acres, supplied      | whimsical part of Acres, supplied     \n",
      "abundant matter for merriment.         | abundant matter for merriment.        \n",
      "================================================================================\n",
      "\n",
      "--- Period: 1850 ---\n",
      "Min OOV Improvement: 0.2000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "LONDON SYMPHONY CONCERTS. The          | LONDON Symphony concerts. The         \n",
      "surprising improvement which has taken | surprising improvement which has taken\n",
      "place this season in the playing of    | place this season in the playing of   \n",
      "Mr. Henschel's orchestra -was still    | Mr. Henschel's orchestra was still    \n",
      "further exhibited last night, when the | further exhibited last night, when the\n",
      "overture to Oberon was magnificently   | overture to oberon was magnificently  \n",
      "played at the opening of a very        | played at the opening of a very       \n",
      "attractive concert, the chief feature  | attractive concert, the chief feature \n",
      "of which was the performance of        | of which was the performance of       \n",
      "Brahms's fine concerto for violin and  | Brahms's fine concerto for violin and \n",
      "violoncello, op. 102. In this masterly | violoncello, op. 102. In this masterly\n",
      "work the solo parts were played with   | work the solo parts were played with  \n",
      "much effect by M. Gorski and Herr Carl | much effect by m. Gorski and herr Carl\n",
      "Fuchs, the former of whom was in far   | Fuchs, the former of whom was in far  \n",
      "more complete possession of his powers | more complete possession of his powers\n",
      "than he was at his own concert. The    | than he was at his own concert. The   \n",
      "melody of the slow movement was given  | melody of the slow movement was given \n",
      "out by both artists with faultless     | out by both artists with faultless    \n",
      "phrasing and expression, and the       | phrasing and expression, and the      \n",
      "performance of the whole work reached  | performance of the whole work reached \n",
      "a high degree of excellence. Raff's \"  | a high degree of excellence. Raff's \" \n",
      "Lenore\" symphony and the overture to   | lenore\" symphony and the overture to  \n",
      "Die Meistersinger were also played in  | die meistersinger were also played in \n",
      "admirable style, and Miss Evangeline   | admirable style, and Miss Evangeline  \n",
      "Florence sang Elsa's air from the      | Florence sang Elsa's air from the     \n",
      "second act of Lo7cn-grin with so much  | second act of loin grin with so much  \n",
      "charm and intelligence that she was    | charm and intelligence that she was   \n",
      "twice recalled to the platform.        | twice recalled to the platform.       \n",
      "================================================================================\n",
      "\n",
      "Max OOV Improvement: 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "In a dainty little volume entitled     | In a dainty little volume entitled    \n",
      "PoETs o% POETS (Kegan Paul and Co.)    | poets o poets (Kegan Paul And Co.)    \n",
      "Mrs. Richard Strachey has collected \"  | Mrs. Richard Strachey has collected \" \n",
      "those passages in our literaturewhere  | those passages in our literature      \n",
      "poets, themselves of distinction, have | poets, themselves of distinction, have\n",
      "cast into a poetic form their judgment | cast into a poetic form their judgment\n",
      "on others worthy of the title.\" The    | on others worthy of the title. \" the  \n",
      "idea is a little fanciful, perhaps,    | idea is a little fanciful, perhaps,   \n",
      "and its execution leads, in some       | and its execution leads, in some      \n",
      "cases, to some rather painful          | cases, to some rather painful         \n",
      "mnutilations of great poems, but the   | mutilations of great poems, but the   \n",
      "volume will not be *vithout its value  | volume will not be vithout its value  \n",
      "to lovers and students of poetry.      | to lovers and students of poetry.     \n",
      "================================================================================\n",
      "\n",
      "--- Period: 1900 ---\n",
      "Min OOV Improvement: -0.6667\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "MIDDLESEX HOSPITAL CONCERT B.B.C.      | Middlesex hospital concert b. B. C.   \n",
      "CHORAL SOCIETY AND STUDENTS' ORCHESTRA | ChORal society and students' ORCHESTRA\n",
      "The weather had taken the direction    | the weather had taken the direction \" \n",
      "\"troppo fresco\" last night at the 'Al  | troppo fresco\" last night at the'al   \n",
      "Fresco Promenade Concert \" given in    | fresco promenade concert \" given in   \n",
      "the paved garden of the Middlesex      | the paved garden of the Middlesex     \n",
      "Hospital, but the entertainment        | Hospital, but the entertainment       \n",
      "justified its name, since it was, in   | justified its name, since it was, in  \n",
      "fact, given under an open sky, however | fact, given under an open sky, however\n",
      "inclement, and since a great part of   | inclement, and since a great part of  \n",
      "the audience stood or walked about.    | the audience stood or walked about.   \n",
      "Some had seats and others, again, Jay  | Some had seats and others, again, jay \n",
      "to listen, for the balconies of the    | to listen, for the balconies of the   \n",
      "court were filled by patients. Except  | court were filled by patients. Except \n",
      "for the chirping of sparrows the       | for the chirping of sparrows the      \n",
      "conditions for hearing were excellent, | conditions for hearing were excellent,\n",
      "the high walls of the hospital         | the high walls of the hospital        \n",
      "shutting out extraneous noises and     | shutting out extraneous noises and    \n",
      "also ensuring ample resonance. Mr.     | also ensuring ample resonance. Mr.    \n",
      "Leslie Woodgate conducted the B.B.C.   | Leslie Woodgate conducted the b. B. C.\n",
      "Choral Society and a students'         | Choral Society and a students'        \n",
      "orchestra from the R.A.M. in an        | orchestra from the r. A. M. In an     \n",
      "attractive programme containing        | attractive programme containing       \n",
      "excerpts from Handel's Solomon.        | excerpts from Handel's solomon.       \n",
      "students' songs (among which were      | Students' songs (among which were     \n",
      "reckoned *'Vilikins \" and \" Little     | reckoned'vilikins \" and \" little brown\n",
      "Brown Jug \"), orchestral pieces by     | jug \") , orchestral pieces by Percy   \n",
      "Percy Grainger, and the Dances from    | Grainger, and the dances from         \n",
      "Purcell's \" Fairy Queen.\" One of the   | Purcell's \" fairy queen. \" one of the \n",
      "most pleasing choices was the          | most pleasing choices was the         \n",
      "conductor's own work for chorus and    | conductor's own work for chorus and   \n",
      "orchestra, \" 0 Pastoral Heart of       | orchestra, \" 0 pastoral heart of      \n",
      "England \": lyrical and tranquil music, | england \": lyrical and tranquil music,\n",
      "well planned and grateful for the      | well planned and grateful for the     \n",
      "voice. The soloists of the evening     | voice. The soloists of the evening    \n",
      "were Miss Margaret Godley, soprano,    | were Miss Margaret Godley, soprano,   \n",
      "and Mr. Raymond Newell, baritone.      | and Mr. Raymond Newell, baritone.     \n",
      "During the interval MAJOR ASTOR, M.P., | During the interval Major Astor, m.   \n",
      "chairman of the hospital, proposed a   | P., chairman of the hospital, proposed\n",
      "vote of thanks to the musicians and    | a vote of thanks to the musicians and \n",
      "paid tribute to the healing influence  | paid tribute to the healing influence \n",
      "of music. He said that since earphones | of music. He said that since earphones\n",
      "had been installed in the wards the    | had been installed in the wards the   \n",
      "average stay of the patients had       | average stay of the patients had      \n",
      "become shorter. He believed that a     | become shorter. He believed that a    \n",
      "concert given in the very heart of the | concert given in the very heart of the\n",
      "hospital would have even more striking | hospital would have even more striking\n",
      "results, and suggested that \" we all   | results, and suggested that \" we all  \n",
      "felt better already.\"                  | felt better already. \"                \n",
      "================================================================================\n",
      "\n",
      "Max OOV Improvement: 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "ENGLISH POETRY EXHIBITION OF FIRST     | English poetry exhibition of first    \n",
      "EDITIONS An exhibition of English      | editions an exhibition of english     \n",
      "poetry, consisting of first and early  | poetry, consisting of first and early \n",
      "editions of the English poets from     | editions of the english poets from    \n",
      "Chaucer to the present day, has been   | Chaucer to the present day, has been  \n",
      "organized by the National Book League  | organized by the National Book League \n",
      "and will open at 7, Albemarle Street   | and will open at 7, Albemarle Street  \n",
      "on Thursday, April 10, at 3 p.m.       | on thursday, april 10, at 3 p. M.     \n",
      "Almost every book in the collection    | Almost every book in the collection   \n",
      "has been chosen because it was, to     | has been chosen because it was, to    \n",
      "those who originally bought it or read | those who originally bought it or read\n",
      "it, the first sign that a new poet had | it, the first sign that a new poet had\n",
      "appeared. It will be the most          | appeared. It will be the most         \n",
      "comprehensive collection of rare of    | comprehensive collection of rare of   \n",
      "especially interesting books o(        | especially interesting books o        \n",
      "English poetry ynt show n in public;   | (english poetry ynt show n in public; \n",
      "10 of the books are believed to be the | 10 of the books are believed to be the\n",
      "only existing copies, and of a number  | only existing copies, and of a number \n",
      "of others only about half-a-dozen      | of others only about half a dozen     \n",
      "copies remain. The books have been     | copies remain. The books have been    \n",
      "chosen and procured for the exhibition | chosen and procured for the exhibition\n",
      "by Mr. John Hayward, who has also      | by Mr. John Hayward, who has also     \n",
      "compiled a detailed and comprehensive  | compiled a detailed and comprehensive \n",
      "catalogue. The books have. all been    | catalogue. The books have. All been   \n",
      "obtained from private collections in   | obtained from private collections in  \n",
      "this country and from college          | this country and from college         \n",
      "libraries in Oxford, Cambridge, and    | libraries in Oxford, Cambridge, and   \n",
      "London. Although many rare books have  | London. Although many rare books have \n",
      "gone to America in recent years there  | gone to America in recent years there \n",
      "remain enough books in private hands   | remain enough books in private hands  \n",
      "in Great Britain to make an exhibition | in Great Britain to make an exhibition\n",
      "of this kind possible, though original | of this kind possible, though original\n",
      "editions of English poetry are now     | editions of english poetry are now    \n",
      "hard to find; even a number of very    | hard to find; even a number of very   \n",
      "recent works have become extremely     | recent works have become extremely    \n",
      "rare. Most of the EnAlish poets are    | rare. Most of the english poets are   \n",
      "represented by a single volune, but    | represented by a single volume, but   \n",
      "two or more are allowed to the major   | two or more are allowed to the major  \n",
      "poets.                                 | poets.                                \n",
      "================================================================================\n",
      "\n",
      "--- Period: 1950 ---\n",
      "Min OOV Improvement: -1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "DATE WITH THE T. -THEATRE FROM OUR     | Date with the t. Theatre from our     \n",
      "SPECIAL CORRESPONDENT . CHICHESrER,    | special correspondent. ChicHEsrer, may\n",
      "MAY 3 To 'walk round 'Chichester       | 3 to'walk round'Chichester Festival   \n",
      "Festival Theatre now is to have the    | Theatre now is to have the feeling    \n",
      "feeling that when it opens on July 3   | that when it opens on july 3 Sir      \n",
      "Sir Laurence Olivier's company will    | Laurence Olivier's company will enter \n",
      "enter from one side and the audience   | from one side and the audience from   \n",
      "from another, to meet, as friends do,  | another, to meet, as friends do, as   \n",
      "as the first words are spoken, and to  | the first words are spoken, and to    \n",
      "spend the evening together, as friends | spend the evening together, as friends\n",
      "do, while the play lasts. The purpose  | do, while the play lasts. The purpose \n",
      "of the theatre is' that such a meeting | of the theatre is' that such a meeting\n",
      "shall take place all over the central  | shall take place all over the central \n",
      "area to which both the parties have    | area to which both the parties have   \n",
      "access. The open stage belongs to the  | access. The open stage belongs to the \n",
      "company and the remainder of the area  | company and the remainder of the area \n",
      "to the audience, but the theatre       | to the audience, but the theatre      \n",
      "itself, the cockpit, the wooden 0      | itself, the cockpit, the wooden 0     \n",
      "built of materials more up to date     | built of materials more up to date    \n",
      "than wood, belongs to them both. It    | than wood, belongs to them both. It   \n",
      "does so right up to a distance of 65   | does so right up to a distance of 65  \n",
      "feet from the outer rim of the         | feet from the outer rim of the        \n",
      "hexagonal stage, where the back row of | hexagonal stage, where the back row of\n",
      "the seats is placed below the lighting | the seats is placed below the lighting\n",
      "box. Leaving the foyer, you mount one  | box. Leaving the foyer, you mount one \n",
      "of four short flights of steps, just   | of four short flights of steps, just  \n",
      "as though you were an actor making an  | as though you were an actor making an \n",
      "entrance in the play, and as soon as   | entrance in the play, and as soon as  \n",
      "the stage comes into view you have a   | the stage comes into view you have a  \n",
      "sense of being \" there-\", of having    | sense of being \" there \", of having   \n",
      "arrived. You look up, and the dim      | arrived. You look up, and the dim     \n",
      "sight of girders spanning all parts of | sight of girders spanning all parts of\n",
      "the area, the actors' as well as the   | the area, the actors' as well as the  \n",
      "spectators, and carrying a hexagonal   | spectators, and carrying a hexagonal  \n",
      "girder which in turn supports the      | girder which in turn supports the     \n",
      "roof, confirms you in your impression  | roof, confirms you in your impression \n",
      "that the whole is already one thing.   | that the whole is already one thing.  \n",
      "WHAT TO EXPECT The boards of the stage | What to expect the boards of the stage\n",
      "need to be darkened to the colour of   | need to be darkened to the colour of  \n",
      "charcoal; more seats need be installed | charcoal; more seats need be installed\n",
      "and upholstered in midnight blue, more | and upholstered in midnight blue, more\n",
      "lengths of sage green carpeting need   | lengths of sage green carpeting need  \n",
      "to be spread. Hammers are still busy.  | to be spread. Hammers are still busy. \n",
      "But an appointment has already been    | But an appointment has already been   \n",
      "made in this place for audiences to    | made in this place for audiences to   \n",
      "meet actors in plays by Fletcher, Ford | meet actors in plays by Fletcher, Ford\n",
      "and Chekhov.                           | And ChEkhov.                          \n",
      "================================================================================\n",
      "\n",
      "Max OOV Improvement: 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "BBS RBI iiiiitliii ^ iii               | Bbs RBI iiiiiiiiii iii huffham bimm   \n",
      "HuffiHHMBipMMEnpn ||jH mum             | espn jh mum                           \n",
      "================================================================================\n",
      "\n",
      "--- Period: 2000 ---\n",
      "Min OOV Improvement: -1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "THE TIMES THURSDAY APRIL 6 2000        | THE TIMES thursday april 6 2000       \n",
      "THEATRE Yes, Yes, Yes BAC Ian Michaels | theatre yes, yes, yes bac Ian Michaels\n",
      "IT IS not every show where the         | it is not every show where the        \n",
      "audience is subjected to some random   | audience is subjected to some random  \n",
      "tickling and \"the world's greatest     | tickling and \" the world's greatest   \n",
      "waltzing kangaroo\" could land in your  | waltzing kangaroo\" could land in your \n",
      "lap. But then not every show is a      | lap. But then not every show is a     \n",
      "quest to find the meaning of life      | quest to find the meaning of life     \n",
      "through such questions as whether you  | through such questions as whether you \n",
      "can get your blood back from lice and  | can get your blood back from lice and \n",
      "is a woman worth it? Welcome to the    | is a woman worth it? Welcome to the   \n",
      "warped world of Ridiculusmus, a duo    | warped world of ridicUlusMus, a duo   \n",
      "with a taste for the chaotic and a     | with a taste for the chaotic and a    \n",
      "serious sense of the surreal. Their    | serious sense of the surreal. Their   \n",
      "particular brand of dark physical      | particular brand of dark physical     \n",
      "comedy, which in the past has captured | comedy, which in the past has captured\n",
      "some of the intoxicating flavour of    | some of the intoxicating flavour of   \n",
      "Flann O'Brien 's novels and shown us   | Flann O'Brien's novels and shown us   \n",
      "what art gallery attendants get up to  | what art gallery attendants get up to \n",
      "when they think no one is looking,     | when they think no one is looking,    \n",
      "might suggest that these curious       | might suggest that these curious      \n",
      "clowns need to be locked up. In their  | clowns need to be locked up. In their \n",
      "latest show, that appears to have      | latest show, that appears to have     \n",
      "happened. Yes, Yes, Yes is set in an   | happened. Yes, yes, yes is set in an  \n",
      "asylum in which two patients, a manic  | asylum in which two patients, a manic \n",
      "and a depressive, TELEPHONE 020 7680   | and a depressive, telepHone 020 7680  \n",
      "6224 OPERA & BALLET                    | 6224 operA BAllet                     \n",
      "================================================================================\n",
      "\n",
      "Max OOV Improvement: 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         N/A\n",
      "Title:          N/A\n",
      "Publication:    N/A\n",
      "Date:           N/A\n",
      "Place:          N/A\n",
      "URL:            N/A\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "How poetry can be found in translation | How poetry can be found in translation\n",
      "Impossible? Or just hugely rewarding?  | impossible? Or just hugely rewarding? \n",
      "A. S. Byatt hails the Times Stephen    | A. S. Byatt hails the times Stephen   \n",
      "Spender prize TO KNOW ANOTHER language | sPender prize to know another language\n",
      "is the best, and most exciting, way of | is the best, and most exciting, way of\n",
      "discovering the strengths — and        | discovering the strengths and         \n",
      "limitations — of the one with which    | limitations of the one with which one \n",
      "one has grown up. Writing within       | has grown up. Writing within          \n",
      "constraints — of length, rhythm, rhyme | constraints of length, rhythm, rhyme  \n",
      "perhaps, stress and repetition — is    | perhaps, stress and repetition is     \n",
      "another. Translating poetry is         | another. Translating poetry is        \n",
      "famously both impossible and wildly    | famously both impossible and wildly   \n",
      "rewarding and interesting. It is an    | rewarding and interesting. It is an   \n",
      "exercise both in exact reading and in  | exercise both in exact reading and in \n",
      "skilful writing. I have come to value  | skilful writing. I have come to value \n",
      "my own translators as my wisest        | my own translators as my wisest       \n",
      "readers — they ask searching questions | readers they ask searching questions  \n",
      "about precise meanings; they hear the  | about precise meanings; they hear the \n",
      "rhythms of long stretches of           | rhythms of long stretches of          \n",
      "interwoven writing, they send lists of | interwoven writing, they send lists of\n",
      "alternative translations of particular | alternative translations of particular\n",
      "words, all of which add a little       | words, all of which add a little      \n",
      "meaning in the other language here,    | meaning in the other language here,   \n",
      "and take it away there, all of which   | and take it away there, all of which  \n",
      "are possible, none of which are        | are possible, none of which are       \n",
      "perfect equivalents. Sometimes they    | perfect equivalents. Sometimes they   \n",
      "reveal things that I had not           | reveal things that i had not          \n",
      "understood or reflected on, strengths  | understood or reflected on, strengths \n",
      "and weaknesses of my English, meanings | and weaknesses of my english, meanings\n",
      "I had thought were self-evident, which | i had thought were self evident, which\n",
      "become truly problematic in Norwegian, | become truly problematic in norwegian,\n",
      "or Spanish, or Japanese. I think of    | or spanish, or japanese. I think of   \n",
      "translating poetry as a series of      | translating poetry as a series of     \n",
      "difficult decisions, one after the     | difficult decisions, one after the    \n",
      "other. If you love a poem, you love    | other. If you love a poem, you love   \n",
      "the sound of it in the inner ear, you  | the sound of it in the inner ear, you \n",
      "love the shape of it inside your head, | love the shape of it inside your head,\n",
      "the way each word changes all the      | the way each word changes all the     \n",
      "words round it, singing or muttering   | words round it, singing or muttering  \n",
      "or shouting. Putting it into English   | or shouting. Putting it into english  \n",
      "entails a whole series of difficult    | entails a whole series of difficult   \n",
      "decisions — do you try to keep a       | decisions do you try to keep a        \n",
      "strictly rhymed form,, or dq ypu go    | strictly rhymed form,, or dq ypu go   \n",
      "for faithful worcls in free verse?     | for faithful worcs in free verse? Have\n",
      "Have you any right to add an image of  | you any right to add an image of your \n",
      "your own as an equivalent image in     | own as an equivalent image in english \n",
      "English of some ancient Greek war cry  | of some ancient greek war cry or      \n",
      "or modern German street argot? Another | modern german street argot? Another   \n",
      "delightful aspect of translating into  | delightful aspect of translating into \n",
      "English is the doubleness of our       | english is the doubleness of our      \n",
      "inheritance — we so often have the     | inheritance we so often have the      \n",
      "choice between Germanic or Anglo-Saxon | choice between germanic or anglo saxon\n",
      "bluntness and Latin elegance. (Both    | bluntness and latin elegance. (both   \n",
      "qualities can be found in the other    | qualities can be found in the other   \n",
      "vocabulary also of course, Latin       | vocabulary also of course, latin      \n",
      "bluntness and Germanic elegance.) Our  | bluntness and germanic elegance.) our \n",
      "language is flexible, craggy, sinuous, | language is flexible, craggy, sinuous,\n",
      "abstract, airy, rapid, ponderous — and | abstract, airy, rapid, ponderous and  \n",
      "can be discovered in all these aspects | can be discovered in all these aspects\n",
      "in finding equivalents for strange and | in finding equivalents for strange and\n",
      "unEnglish words and thought forms. The | unenglish words and thought forms. The\n",
      "Times Stephen Spender prize for poetry | times Stephen spender prize for poetry\n",
      "in translation was created partly      | in translation was created partly     \n",
      "because its founders felt that         | because its founders felt that        \n",
      "languages were less and less taught in | languages were less and less taught in\n",
      "schools and universities, and that     | schools and universities, and that    \n",
      "translated literature was less and     | translated literature was less and    \n",
      "less read and talked about! At first,  | less read and talked about! At first, \n",
      "for these reasons, it was confined to  | for these reasons, it was confined to \n",
      "translators under the age of 30.1 am   | translators under the age of 30. 1 am \n",
      "glad to see that this has been         | glad to see that this has been        \n",
      "changed, with splendid results. The    | changed, with splendid results. The   \n",
      "age-range ofthe entrants in 2005 was   | age range ofthe entrants in 2005 was  \n",
      "ten to 94. Twenty-seven languages were | ten to 94. Twenty seven languages were\n",
      "offered, from Anglo-Saxon to Urdu,     | offered, from anglo saxon to urdu,    \n",
      "from Japanese to Tagalog, from Greek   | from japanese to tagalog, from greek  \n",
      "and Latin to Persian, Welsh and        | and latin to persian, welsh and       \n",
      "Esperanto. There are prizes for the    | esperanto. There are prizes for the   \n",
      "young — one for 14 and under, one for  | young one for 14 and under, one for 18\n",
      "18 and under, that encourage those at  | and under, that encourage those at    \n",
      "school to use the skills they are      | school to use the skills they are     \n",
      "learning. But there is no age limit on | learning. But there is no age limit on\n",
      "learning about language. A splendidly  | learning about language. A splendidly \n",
      "intelligent part of the prize is that  | intelligent part of the prize is that \n",
      "translators are asked for a commentary | translators are asked for a commentary\n",
      "on their poem and form of translation. | on their poem and form of translation.\n",
      "These are really illuminating. The     | These are really illuminating. The    \n",
      "whole idea is to make us think about   | whole idea is to make us think about  \n",
      "language, poetry, and the English      | language, poetry, and the english     \n",
      "language in particular. A good         | language in particular. A good        \n",
      "translator must make everything        | translator must make everything       \n",
      "problematic and strange, and then hear | problematic and strange, and then hear\n",
      "the poem singing again, Witfr a        | the poem singing again, witlr a       \n",
      "djife'rence.. ^,' ,,'.,,,              | diference.., ',, '.,,,                \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "stats, output_folder = evaluate_pipeline_oov(wordlist, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0ad49f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated LaTeX Table:\n",
      " \\begin{table}\n",
      "\\caption{Summary of OOV Correction Improvement across Historical Art Reviews}\n",
      "\\label{tab:oov_pipeline_summary}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\toprule\n",
      "Period & Count & Mean Imp. (%) & Median Imp. (%) & Std Dev \\\\\n",
      "\\midrule\n",
      "1750 & 422 & 0.81 & 0.84 & 0.15 \\\\\n",
      "1800 & 2542 & 0.86 & 0.89 & 0.12 \\\\\n",
      "1850 & 4260 & 0.81 & 0.84 & 0.12 \\\\\n",
      "1900 & 19977 & 0.68 & 0.71 & 0.21 \\\\\n",
      "1950 & 18050 & 0.53 & 0.54 & 0.21 \\\\\n",
      "2000 & 4993 & 0.48 & 0.48 & 0.17 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plot_oov_performance(stats, output_folder)\n",
    "plot_merged_performance(stats, output_folder)\n",
    "latex_table = export_to_latex(stats, output_folder)\n",
    "print(\"Generated LaTeX Table:\\n\", latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andreas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
