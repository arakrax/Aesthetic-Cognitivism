{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3602d209",
   "metadata": {},
   "source": [
    "### Comprehensive Text Pre-processing Pipeline\n",
    "\n",
    "This notebook implements a multi-stage pipeline designed to clean, correct, and standardize raw text extracted from historical documents, which often contain OCR errors. The process is broken down into four main stages, each applied sequentially to the data.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "\n",
    "1.  **Initial Cleaning (`clean_text_piece_batched`):**\n",
    "    *   This is the first pass over the raw text. It focuses on removing noise and restructuring the text into a more readable format.\n",
    "    *   **Actions:** Removes metadata tags, strips invalid characters, intelligently de-hyphenates words at the end of lines, joins lines into paragraphs, and applies basic punctuation spacing.\n",
    "\n",
    "2.  **Correction & Capitalization (`capitalization_and_correction_batched`):**\n",
    "    *   This is the core correction stage, which uses a Named Entity Recognition (NER) model to intelligently apply changes.\n",
    "    *   **Actions:**\n",
    "        *   Identifies and merges fragmented entities (e.g., \"Royal\", \"Academy\" -> \"Royal Academy\").\n",
    "        *   Applies sentence case to the entire text and properly capitalizes the identified entities.\n",
    "        *   Performs multiple passes of spelling and word segmentation correction, using the NER entities as a \"shield\" to prevent incorrect changes to names and places.\n",
    "\n",
    "3.  **Honorific Standardization (`final_polishing_batched`):**\n",
    "    *   This stage focuses on a specific type of token: titles and honorifics.\n",
    "    *   **Actions:** Standardizes titles like \"Mr\", \"sir\", \"Dr.\", etc., to a consistent format (e.g., `Mr.`, `Sir`, `Dr.`).\n",
    "\n",
    "4.  **Final Spacing Cleanup (`final_spacing_batched`):**\n",
    "    *   A final, non-destructive polishing step to ensure all punctuation and spacing is consistent after the previous modifications.\n",
    "    *   **Actions:** Re-applies rules to fix issues like \"word.word\" -> \"word. word\" or \"word . \" -> \"word.\".\n",
    "\n",
    "The `pipeline_driver` function at the end of the notebook orchestrates these four stages, processing all `.csv` files from an input directory and saving the results to an output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55998318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "import pickle\n",
    "import shutil\n",
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import importlib.resources\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bb045cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETE_WORDLIST = set()\n",
    "WORDLIST_NAME = 'complete_wordlist.pkl'\n",
    "DICTIONARY_DATA_DIR = './Dictionary_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e784cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading NER model: dslim/bert-large-NER...\n",
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "INFO:__main__:NER model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setup basic logging to see warnings\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "# It's better to load these outside the function so it's not done repeatedly\n",
    "model_name = \"dslim/bert-large-NER\"\n",
    "try:\n",
    "    log.info(f\"Loading NER model: {model_name}...\")\n",
    "    # Using AutoModel and AutoTokenizer allows for more control if needed later\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    # Using aggregation_strategy=\"simple\" helps merge B-TAG I-TAG sequences\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "    log.info(\"NER model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    log.error(f\"Failed to load NER model: {e}\")\n",
    "    # Handle error appropriately - maybe raise it or set pipeline to None\n",
    "    ner_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a551aff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words loaded: 495279\n"
     ]
    }
   ],
   "source": [
    "with open(DICTIONARY_DATA_DIR + WORDLIST_NAME, 'rb') as file:\n",
    "    COMPLETE_WORDLIST = pickle.load(file)\n",
    "print(\"Total unique words loaded:\", len(COMPLETE_WORDLIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6805e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_file(file_path):\n",
    "    try:\n",
    "        ds = load_dataset('csv', data_files=file_path)\n",
    "        file = Path(file_path)\n",
    "\n",
    "        return {'name': file.name, \n",
    "                'data': ds,\n",
    "                'length': ds.get('train').num_rows # ds is a dict like dataset while train is an arrow dataset\n",
    "                }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Caught error when loading csv file from path: {file_path} \\n\\n {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb7896",
   "metadata": {},
   "source": [
    "### Stage 1: Initial Text Cleaning (`clean_text_piece_batched`)\n",
    "\n",
    "This function performs several essential text cleaning steps on batches of text data, specifically designed for use with the `.map(batched=True)` method in datasets like Hugging Face's `datasets`. It focuses on improving readability and structure based on common issues found in OCR'd historical texts, without attempting to invent content.\n",
    "\n",
    "**Cleaning Steps Performed:**\n",
    "\n",
    "1.  **Metadata and Tag Removal:**\n",
    "    * Removes backslashes (`\\`).\n",
    "    * Removes `<NEWPAGE>` tags.\n",
    "    * *(Note: Placeholder for other specific metadata patterns if identified).*\n",
    "\n",
    "2.  **Initial Noise Removal:**\n",
    "    * *(Optional line filtering based on character count is currently commented out in the code, as it could interfere with de-hyphenation).*\n",
    "    * Removes characters that are not letters, numbers, basic punctuation (`.,;:?!'\"()`), the pound sign (`£`), or hyphens (`-`), replacing them with spaces.\n",
    "\n",
    "3.  **Line Break and Hyphenation Handling:**\n",
    "    * **Robust End-of-Line De-hyphenation:** Identifies and removes hyphens connecting word fragments across lines (e.g., `word-\\nfragment` or `word- \\n fragment`). This step uses a specific pattern (`([a-zA-Z])-\\s*\\n\\s*([a-zA-Z])`) and is applied repeatedly *before* general line joining to handle consecutive hyphenations accurately.\n",
    "    * **Paragraph/Sentence Joining:** After handling EOL hyphens, replaces all remaining newline characters (`\\n`) with spaces to join lines into continuous text blocks, aiming to reconstruct paragraphs disrupted by arbitrary line breaks.\n",
    "\n",
    "4.  **Specific OCR/Formatting Fixes (Rule-Based):**\n",
    "    * **Currency:** Corrects common OCR errors like ` l.` (space-l-dot followed by space/digit) to ` £`.\n",
    "    * **Contextual:** Includes *example* rules for correcting specific number misrecognitions like ` l00` to ` 100` and ` O0` to ` 00` when followed by space or punctuation. *(More rules should be added based on data analysis).*\n",
    "\n",
    "5.  **Punctuation and Formatting Refinement:**\n",
    "    * Ensures consistent spacing around punctuation marks (removes space before `.,;:?!`, ensures one space after).\n",
    "    * Standardises spacing around parentheses (`()`).\n",
    "    * Applies basic spacing around quotation marks (`'` `\"`).\n",
    "\n",
    "6.  **Whitespace Normalisation:**\n",
    "    * Collapses multiple consecutive spaces into single spaces.\n",
    "    * Removes leading and trailing whitespace from the final text.\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "```python\n",
    "# Assuming 'raw_dataset' is your dataset object with a 'Full_text' column\n",
    "# and the function 'clean_text_piece_batched' is defined.\n",
    "cleaned_dataset = raw_dataset.map(clean_text_piece_batched, batched=True)\n",
    "\n",
    "# The cleaned text will be in a new column named 'Cleaned_text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3baf2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spellchecker(master_word_set):\n",
    "    \"\"\"\n",
    "    Creates and configures a SymSpellPy instance by loading BOTH a\n",
    "    standard frequency dictionary AND your custom master word set.\n",
    "    \n",
    "    Crucially, it ONLY adds words from the master set if they\n",
    "    are NOT already present in the standard dictionary, preserving\n",
    "    the original frequencies of common words.\n",
    "    \"\"\"\n",
    "    log.info(f\"Setting up SymSpellPy...\")\n",
    "    symspell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "    # --- Step 1: Load the STANDARD frequency dictionary ---\n",
    "    try:\n",
    "        resource_path = importlib.resources.files(\"symspellpy\").joinpath(\"frequency_dictionary_en_82_765.txt\")\n",
    "        with importlib.resources.as_file(resource_path) as usable_path:\n",
    "            dictionary_path = str(usable_path)\n",
    "            if not os.path.exists(dictionary_path):\n",
    "                 log.error(\"SymSpell: Standard dictionary not found.\")\n",
    "                 return None\n",
    "            if not symspell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "                log.error(\"SymSpell: Could not load standard frequency dictionary.\")\n",
    "                return None\n",
    "        log.info(f\"Loaded standard frequency dictionary. Word count: {symspell.word_count}\")\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Could not load standard dictionary: {e}. Continuing with custom set only.\")\n",
    "\n",
    "    # --- Step 2: Load ONLY NEW words from YOUR master_word_set ---\n",
    "    # We give them a high, fake frequency so they are treated as valid.\n",
    "    # We use a count lower than the top words (like \"the\") but\n",
    "    # high enough to be a very strong suggestion.\n",
    "    custom_word_count = 1000000 # 1 million is a high, safe default\n",
    "    new_words_added = 0\n",
    "    \n",
    "    if master_word_set:\n",
    "        for word in master_word_set:\n",
    "            # Check if the word already exists in the dictionary\n",
    "            # lookup() with distance 0 is the way to check for existence\n",
    "            if not symspell.lookup(word, Verbosity.TOP, max_edit_distance=0):\n",
    "                # Word does NOT exist, so add it\n",
    "                symspell.create_dictionary_entry(word, custom_word_count)\n",
    "                new_words_added += 1\n",
    "            # If the word *does* exist, we do nothing and preserve its original frequency.\n",
    "            \n",
    "    log.info(f\"Added {new_words_added} new unique words from master set to SymSpell.\")\n",
    "    log.info(f\"SymSpell setup complete. Total unique terms: {symspell.word_count}\")\n",
    "    return symspell\n",
    "\n",
    "# (You will still need your preserve_case function)\n",
    "def preserve_case(original_word, new_word):\n",
    "    \"\"\"Matches the case of the new word to the original word.\"\"\"\n",
    "    if original_word.isupper():\n",
    "        return new_word.upper()\n",
    "    if original_word.istitle():\n",
    "        return new_word.title()\n",
    "    if original_word[0].isupper() and (len(original_word) == 1 or original_word[1:].islower()):\n",
    "        return new_word.title()\n",
    "    return new_word.lower()\n",
    "\n",
    "\n",
    "def correct_spelling_safe(text, ner_entities, symspell, master_word_set, \n",
    "                          max_edit_distance=1, segmentation_threshold=12, \n",
    "                          segmentation_ratio=5):\n",
    "    \"\"\"\n",
    "    Corrects misspellings in a text string, using NER entities as a \"shield\".\n",
    "    \n",
    "    This function now includes a 3-stage check:\n",
    "    1. Check against master word set.\n",
    "    2. If unknown, check if NER-shielded.\n",
    "    3. If not shielded, try 1-edit-distance correction.\n",
    "    4. If that fails AND THE WORD IS LONG, try word_segmentation() as a fallback.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text after capitalization (output of apply_capitalization).\n",
    "        ner_entities (list): The list of merged/filtered NER entities for this text.\n",
    "        master_word_set (set): The set of all known valid words.\n",
    "        symspell (SymSpell): The pre-configured SymSpellPy instance.\n",
    "        max_edit_distance (int): The max distance for simple 1-to-1 corrections.\n",
    "        segmentation_threshold (int): Words *longer* than this will be checked\n",
    "                                      for segmentation errors (e.g., 'exhibitioanof').\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with safe spelling corrections applied.\n",
    "    \"\"\"\n",
    "    # Skip if spellchecker isn't configured\n",
    "    if symspell is None or master_word_set is None or not text:\n",
    "        return text\n",
    "\n",
    "    # 1. Create a \"shield\" set of all indices that are part of a NER entity\n",
    "    entity_indices = set()\n",
    "    for ent in ner_entities:\n",
    "        entity_indices.update(range(ent['start'], ent['end']))\n",
    "\n",
    "    # 2. Iterate through words and build the corrected text piece by piece\n",
    "    corrected_parts = []\n",
    "    last_index = 0\n",
    "    # This regex finds words, including those with apostrophes\n",
    "    word_pattern = re.compile(r\"\\b[a-zA-Z]+(?:'[a-zA-Z]+)?\\b\")\n",
    "\n",
    "    for match in word_pattern.finditer(text):\n",
    "        word = match.group(0)\n",
    "        word_lower = word.lower()\n",
    "        match_start = match.start()\n",
    "        match_end = match.end()\n",
    "        \n",
    "        corrected_word = word # Default: no change\n",
    "\n",
    "        # --- Correction Logic ---\n",
    "        \n",
    "        # 3. Check if word is known\n",
    "        if word_lower not in master_word_set:\n",
    "            \n",
    "            # 4. If unknown, check if it's \"shielded\" by NER\n",
    "            # We check if *any* part of the word is shielded\n",
    "            is_shielded = any(idx in entity_indices for idx in range(match_start, match_end))\n",
    "            \n",
    "            if not is_shielded:\n",
    "                # 5. Word is UNKNOWN and NOT shielded. Try to correct it.\n",
    "                \n",
    "                # --- STAGE 1: Try 1-edit distance (for simple typos like 'wvas') ---\n",
    "                suggestions_dist1 = symspell.lookup(\n",
    "                    word,\n",
    "                    Verbosity.CLOSEST,\n",
    "                    max_edit_distance=max_edit_distance, \n",
    "                    include_unknown=False\n",
    "                )\n",
    "                \n",
    "                if len(suggestions_dist1) == 1:\n",
    "                    # Success with 1-edit distance\n",
    "                    correction = suggestions_dist1[0].term\n",
    "                    corrected_word = preserve_case(word, correction)\n",
    "                    # log.info(f\"Spellcheck 1-edit: Corrected '{word}' -> '{corrected_word}'\")\n",
    "                \n",
    "                else:\n",
    "                    # --- STAGE 2: 1-edit failed. Try segmentation (for merged words) ---\n",
    "                    # Check if the word is LONG enough to be a likely merge.\n",
    "                    if len(word) > segmentation_threshold:\n",
    "                        # Calculate a dynamic edit distance budget\n",
    "                        # (e.g., 1 edit for every 5 chars)\n",
    "                        dynamic_max_edits = len(word) // segmentation_ratio\n",
    "                        # Ensure a minimum budget of 2 for segmentation\n",
    "                        if dynamic_max_edits < 2:\n",
    "                            dynamic_max_edits = 2\n",
    "\n",
    "                        try:\n",
    "                            # Run segmentation on *only this word*\n",
    "                            segment_suggestion = symspell.word_segmentation(\n",
    "                                word,\n",
    "                                max_edit_distance=dynamic_max_edits # Allow more edits\n",
    "                            )\n",
    "                            \n",
    "                            # Check if a valid, *different* segmentation was found\n",
    "                            if segment_suggestion.corrected_string and \\\n",
    "                               segment_suggestion.corrected_string.lower() != word_lower:\n",
    "                                \n",
    "                                corrected_word = preserve_case(word, segment_suggestion.corrected_string)\n",
    "                                # log.info(f\"Spellcheck Segment: Corrected '{word}' -> '{corrected_word}'\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            log.error(f\"Error during word_segmentation on '{word}': {e}\")\n",
    "                            # Keep original word if segmentation fails\n",
    "\n",
    "        # 6. Append the text *before* this word, then the (possibly corrected) word\n",
    "        corrected_parts.append(text[last_index:match_start])\n",
    "        corrected_parts.append(corrected_word)\n",
    "        last_index = match_end\n",
    "\n",
    "    # 7. Append any remaining text after the last word\n",
    "    corrected_parts.append(text[last_index:])\n",
    "    \n",
    "    return \"\".join(corrected_parts)\n",
    "\n",
    "def correct_segmentation_errors_safe(text, ner_entities, symspell, max_edit_distance=2):\n",
    "    \"\"\"\n",
    "    Corrects segmentation errors by processing chunks *between* NER entities.\n",
    "    It further splits these chunks to isolate and correct *word-only* sections,\n",
    "    avoiding errors from numbers and symbols.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to correct (output of 1-to-1 correction).\n",
    "        ner_entities (list): The list of merged/filtered NER entities for this text.\n",
    "        symspell (SymSpell): The pre-configured SymSpellPy instance.\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with segmentation corrections applied.\n",
    "    \"\"\"\n",
    "    if symspell is None or not text:\n",
    "        return text\n",
    "\n",
    "    # Sort entities by start index to process them in order\n",
    "    sorted_entities = sorted(ner_entities, key=lambda x: x['start'])\n",
    "    \n",
    "    corrected_fragments = []\n",
    "    last_index = 0 # Our cursor for slicing the text\n",
    "\n",
    "    # --- This regex splits a string by anything that ISN'T a letter, \n",
    "    # apostrophe, or space, but it KEEPS the delimiter.\n",
    "    # It finds numbers (0-9), symbols (like ()-), and punctuation (\\.,;:?!).\n",
    "    splitter_pattern = re.compile(r\"([^a-zA-Z'\\s]+)\")\n",
    "\n",
    "    for entity in sorted_entities:\n",
    "        start_index = entity['start']\n",
    "        \n",
    "        # --- 1. Process the \"Safe Chunk\" BEFORE the entity ---\n",
    "        if start_index > last_index:\n",
    "            safe_chunk_before_entity = text[last_index:start_index]\n",
    "            \n",
    "            # Now, split this chunk by numbers/symbols\n",
    "            corrected_sub_fragments = []\n",
    "            sub_fragments = splitter_pattern.split(safe_chunk_before_entity)\n",
    "            \n",
    "            for sub_fragment in sub_fragments:\n",
    "                if not sub_fragment:\n",
    "                    continue\n",
    "                \n",
    "                # Check if the sub_fragment is a delimiter or \"pure text\"\n",
    "                # We check if it's ONLY letters, apostrophes, and spaces.\n",
    "                if re.fullmatch(r\"[a-zA-Z'\\s]*\", sub_fragment):\n",
    "                    # It's \"pure text\". This is safe for lookup_compound.\n",
    "                    try:\n",
    "                        suggestions = symspell.lookup_compound(\n",
    "                            sub_fragment,\n",
    "                            max_edit_distance=max_edit_distance,\n",
    "                            ignore_non_words=False,\n",
    "                            transfer_casing=True\n",
    "                        )\n",
    "                        if suggestions:\n",
    "                            corrected_sub_fragments.append(suggestions[0].term)\n",
    "                        else:\n",
    "                            corrected_sub_fragments.append(sub_fragment)\n",
    "                    except Exception as e:\n",
    "                        # Catch the string index error specifically\n",
    "                        log.error(f\"Error during lookup_compound on sub-fragment '{sub_fragment}': {e}\")\n",
    "                        corrected_sub_fragments.append(sub_fragment)\n",
    "                else:\n",
    "                    # It's a delimiter (like '(', '8300', '.'), append it as-is\n",
    "                    corrected_sub_fragments.append(sub_fragment)\n",
    "\n",
    "            # Re-join the processed sub-fragments for this chunk\n",
    "            corrected_fragments.append(\"\".join(corrected_sub_fragments))\n",
    "\n",
    "        # --- 2. Append the \"Shielded Entity\" as-is ---\n",
    "        if entity['end'] > last_index:\n",
    "            entity_chunk = text[entity['start']:entity['end']]\n",
    "            corrected_fragments.append(entity_chunk)\n",
    "            # Move our cursor to the end of this entity\n",
    "            last_index = entity['end']\n",
    "\n",
    "    # --- 3. Process the Final \"Safe Chunk\" after the last entity ---\n",
    "    if last_index < len(text):\n",
    "        final_safe_chunk = text[last_index:]\n",
    "        \n",
    "        # Repeat the same sub-fragment logic for the final chunk\n",
    "        corrected_sub_fragments = []\n",
    "        sub_fragments = splitter_pattern.split(final_safe_chunk)\n",
    "        \n",
    "        for sub_fragment in sub_fragments:\n",
    "            if not sub_fragment:\n",
    "                continue\n",
    "            if re.fullmatch(r\"[a-zA-Z'\\s]*\", sub_fragment):\n",
    "                # It's \"pure text\".\n",
    "                try:\n",
    "                    suggestions = symspell.lookup_compound(\n",
    "                        sub_fragment,\n",
    "                        max_edit_distance=max_edit_distance,\n",
    "                        ignore_non_words=False,\n",
    "                        transfer_casing=True\n",
    "                    )\n",
    "                    if suggestions:\n",
    "                        corrected_sub_fragments.append(suggestions[0].term)\n",
    "                    else:\n",
    "                        corrected_sub_fragments.append(sub_fragment)\n",
    "                except Exception as e:\n",
    "                    log.error(f\"Error during lookup_compound on final sub-fragment '{sub_fragment}': {e}\")\n",
    "                    corrected_sub_fragments.append(sub_fragment)\n",
    "            else:\n",
    "                # It's a delimiter.\n",
    "                corrected_sub_fragments.append(sub_fragment)\n",
    "        \n",
    "        corrected_fragments.append(\"\".join(corrected_sub_fragments))\n",
    "    \n",
    "    # --- 4. Re-join all fragments ---\n",
    "    return \"\".join(corrected_fragments)\n",
    "\n",
    "def fix_targeted_segmentation(text, ner_entities, symspell, max_edit_distance=2):\n",
    "    \"\"\"\n",
    "    Finds and corrects *only* specific, targeted segmentation errors\n",
    "    (like \"wa ter\") using regex and SymSpellPy, AND SKIPPING matches\n",
    "    that overlap with NER entities.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to correct (output of 1-to-1 correction).\n",
    "        ner_entities (list): The list of merged/filtered NER entities for this text.\n",
    "        symspell (SymSpell): The pre-configured SymSpellPy instance.\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with targeted segmentation corrections.\n",
    "    \"\"\"\n",
    "    if symspell is None or not text:\n",
    "        return text\n",
    "\n",
    "    # --- Step 1: Create the \"shield\" set of all indices ---\n",
    "    entity_indices = set()\n",
    "    for ent in ner_entities:\n",
    "        entity_indices.update(range(ent['start'], ent['end']))\n",
    "\n",
    "    # Define the patterns (excluding the risky p3)\n",
    "    p1 = r\"\\b([a-zA-Z']{1,2})\\s([a-zA-Z']{3,})\\b\"\n",
    "    p2 = r\"\\b([a-zA-Z']{3,})\\s([a-zA-Z']{1,2})\\b\"\n",
    "    candidate_pattern = re.compile(f\"({p1})|({p2})\", flags=re.IGNORECASE)\n",
    "    \n",
    "    matches = list(candidate_pattern.finditer(text))\n",
    "    corrected_text = text\n",
    "    \n",
    "    for match in reversed(matches):\n",
    "        original_phrase = match.group(0)\n",
    "        start_index = match.start()\n",
    "        end_index = match.end()\n",
    "\n",
    "        # --- Step 2: The Shield Check ---\n",
    "        # Check if *any* part of this match overlaps with the NER shield\n",
    "        is_shielded = False\n",
    "        for i in range(start_index, end_index):\n",
    "            if i in entity_indices:\n",
    "                is_shielded = True\n",
    "                break\n",
    "        \n",
    "        if is_shielded:\n",
    "            # log.info(f\"Skipping shielded phrase: '{original_phrase}'\")\n",
    "            continue # This match is part of a NER entity, do not correct.\n",
    "\n",
    "        # --- Step 3: The Decider (SymSpellPy) ---\n",
    "        # (This part is only reached if the phrase is NOT shielded)\n",
    "        try:\n",
    "            suggestions = symspell.lookup_compound(\n",
    "                original_phrase,\n",
    "                max_edit_distance=max_edit_distance,\n",
    "                transfer_casing=True\n",
    "            )\n",
    "            \n",
    "            if suggestions:\n",
    "                best_suggestion = suggestions[0].term\n",
    "                if best_suggestion != original_phrase and \" \" not in best_suggestion:\n",
    "                    corrected_text = (\n",
    "                        corrected_text[:start_index] + \n",
    "                        best_suggestion + \n",
    "                        corrected_text[end_index:]\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during lookup_compound on phrase '{original_phrase}': {e}\")\n",
    "            # Continue without correcting this phrase\n",
    "\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4ab51904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_piece_batched(batch):\n",
    "    \"\"\"\n",
    "    Cleans a batch of text pieces, incorporating initial normalization,\n",
    "    **robust end-of-line de-hyphenation**, line break handling,\n",
    "    currency/contextual fixes, and punctuation spacing.\n",
    "    Designed for dataset.map(batched=True).\n",
    "    \"\"\"\n",
    "    input_texts = batch['Full_text']\n",
    "    cleaned_texts = []\n",
    "\n",
    "    # --- Pre-compiled regex patterns ---\n",
    "    # Step 1: Metadata/Tags\n",
    "    backslash_re = re.compile(r'\\\\')\n",
    "    newpage_re = re.compile(r'<NEWPAGE>')\n",
    "\n",
    "    # Step 2: Junk Characters\n",
    "    junk_char_re = re.compile(r'[^a-zA-Z0-9\\s\\.,;:?!£\\'\"()-]')\n",
    "\n",
    "    # Step 3: De-hyphenation (End-of-Line specific)\n",
    "    # Looks for letter, hyphen, optional space, newline, optional space, letter\n",
    "    dehyphen_eol_re = re.compile(r'([a-zA-Z])-\\s*\\n\\s*([a-zA-Z])')\n",
    "\n",
    "    # Step 3: General Line Joining\n",
    "    newline_re = re.compile(r'\\n')\n",
    "\n",
    "    # Step 4: Currency & Contextual Fixes\n",
    "    currency_l_dot_re = re.compile(r'\\s+l\\.(?=\\s+|\\d)')\n",
    "    num_l00_re = re.compile(r'\\sl00(?=\\s|[,.])')\n",
    "    num_O0_re = re.compile(r'\\sO0(?=\\s|[,.])')\n",
    "    # Add more specific rules based on your data...\n",
    "\n",
    "    # Step 5: Punctuation Spacing\n",
    "    space_before_punct_re = re.compile(r'\\s+([,\\.;:?!])')\n",
    "    space_after_punct_re = re.compile(r'([,\\.;:?!])\\s*')\n",
    "    space_around_brackets_open_re = re.compile(r'\\s*\\(\\s*')\n",
    "    space_around_brackets_close_re = re.compile(r'\\s*\\)\\s*')\n",
    "    space_around_quotes_re = re.compile(r'\\s*([\"\\'])\\s*')\n",
    "\n",
    "    # Final whitespace cleanup\n",
    "    multi_space_re = re.compile(r'\\s+')\n",
    "    # --- End of pre-compiled patterns ---\n",
    "\n",
    "\n",
    "    for text in input_texts:\n",
    "        # 1. Remove distracting metadata and tags\n",
    "        cleaned_text = backslash_re.sub('', text)\n",
    "        cleaned_text = newpage_re.sub('', cleaned_text)\n",
    "\n",
    "        # Optional: Filter lines (consider removing if it affects hyphenation)\n",
    "        # lines = cleaned_text.split('\\n')\n",
    "        # cleaned_lines = [line for line in lines if len(re.findall(r'[a-zA-Z]', line)) > 5]\n",
    "        # cleaned_text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "        # 2. Initial Noise Removal (Junk Characters)\n",
    "        cleaned_text = junk_char_re.sub(' ', cleaned_text)\n",
    "\n",
    "        # --- Step 3: Handle Line Breaks ---\n",
    "        # 3a. De-hyphenate End-of-Line words repeatedly\n",
    "        new_text = cleaned_text\n",
    "        while True:\n",
    "            processed_text = dehyphen_eol_re.sub(r'\\1\\2', new_text)\n",
    "            if processed_text == new_text: # No more changes\n",
    "                break\n",
    "            new_text = processed_text # Update for next iteration\n",
    "        cleaned_text = new_text\n",
    "\n",
    "        # 3b. Join remaining lines into paragraphs/sentences\n",
    "        cleaned_text = newline_re.sub(' ', cleaned_text)\n",
    "        # --- End of Step 3 ---\n",
    "\n",
    "\n",
    "        # 4. Apply Currency & Contextual Fixes\n",
    "        cleaned_text = currency_l_dot_re.sub(' £', cleaned_text)\n",
    "        cleaned_text = num_l00_re.sub(' 100', cleaned_text)\n",
    "        cleaned_text = num_O0_re.sub(' 00', cleaned_text)\n",
    "        # Add more specific rules here...\n",
    "\n",
    "        # 5. Refine Punctuation Spacing\n",
    "        cleaned_text = space_before_punct_re.sub(r'\\1', cleaned_text)\n",
    "        cleaned_text = space_after_punct_re.sub(r'\\1 ', cleaned_text)\n",
    "        cleaned_text = space_around_brackets_open_re.sub(' (', cleaned_text)\n",
    "        cleaned_text = space_around_brackets_close_re.sub(') ', cleaned_text)\n",
    "        cleaned_text = space_around_quotes_re.sub(r' \\1 ', cleaned_text) # Basic quote spacing\n",
    "\n",
    "        # Final Cleanup\n",
    "        cleaned_text = multi_space_re.sub(' ', cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    return {'Cleaned_text': cleaned_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d7db91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities_generic(ner_results, text, entity_types_to_merge={'PER', 'ORG', 'LOC'}):\n",
    "    \"\"\"\n",
    "    Merges adjacent, fragmented entities (PER, ORG, LOC by default)\n",
    "    from a NER model's output if separated by common connectors or whitespace.\n",
    "    Handles initials specifically for PER entities.\n",
    "\n",
    "    Args:\n",
    "        ner_results (list): The raw output from the NER pipeline (ideally\n",
    "                            with an aggregation strategy like \"simple\" already applied).\n",
    "        text (str): The original text that was analyzed.\n",
    "        entity_types_to_merge (set): A set of entity group labels to consider for merging\n",
    "                                     (e.g., {'PER', 'ORG', 'LOC'}).\n",
    "\n",
    "    Returns:\n",
    "        list: A new list of entities with fragmented ones merged.\n",
    "    \"\"\"\n",
    "    if not ner_results:\n",
    "        return []\n",
    "\n",
    "    merged_results = []\n",
    "    i = 0\n",
    "    while i < len(ner_results):\n",
    "        current_entity = ner_results[i]\n",
    "\n",
    "        # Check if the current entity is one we want to merge and if there's a next entity\n",
    "        if current_entity['entity_group'] in entity_types_to_merge and i + 1 < len(ner_results):\n",
    "            next_entity = ner_results[i+1]\n",
    "\n",
    "            # --- MERGING LOGIC ---\n",
    "            # Condition 1: Check if the next entity is of the SAME type\n",
    "            if next_entity['entity_group'] == current_entity['entity_group']:\n",
    "                # Get the text between the two entities\n",
    "                start = current_entity['end']\n",
    "                end = next_entity['start']\n",
    "                text_between = text[start:end]\n",
    "                stripped_text_between = text_between.strip()\n",
    "\n",
    "                should_merge = False\n",
    "                # Merge based on simple separators ('and', '&', space/nothing)\n",
    "                if stripped_text_between in ['and', '&', '']:\n",
    "                    should_merge = True\n",
    "\n",
    "                # Merge initials specifically for PER entities\n",
    "                elif current_entity['entity_group'] == 'PER' and \\\n",
    "                     current_entity['word'].endswith('.') and \\\n",
    "                     len(current_entity['word'].strip('.')) <= 1 and \\\n",
    "                     stripped_text_between == '':\n",
    "                    # Ensure space is added between initials\n",
    "                     text_between = \" \"\n",
    "                     should_merge = True\n",
    "\n",
    "                if should_merge:\n",
    "                    # Combine words, average scores, update indices\n",
    "                    # Add space if original separation was just whitespace but not empty\n",
    "                    if stripped_text_between == '' and text_between != '':\n",
    "                        new_word = current_entity['word'] + ' ' + next_entity['word']\n",
    "                    else:\n",
    "                        new_word = current_entity['word'] + text_between + next_entity['word']\n",
    "\n",
    "                    new_score = (current_entity['score'] + next_entity['score']) / 2\n",
    "\n",
    "                    # Create a new merged entity dictionary\n",
    "                    merged_entity = {\n",
    "                        'entity_group': current_entity['entity_group'],\n",
    "                        'word': new_word.replace(\" ##\", \"\"), # Clean up sub-word tokens if necessary\n",
    "                        'score': new_score,\n",
    "                        'start': current_entity['start'],\n",
    "                        'end': next_entity['end']\n",
    "                    }\n",
    "\n",
    "                    # Replace the current entity with the merged one for the next iteration/append step\n",
    "                    current_entity = merged_entity\n",
    "                    # Increment 'i' to skip the next entity that we just merged\n",
    "                    i += 1\n",
    "\n",
    "        # Append the current (potentially merged) entity to the results\n",
    "        merged_results.append(current_entity)\n",
    "        i += 1 # Move to the next entity\n",
    "\n",
    "    # --- Final Filtering ---\n",
    "    # Keep only the desired entity types and apply basic length filtering\n",
    "    # Allows single initials ending with '.' for PER, otherwise length > 1\n",
    "    final_results = [\n",
    "        ent for ent in merged_results\n",
    "        if ent['entity_group'] in entity_types_to_merge and \\\n",
    "           (len(ent['word'].strip()) > 1 or \\\n",
    "            (ent['entity_group'] == 'PER' and ent['word'].strip().endswith('.') and len(ent['word'].strip()) <= 2)\n",
    "           )\n",
    "    ]\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "34e32744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_capitalization(text, ner_entities):\n",
    "    \"\"\"\n",
    "    Applies sentence case and capitalizes recognized entities,\n",
    "    handling common abbreviations/acronyms as special cases (keeping them uppercase).\n",
    "    \"\"\"\n",
    "    if not text or text.isspace():\n",
    "        return text\n",
    "\n",
    "    # Convert text to list of characters for easier modification\n",
    "    text_chars = list(text.lower())\n",
    "\n",
    "    # --- Apply Sentence Capitalization ---\n",
    "    capitalize_next = True # Capitalize the very first char\n",
    "    for i, char in enumerate(text_chars):\n",
    "        if capitalize_next and char.isalpha():\n",
    "            text_chars[i] = char.upper()\n",
    "            capitalize_next = False\n",
    "        # Capitalize after sentence-ending punctuation followed by space\n",
    "        elif char in '.?!' and i + 1 < len(text_chars) and text_chars[i+1].isspace():\n",
    "             capitalize_next = True\n",
    "        # Don't capitalize if the next char isn't a letter or if inside a word\n",
    "        elif not char.isspace():\n",
    "            capitalize_next = False\n",
    "    # Ensure first letter is capitalized even if text starts with non-alpha\n",
    "    for i, char in enumerate(text_chars):\n",
    "        if char.isalpha():\n",
    "            text_chars[i] = char.upper()\n",
    "            break\n",
    "\n",
    "\n",
    "    # --- Apply Entity Capitalization ---\n",
    "    sorted_entities = sorted(ner_entities, key=lambda x: x['start'])\n",
    "    modified_indices = set()\n",
    "\n",
    "    # --- Define known abbreviations or patterns ---\n",
    "    # More robust: list of specific lowercased abbreviations to keep upper\n",
    "    known_abbreviations = {'llc', 'ltd', 'inc', 'co', 'plc', 'corp',\n",
    "                           'ra', 'mp', 'esq', 'bart', 'kbe', 'cbe', 'obe', 'mbe', # Titles/Honours\n",
    "                           'mph', 'kph' # Units often uppercase\n",
    "                           }\n",
    "    # Regex for patterns like X.Y. or X.Y.Z. (allows letters only)\n",
    "    initials_pattern = re.compile(r'^([A-Z]\\.\\s?)+$')\n",
    "    # Regex for simple all-caps words (e.g., BBC, NATO) - adjust length as needed\n",
    "    all_caps_pattern = re.compile(r'^[A-Z]{2,}$')\n",
    "\n",
    "\n",
    "    for entity in sorted_entities:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        entity_group = entity['entity_group'] # PER, ORG, LOC\n",
    "        original_word = entity['word'] # Get the word as identified by NER\n",
    "\n",
    "        # Basic overlap check\n",
    "        if any(idx in modified_indices for idx in range(start, end)):\n",
    "            continue\n",
    "\n",
    "        current_span_list = text_chars[start:end]\n",
    "        if not current_span_list:\n",
    "            continue\n",
    "        current_span_lower = \"\".join(current_span_list) # This is already lowercase\n",
    "\n",
    "        # --- Capitalization Rules ---\n",
    "        capitalized_span = \"\"\n",
    "\n",
    "        # Rule 1: Check against known abbreviations list (case-insensitive check)\n",
    "        # We check the version without trailing punctuation if present\n",
    "        check_word = current_span_lower.rstrip('.,;:?!')\n",
    "        if check_word in known_abbreviations:\n",
    "            capitalized_span = current_span_lower.upper()\n",
    "\n",
    "        # Rule 2: Check for initials pattern (e.g., R.A., M.P.) using original word\n",
    "        # Needs original word casing info\n",
    "        elif initials_pattern.match(original_word):\n",
    "             capitalized_span = current_span_lower.upper() # Keep uppercase\n",
    "\n",
    "        # Rule 3: Check for simple all-caps pattern (e.g., BBC) using original word\n",
    "        elif entity_group == 'ORG' and all_caps_pattern.match(original_word):\n",
    "             capitalized_span = current_span_lower.upper() # Keep uppercase\n",
    "\n",
    "        # Rule 4: Default - Title Case\n",
    "        else:\n",
    "            capitalized_span = current_span_lower.title()\n",
    "            # Post-title case fixes (e.g., McDonald -> McDonald) could go here if needed\n",
    "            # Example:\n",
    "            # if 'mc' in capitalized_span.lower() and capitalized_span.startswith('Mc'):\n",
    "            #     mc_index = capitalized_span.lower().find('mc')\n",
    "            #     if mc_index + 2 < len(capitalized_span):\n",
    "            #         third_char = capitalized_span[mc_index + 2]\n",
    "            #         if third_char.islower(): # Already title cased like Mcdonald\n",
    "            #              capitalized_span = capitalized_span[:mc_index+2] + third_char.upper() + capitalized_span[mc_index+3:]\n",
    "\n",
    "\n",
    "        # --- Apply the change ---\n",
    "        if len(capitalized_span) == len(current_span_list):\n",
    "            text_chars[start:end] = list(capitalized_span)\n",
    "            modified_indices.update(range(start, end))\n",
    "        else:\n",
    "            log.warning(f\"Length mismatch during capitalization for entity: '{entity['word']}' \"\n",
    "                        f\"Original span: '{current_span_lower}', Capitalized: '{capitalized_span}'\")\n",
    "\n",
    "    return \"\".join(text_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalization_and_correction_batched(batch, text_column=\"Cleaned_text\", master_word_set=COMPLETE_WORDLIST, symspell=None):\n",
    "    \"\"\"\n",
    "    Applies a multi-step, NER-aware correction and capitalization process.\n",
    "\n",
    "    This function orchestrates the following steps for each text entry:\n",
    "    1. Merges fragmented NER entities (e.g., 'Royal' 'Academy' -> 'Royal Academy').\n",
    "    2. Applies sentence case to the text and capitalizes the merged entities.\n",
    "    3. Performs a first pass of NER-shielded spelling correction for simple typos.\n",
    "    4. Corrects targeted segmentation errors (e.g., 'wa ter' -> 'water').\n",
    "    5. Corrects general segmentation errors (e.g., 'theexhibition' -> 'the exhibition').\n",
    "    6. Performs a final, more aggressive spelling correction pass on the result.\n",
    "    \n",
    "    Designed for dataset.map(batched=True).\n",
    "    \"\"\"\n",
    "    if ner_pipeline is None:\n",
    "        log.error(\"NER pipeline not loaded. Cannot perform capitalization.\")\n",
    "        return {f\"Corrected_{text_column}\": batch[text_column]}\n",
    "\n",
    "    input_texts = batch[text_column]\n",
    "    final_texts = []\n",
    "\n",
    "    try:\n",
    "        all_ner_raw_results = ner_pipeline(input_texts)\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error during NER processing: {e}\")\n",
    "        return {f\"Corrected_{text_column}\": input_texts}\n",
    "\n",
    "    if len(all_ner_raw_results) != len(input_texts):\n",
    "         log.error(f\"NER results/input length mismatch. Skipping batch.\")\n",
    "         return {f\"Corrected_{text_column}\": input_texts}\n",
    "\n",
    "    # Process each text individually\n",
    "    for i, text in enumerate(input_texts):\n",
    "        if not text or text.isspace():\n",
    "             final_texts.append(text)\n",
    "             continue\n",
    "\n",
    "        ner_results_for_text = all_ner_raw_results[i]\n",
    "\n",
    "        # --- Step 1: Merge Entities ---\n",
    "        merged_entities = merge_entities_generic(ner_results_for_text, text)\n",
    "\n",
    "        # --- Step 2: Apply Capitalization ---\n",
    "        capitalized_text = apply_capitalization(text, merged_entities)\n",
    "        \n",
    "        # --- Step 3: Apply Safe 1-to-1 Spell Correction ---\n",
    "        # This fixes simple typos like \"wvas\" -> \"was\"\n",
    "        corrected_text_simple = correct_spelling_safe(\n",
    "            capitalized_text, \n",
    "            merged_entities, \n",
    "            symspell,\n",
    "            master_word_set            \n",
    "        )\n",
    "        \n",
    "# Step 4: Apply *Targeted* Segmentation Correction (now also shielded)\n",
    "        # This fixes things like \"wa ter\"\n",
    "        targeted_segmentation_fixes = fix_targeted_segmentation(\n",
    "            corrected_text_simple,\n",
    "            merged_entities, # <-- Pass the shield\n",
    "            symspell\n",
    "        )\n",
    "        \n",
    "        general_segmentation_fixes = correct_segmentation_errors_safe(\n",
    "            targeted_segmentation_fixes,\n",
    "            merged_entities,      # <-- Pass the shield\n",
    "            symspell\n",
    "        )\n",
    "\n",
    "        final_corrected_text = correct_spelling_safe(\n",
    "            general_segmentation_fixes, \n",
    "            merged_entities, \n",
    "            symspell,\n",
    "            master_word_set,\n",
    "            max_edit_distance=2, # Allow more edits for final pass            \n",
    "            segmentation_ratio=3, # Lower threshold for final pass\n",
    "        )\n",
    "        \n",
    "        \n",
    "        final_texts.append(final_corrected_text)\n",
    "\n",
    "    # Use a new output column name\n",
    "    return {f\"Corrected_{text_column}\": final_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ce290d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_honorifics(text, style='dot'):\n",
    "    \"\"\"\n",
    "    Standardizes common honorifics (Mr, Mrs, Sir, etc.) to a consistent\n",
    "    capitalization and punctuation style, running as a final polish.\n",
    "    \n",
    "    This function should be applied *after* all NER and spelling corrections.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "        style (str): \n",
    "            'dot'   -> Enforces a dot: 'Mr.', 'Mrs.', 'Dr.', 'Esq.'\n",
    "            'no_dot' -> Enforces no dot: 'Mr', 'Mrs', 'Dr', 'Esq' \n",
    "                         (Closer to modern British English style)\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with standardized honorifics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # --- Define Replacements ---\n",
    "    \n",
    "    # These titles are always capitalized but never take a dot.\n",
    "    # We run these first.\n",
    "    no_dot_titles = {\n",
    "        r'\\bsir\\b': 'Sir',\n",
    "        r'\\bdame\\b': 'Dame',\n",
    "        r'\\blord\\b': 'Lord',\n",
    "        r'\\blady\\b': 'Lady',\n",
    "        r'\\bmiss\\b': 'Miss', # Miss is a full word, not an abbreviation\n",
    "    }\n",
    "\n",
    "    # These are abbreviations, and their punctuation depends on the style.\n",
    "    # The regex \\.? matches if a dot is present or not, standardizing both.\n",
    "    if style == 'dot':\n",
    "        style_dependent_titles = {\n",
    "            r'\\bmr\\.?\\b': 'Mr.',\n",
    "            r'\\bmrs\\.?\\b': 'Mrs.',\n",
    "            r'\\bms\\.?\\b': 'Ms.',\n",
    "            r'\\bdr\\.?\\b': 'Dr.',\n",
    "            r'\\brev\\.?\\b': 'Rev.',\n",
    "            r'\\bprof\\.?\\b': 'Prof.',\n",
    "            r'\\bcapt\\.?\\b': 'Capt.',\n",
    "            r'\\bcol\\.?\\b': 'Col.',\n",
    "            r'\\bgen\\.?\\b': 'Gen.',\n",
    "            r'\\besq\\.?\\b': 'Esq.',\n",
    "            r'\\bwm\\.?\\b': 'Wm.', # For William\n",
    "        }\n",
    "    else: # style == 'no_dot' (Modern British)\n",
    "        style_dependent_titles = {\n",
    "            r'\\bmr\\.?\\b': 'Mr',\n",
    "            r'\\bmrs\\.?\\b': 'Mrs',\n",
    "            r'\\bms\\.?\\b': 'Ms',\n",
    "            r'\\bdr\\.?\\b': 'Dr',\n",
    "            r'\\brev\\.?\\b': 'Rev',\n",
    "            r'\\bprof\\.?\\b': 'Prof',\n",
    "            r'\\bcapt\\.?\\b': 'Capt',\n",
    "            r'\\bcol\\.?\\b': 'Col',\n",
    "            r'\\bgen\\.?\\b': 'Gen',\n",
    "            r'\\besq\\.?\\b': 'Esq', # For consistency, we'll remove the dot here too\n",
    "            r'\\bwm\\.?\\b': 'Wm', \n",
    "        }\n",
    "\n",
    "    # Apply the replacements, starting with the non-dotted ones.\n",
    "    # We use re.IGNORECASE to catch all variants (e.g., 'mr', 'Mr', 'MR').\n",
    "    \n",
    "    for pattern, replacement in no_dot_titles.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "        \n",
    "    for pattern, replacement in style_dependent_titles.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def final_polishing_batched(batch, text_column=\"Corrected_Cleaned_text\", style='dot'):\n",
    "    \"\"\"\n",
    "    Wrapper function to apply honorific standardization to a\n",
    "    dataset batch.\n",
    "    \"\"\"\n",
    "    input_texts = batch[text_column]\n",
    "    \n",
    "    polished_texts = [\n",
    "        standardize_honorifics(text, style=style) for text in input_texts\n",
    "    ]\n",
    "    \n",
    "    # Return in a new column to preserve the previous step\n",
    "    return {f\"Polished_{text_column}\": polished_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9bf70ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reapply_punctuation_spacing(text):\n",
    "    \"\"\"\n",
    "    Cleans up all spacing around punctuation. This is a final\n",
    "    polishing step to be run AFTER all other corrections.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # 1. Removes space BEFORE punctuation\n",
    "    _space_before_punct_re = re.compile(r'\\s+([,\\.;:?!])')\n",
    "\n",
    "    # 2. Ensures one space AFTER punctuation if it's followed by a letter/number\n",
    "    #    (This fixes \"word.word\" -> \"word. word\")\n",
    "    _space_after_punct_re = re.compile(r'([,\\.;:?!])([a-zA-Z0-9])')\n",
    "\n",
    "    # 3. Cleans up spaces around brackets\n",
    "    _space_around_brackets_open_re = re.compile(r'\\s*\\(\\s*')\n",
    "    _space_around_brackets_close_re = re.compile(r'\\s*\\)\\s*')\n",
    "\n",
    "    # 4. Collapses any remaining multiple spaces\n",
    "    _multi_space_re = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "    # \"word . word\" -> \"word. word\"\n",
    "    text = _space_before_punct_re.sub(r'\\1', text)\n",
    "    \n",
    "    # \"word.word\" -> \"word. word\"\n",
    "    text = _space_after_punct_re.sub(r'\\1 \\2', text)\n",
    "    \n",
    "    # \"word ( word ) word\" -> \"word (word) word\"\n",
    "    text = _space_around_brackets_open_re.sub(' (', text)\n",
    "    text = _space_around_brackets_close_re.sub(') ', text)\n",
    "\n",
    "    # \"word.  word\" -> \"word. word\"\n",
    "    text = _multi_space_re.sub(' ', text)\n",
    "    \n",
    "    # Final strip\n",
    "    return text.strip()\n",
    "\n",
    "def final_spacing_batched(batch, text_column=\"Corrected_Cleaned_text\"):\n",
    "    \"\"\"\n",
    "    Batch-processing wrapper for the final spacing cleanup.\n",
    "    Run this on the output of your *previous* final step.\n",
    "    \"\"\"\n",
    "    # Get the text from the *last* processing column\n",
    "    input_texts = batch[text_column]\n",
    "    \n",
    "    # Use a list comprehension for speed\n",
    "    polished_texts = [reapply_punctuation_spacing(text) for text in input_texts]\n",
    "    \n",
    "    # Return in a new, final column\n",
    "    return {f\"Final_Polished_Text\": polished_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_record(record):\n",
    "    \"\"\"\n",
    "    Displays a Hugging Face dataset record with metadata followed by two\n",
    "    text fields (Full_text, Final_Polished_Text) in side-by-side\n",
    "    columns in the terminal.\n",
    "    \n",
    "    This function works with both dictionaries and pandas Series\n",
    "    (e.g., a row from a DataFrame like df.loc[0]).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Display Metadata ---\n",
    "    print(\"-\" * 80)\n",
    "    print(\"METADATA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Define metadata keys to extract and display\n",
    "    metadata_keys = [\n",
    "        'Author', 'Title', 'Publication', 'Date', 'Place', 'URL'\n",
    "    ]\n",
    "    \n",
    "    for key in metadata_keys:\n",
    "        # Use .get() to safely access keys, providing 'N/A' if key is missing\n",
    "        value = record.get(key, 'N/A')\n",
    "        print(f\"{key+':':<15} {value}\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # --- 2. Prepare Columnar Text ---\n",
    "    \n",
    "    # Get text fields\n",
    "    full_text = record.get('Full_text', '')\n",
    "    capitalized_text = record.get('Final_Polished_Text', '')\n",
    "\n",
    "    # Get terminal width to make columns responsive\n",
    "    try:\n",
    "        terminal_width = shutil.get_terminal_size().columns\n",
    "    except OSError:\n",
    "        # Fallback if not in a real terminal (e.g., CI/CD)\n",
    "        terminal_width = 120\n",
    "\n",
    "    # Define spacing: 3 columns + 2 separators (\" | \")\n",
    "    # We give a little extra buffer for the separators\n",
    "    padding = 4 \n",
    "    col_width = (terminal_width - padding) // 2\n",
    "    \n",
    "    if col_width < 10:\n",
    "        print(\"Terminal is too narrow to display columns effectively.\")\n",
    "        print(\"\\nFull_text:\\n\", full_text)\n",
    "        print(\"\\Final_Polished_Text:\\n\", capitalized_text)\n",
    "        return\n",
    "\n",
    "    # Wrap text for each column\n",
    "    wrapped_full = textwrap.wrap(full_text, width=col_width)\n",
    "    wrapped_capitalized = textwrap.wrap(capitalized_text, width=col_width)\n",
    "\n",
    "    # Find the maximum number of lines needed\n",
    "    max_lines = max(len(wrapped_full), len(wrapped_capitalized))\n",
    "\n",
    "    # --- 3. Print Headers and Rows ---\n",
    "    \n",
    "    # Create header strings, left-aligned and truncated if necessary\n",
    "    header_full = \"Full_text\".ljust(col_width)\n",
    "    header_capitalized = \"Final_Polished_Text\".ljust(col_width)\n",
    "\n",
    "    print(f\"{header_full} | {header_capitalized}\")\n",
    "    print(f\"{'-' * col_width} | {'-' * col_width}\")\n",
    "\n",
    "    # Print each row\n",
    "    for i in range(max_lines):\n",
    "        # Get the line for each column, or an empty string if text is shorter\n",
    "        line_full = wrapped_full[i] if i < len(wrapped_full) else \"\"\n",
    "        line_capitalized = wrapped_capitalized[i] if i < len(wrapped_capitalized) else \"\"\n",
    "        \n",
    "        # Print the formatted row, ensuring each part adheres to the column width\n",
    "        print(f\"{line_full.ljust(col_width)} | {line_capitalized.ljust(col_width)}\")\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_driver():\n",
    "    \"\"\"\n",
    "    Main driver function to run the full pre-processing pipeline.\n",
    "\n",
    "    - Finds all .csv files in a specified input directory.\n",
    "    - Sequentially applies the four main processing stages:\n",
    "      1. Initial text cleaning (`clean_text_piece_batched`).\n",
    "      2. NER-driven correction and capitalization (`capitalization_and_correction_batched`).\n",
    "      3. Honorific standardization (`final_polishing_batched`).\n",
    "      4. Final punctuation spacing cleanup (`final_spacing_batched`).\n",
    "    - Saves the fully processed data to new .csv files in an output directory.\n",
    "    - Returns the DataFrame from the last processed file for inspection.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # Define the folder where your raw .csv files are located\n",
    "    input_folder = Path('./Data_test')\n",
    "    # Define the folder where processed files will be saved\n",
    "    output_folder = Path('./Data_processed/full_pipeline/')\n",
    "    \n",
    "    # --- Setup ---\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    log.info(f\"Input folder: '{input_folder}'\")\n",
    "    log.info(f\"Output folder: '{output_folder}'\")\n",
    "\n",
    "    # Setup SymSpellPy with the master word set\n",
    "    symspell_checker = setup_spellchecker(COMPLETE_WORDLIST)\n",
    "\n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = list(input_folder.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        log.warning(f\"No CSV files found in '{input_folder}'. Exiting.\")\n",
    "        return\n",
    "\n",
    "    log.info(f\"Found {len(csv_files)} CSV file(s) to process: {[f.name for f in csv_files]}\")\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    for file_path in csv_files:\n",
    "        log.info(f\"\\n{'='*20} Processing file: {file_path.name} {'='*20}\")\n",
    "        \n",
    "        # 1. Load the data using the helper function\n",
    "        loaded_data = load_csv_file(str(file_path))\n",
    "        if not loaded_data:\n",
    "            log.error(f\"Failed to load {file_path.name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        raw_dataset = loaded_data['data']\n",
    "        log.info(f\"Loaded {loaded_data['length']} rows.\")\n",
    "\n",
    "        # --- 2. RUN THE FULL PIPELINE ---\n",
    "        log.info(\"Starting text cleaning pipeline...\")\n",
    "\n",
    "        log.info(\"Step 1/4: Initial cleaning...\")\n",
    "        cleaned_dataset = raw_dataset['train'].map(\n",
    "            clean_text_piece_batched, \n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "        log.info(\"Step 2/4: NER, capitalization, and correction...\")\n",
    "        corrected_dataset = cleaned_dataset.map(\n",
    "            capitalization_and_correction_batched,\n",
    "            batched=True,\n",
    "            batch_size=8,\n",
    "            fn_kwargs={\n",
    "                'text_column': 'Cleaned_text',\n",
    "                'master_word_set': COMPLETE_WORDLIST,\n",
    "                'symspell': symspell_checker\n",
    "            }\n",
    "        )\n",
    "\n",
    "        log.info(\"Step 3/4: Polishing honorifics...\")\n",
    "        honorifics_dataset = corrected_dataset.map(\n",
    "            final_polishing_batched,\n",
    "            batched=True,\n",
    "            fn_kwargs={\n",
    "                'text_column': 'Corrected_Cleaned_text',\n",
    "                'style': 'dot' \n",
    "            }\n",
    "        )\n",
    "\n",
    "        log.info(\"Step 4/4: Final punctuation spacing...\")\n",
    "        final_dataset = honorifics_dataset.map(\n",
    "            final_spacing_batched,\n",
    "            batched=True,\n",
    "            fn_kwargs={'text_column': 'Polished_Corrected_Cleaned_text'}\n",
    "        )\n",
    "\n",
    "        log.info(\"Pipeline complete. Final text is in 'Final_Polished_Text'.\")\n",
    "\n",
    "        # 4. Save the results to a new CSV file\n",
    "        log.info(\"Step 3/3: Saving processed data...\")\n",
    "        output_filename = f\"{file_path.stem}_processed.csv\"\n",
    "        output_path = output_folder / output_filename\n",
    "        \n",
    "        # Convert the final dataset to a pandas DataFrame to save as CSV\n",
    "        final_df = final_dataset.to_pandas()\n",
    "        \n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        log.info(f\"Successfully saved processed data to '{output_path}'\")\n",
    "\n",
    "    log.info(f\"\\n{'='*20} Pipeline finished. {'='*20}\")\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "365760f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input folder: 'Data_test'\n",
      "INFO:__main__:Output folder: 'Data_processed/full_pipeline'\n",
      "INFO:__main__:Setting up SymSpellPy...\n",
      "INFO:__main__:Loaded standard frequency dictionary. Word count: 82834\n",
      "INFO:__main__:Added 415814 new unique words from master set to SymSpell.\n",
      "INFO:__main__:SymSpell setup complete. Total unique terms: 498648\n",
      "INFO:__main__:Found 1 CSV file(s) to process: ['200_full_text_samples.csv']\n",
      "INFO:__main__:\n",
      "==================== Processing file: 200_full_text_samples.csv ====================\n",
      "INFO:__main__:Loaded 196 rows.\n",
      "INFO:__main__:Starting text cleaning pipeline...\n",
      "INFO:__main__:Step 1/4: Initial cleaning...\n",
      "INFO:__main__:Step 2/4: NER, capitalization, and correction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4068e0be4414464dbef7b41fd5421f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error during word_segmentation on 'leieesteisquare': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'PeterfromPrisontm': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Tionswhicharcsocrbditable': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Westminsterbridge': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Tionswhicharcsocrbditable': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tleeparliamentary': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'lbcelteceniththtisetandt': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ethearchntiologset': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'conscieottously': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theintelligence': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'presenrtpromsinence': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'wiiavsnwiwkcins': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'commiusasioners': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Wasdistribuxted': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tobesodiatuibuted': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'lbcelteceniththtisetandt': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ethearchntiologset': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'wiiavsnwiwkcins': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'butdidsofromtinie': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'conforanotionern': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Thepublicaremostly': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'butdidsofromtinie': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'audiencecommenced': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tdeesenehigffroui': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tdeesenehigffroui': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thelcaderefthepresentfrenchschool': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'contemplationof': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'iswatteauishandagree': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'delightedApelles': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thelcaderefthepresentfrenchschool': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'iswatteauishandagree': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tbatahaxprinciple': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'becompartivelylarge': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Herrinwillvemlnd': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'becompartivelylarge': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Herrinwillvemlnd': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'sixteenthcentury': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'itssimplerforms': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'fromAddisontoNewmanthe': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'exceptCeiriogandone': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'blindfoldGames': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofLondonexchanges': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'MademoselleDenisplayed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Birthelcinohleads': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'GardenfromHart': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'lordChetTerFtelO': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'BesidesMarchesi': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'BarthelEmonled': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'withHollandand': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'marshalLaudhonin': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'usualTreasurywarrant': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'abhriacquititiori': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'abhriacquititiori': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'williiughbysydllito': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'wllliamgordoriy': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'signorBianchi': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thePantheonfor': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'signorPacchierotti': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'williiughbysydllito': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ticOechbstracould': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'GaxrickofItalythough': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'anOrcheFtrafinger': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andKnyvettand': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Irtatforiminarx': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Harrisonselegant': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'dangerousdifeasesincideotto': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'estcltuatiycures': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'dangerousdifeasesincideotto': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'estcltuatiycures': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'inTottenbamcourt': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofDevondesire': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'TheMatchgonefs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofSalisburyreturned': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'masterKellner': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'regularItyprevailed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thePrinceofWales': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'followingNoability': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'missesShakewill': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ronxiderablerielayoccurred': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'sibimcdiaxecccdae': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'siajonsrmrotoxoc': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'givepowcrfitimadame': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ronxiderablerielayoccurred': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'sibimcdiaxecccdae': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'siajonsrmrotoxoc': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'givepowcrfitimadame': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'gratiousslajesty': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'inCheapsideand': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Aloschelesmust': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'firstefwhichconsiste': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'firstefwhichconsiste': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'advertisemiients': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'aLLamecellinii': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andsangwithgreatfacilicy': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andsangwithgreatfacilicy': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thatHandelwrote': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Theprimidpalvocal': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Anothernastrumentalexhibicei': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Theprimidpalvocal': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'mademoisellee': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Anothernastrumentalexhibicei': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Piofianchetti': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'superfluoustosay': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofLindleadded': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'dameMalibransang': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'executionrequired': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Ablefissemblage': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'withBeethovens': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'etentepoclisare': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tlieyiareindeed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'noblescbequescwecanleave': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'soureceofinformation': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'analyzingthemilitaryutiovements': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'notonlyrfacilitated': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'butstampedhisdecisionswith': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'pussuethestibject': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'reachipegilails': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tlieyiareindeed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'noblescbequescwecanleave': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'analyzingthemilitaryutiovements': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'butstampedhisdecisionswith': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'pussuethestibject': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'signorSpagnoletti': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'unianimousvoice': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'partofherperformunce': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'singularlydeficientin': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'adistinctarticulation': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'missWilliamsmade': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theearliestepportunity': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Grourdulandlord': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tlieadlusunentol': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'prorrtionoftheir': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'mevourofthenewrenters': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'itsconstruction': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'expenvivearticles': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'leaseheldproperty': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ythevalujoftheadjining': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'extensivepremiscs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'licencesattached': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Sucharetheptans': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'unexeetedlycalld': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'publicgratification': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'becninstrumcntal': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'particularlyfirst': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'pinionofthismeeting': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theirsubscriptions': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theearliestepportunity': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tlieadlusunentol': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'mevourofthenewrenters': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ythevalujoftheadjining': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'mostsatisfactory': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'madamElazande': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Stockihausten': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'nsasperformed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'inampititheatre': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'scarcelypossihle': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thingmoreperfectas': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Everyinfiection': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'everyomramentis': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'mostindeseribable': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'almostexclusively': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Knyvetthaving': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'zenicTellwere': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'werecinelysubdued': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'instrumentalbasses': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'strikingdevelopmentofhlandel': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'strikingdevelopmentofhlandel': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'subjectispresented': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'performancelast': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theKiingsTheatre': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'altogetherpreaeated': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thingmoreperfect': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Itnisflexibility': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'accompanimentin': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thepiuixcatooftheguitar': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'tboughrepeatedly': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Psaganiniplays': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thepiuixcatooftheguitar': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'concertattracted': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'englishlournals': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'iaeredmechauical': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'lehetfjhrtioiof': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'lehetfjhrtioiof': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'afterthescenefrom': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'feelingsoftheaudience': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theintendedefect': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Siveperformnancaof': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Illadeinoisdfle': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theadistinguished': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Illadeinoisdfle': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'splendidlyperformed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Alesdamsucintiaiasnoreau': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'BarryComnwallhas': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andpowerfultonesof': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'PastaandMalibran': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'timeestablished': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'goneoffadmirably': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theLondonpublic': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'himtelflreriamred': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'himtelflreriamred': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'missOstergard': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'erenchipatnqfisis': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theKingsService': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Gagetdiscusses': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thirdRepubliccan': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'LatheRobinsons': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'containsaseries': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'astronomerToscaneuito': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofToscaneuiwas': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'betweenToscaneui': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'towardsAmerica': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'reachedIndiaby': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'toldColumbusof': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Columbuslistened': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'entitledChatham': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Fortescueplaced': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theHistorical': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'youngerPittin': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'aneclarcissemcnt': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'relationFrfolicKupon': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'daughterDiana': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'introducingBeaumontfo': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'introducingSir': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'AnthonyandBeaumontto': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'toDabbletheDenlist': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'signorLucilio': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theLondonstage': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'curptismentstfiviirfidld': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'generalHowitzer': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'curptismentstfiviirfidld': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'beenHendersons': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'exceedJidwinin': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'sirHugAjevanS': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'WroughtonperformedMr': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Jordancontinues': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Notirithflandings': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Siddonsplayed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'considerabjjssharc': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'mannerJohnstones': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'introducingArcherto': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'duringAiweirs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'MistakenSullen': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'performedStockwell': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Harlandexhibited': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'indianwatperformed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'firsfrepresentation': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'applauseWilsons': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Mortonperformance': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'succeededMils': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'stateynotwithstanding': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'missAngiebeing': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Pleisantprelude': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andShakespearereduced': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofLoutherbdurghin': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'paipableplagiarisms': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'extremeyrequent': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thisDraniawere': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Preuxdetermines': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'threatensElfifa': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'NeighbouringAbbey': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andZaradisplaced': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andElvirAevinced': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'sovereignGeramisold': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'findsNorbaldead': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'leavingHirfol': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'paperGeramisis': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'adoptsNopthisfor': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'declaresNopthisto': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'whichHirfalstabs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'COawiderAmefliting': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'roughTonshould': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofForesiGhtwas': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theContingent': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theCommitteeand': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andGalliniwho': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'princessRoyaland': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'princessAugusta': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'widowRacketwith': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'georgeTouchwood': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theTerraceatWindsoron': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'PawsonsinOldcastle': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'exceptionJewells': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'aLondonaudience': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'youngerBannister': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'brigbthelmjtone': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'wifofecharacter': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Tvclfcttablistied': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'andfoisyiuffier': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ksttallivcfeyjthen': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'heroicDouglaso': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Randolphwould': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Tvclfcttablistied': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ksttallivcfeyjthen': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'characterMonimia': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Mansfieldhangs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'experiencedevery': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Palmersubmits': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theRoyaltytheatRewill': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'aboutLondonet': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Palmerproposes': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Theatresuperiorly': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'prbducinghovelty': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Bulkeleylikewise': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theirpcrfofmandei': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'afcwmurernftedccs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theirpcrfofmandei': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'afcwmurernftedccs': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'whichParisabounds': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'understandirigyrcfigncd': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Rannisierwill': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'princessElizabethis': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'princeOfWalesor': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofBensleysIago': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ThegentleDesdemonahas': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'hearingOthellos': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thoughtSignora': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'reipreseuntstve': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'thisestonishing': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'alesdemoiselles': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'journeyElizabethwas': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'theOperastage': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'anycircumnstances': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofgivinginehargeone': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofgivinginehargeone': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'madameAlbertand': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofMephiStopheleswith': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ofCanovaorFlaxman': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'ChoristErssang': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'atSalzburgshowed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Philharmonicplayed': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'Philharmonicstrings': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'harryingNapoleon': distance too large\n",
      "ERROR:__main__:Error during word_segmentation on 'likeGlyndebourneshould': distance too large\n",
      "INFO:__main__:Step 3/4: Polishing honorifics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a961b09ad8054d358e2d99c9986735f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 4/4: Final punctuation spacing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6f6674b4694d22a391d273df68fb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pipeline complete. Final text is in 'Final_Polished_Text'.\n",
      "INFO:__main__:Step 3/3: Saving processed data...\n",
      "INFO:__main__:Successfully saved processed data to 'Data_processed/full_pipeline/200_full_text_samples_processed.csv'\n",
      "INFO:__main__:\n",
      "==================== Pipeline finished. ====================\n"
     ]
    }
   ],
   "source": [
    "# --- Execute the main function ---\n",
    "# This block ensures the code runs when the script is executed\n",
    "df = pipeline_driver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "36883a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "Author:         Null\n",
      "Title:          BRITISH INSTITUTION.-The annual exhibition of paintings by old masters is now open at the British\n",
      "Publication:    The Times\n",
      "Date:           1838-Jun-18\n",
      "Place:          London, England\n",
      "URL:            https://link.gale.com/apps/doc/CS100821714/TTDA?u=uppsala&sid=bookmark-TTDA&xid=df660182\n",
      "\n",
      "================================================================================\n",
      "TEXT COMPARISON\n",
      "================================================================================\n",
      "Full_text                              | Final_Polished_Text                   \n",
      "-------------------------------------- | --------------------------------------\n",
      "BRITISH INSTITUTIO'.-The annual        | British institution.-the annual       \n",
      "exchibition of paiHting;s by old       | exhibition of painting; s by old      \n",
      "masters is now open at the British     | masters is now open at                \n",
      "Institution, Pall-mall, and the        | theBritishInstitution, Pall-Mall, and \n",
      "nunmber of wvorlzs here brought to-    | the number of worlds here brought toe.\n",
      "et.her tro.n different private         | Her tro. N different private          \n",
      "collections amounts to 148. lie worth  | collections amounts to148. Lie worth  \n",
      "of the pictures varies conisiderably,  | of the pictures varies considerably,  \n",
      "as they are the productione of artists | as they are the production of artists \n",
      "of diffcrent degrees of merit; but     | of different degrees of merit; but    \n",
      "there are smong them several paintings | there are among them several paintings\n",
      "of the highest order of talent. To     | of the highest order of talent. To    \n",
      "this rank belonu the \" St. John with   | this rank belong the\"st. John wwith   \n",
      "the Lamb,\" an \"'*he Good Shepherd,' by | the lamb,\"an\"he good Shepherd, by     \n",
      "Murillo; the portrait of a \"Knighit of | Murillo; the portrait of a\"Knighit Of \n",
      "Calhtrava\" by the same master ; the \"  | Calatrava \"by the same master;        \n",
      "Duke *d'Olivarez\"l andI other SpAnish  | the\"Duke D ' OlivaRez \"l andi other   \n",
      "portraits, by VelasqAeea; th& \"Coo     | spanish portraits, by VelasqaEea;     \n",
      "nsellor Triost,\" by Vandyke ; the      | th\"COo nse lor Triost, \"by Vandyke;   \n",
      "portrait of \"Cornelius Vana Htooft,\"   | the portrait of\"Cornelius Vana Htooft,\n",
      "the translator of Homer into Dutch, by | \"the translator of Homer into dutch,  \n",
      "Rembrandt ; and two orsthree views by  | by RembRandt; and two or three views  \n",
      "Canaletto. Thtre is also a portrait of | by Canaletto. There is also a portrait\n",
      "Martin Luther,-strongly characteristic | of Martin Luther, -strongly           \n",
      "of his indomitable spirit, by          | characteristic of his indomitable     \n",
      "Pordenene. The novelty of the present  | spirit, by Hordenine. The novelty of  \n",
      "exhibition consists in the             | the present exhibition consists in the\n",
      "introduction of two paintings by a     | introduction of two paintings by a    \n",
      "modern French artist Paul de la Roche, | modern french artist Paul De La Roche,\n",
      "who has gained much celebrity abroad   | who has gained much celebrity abroad  \n",
      "by his illustrations of points in      | by his illustrations of points in     \n",
      "English history. One of the paintings  | english history. One of the paintings \n",
      "now exhibited has been purchasedi b    | now exhibited has been purchased b the\n",
      "the Duke of Sutherland, it represents  | Duee Of Sutherland, it represents Lord\n",
      "Lord Strafford going to execution. The | Strafford going to execution. The     \n",
      "other is the property of Lord F.       | other is the property of Lord F.      \n",
      "Egerton, and has for its subject       | Egerton, and has for its subject      \n",
      "Charles I. in captivity exposed to the | Charles I. In captivity exposed to the\n",
      "insult% of the soldiery. The designs   | insult of the soldiery. The designs   \n",
      "are coboposed of several figures of    | are composed of several figures of the\n",
      "the size pf life, and the necessarily  | size pf life, and the necessarily     \n",
      "large dirnensions of the canvass have  | large dimensions of the canvass have  \n",
      "induced the 'directors of tie          | induced the directors of tie          \n",
      "institution to place the paintings in  | institution to place the paintings in \n",
      "thie ,re 5ent exhibition, rather than  | the, re5ent exhibition, rather than in\n",
      "in the exhibition of the works of      | the exhibition of the works of modern \n",
      "tmoderi artists, wnhere they raust     | artists, where they rust have occupied\n",
      "bave occupied the room -of teany       | the room-of beany prefires intended   \n",
      "piefuires intended for ale. It is not  | for ale. It is not too much to say, th\n",
      "too much to say, 'th&t BE ito la       | t be ito la Roche ' s paintings are   \n",
      "Roche's paintings are entitled by      | entitled by their own intrinsic merits\n",
      "their own intrinsic merits to the      | to the honourable distinction thus    \n",
      "honourable distinction thus conferred  | conferred on there is now exhibited at\n",
      "on There is now exhibited at No. .,    | no.., haymarket, a set. Of tapestries \n",
      "Haymarket, a set .of tapestries        | wrought in Brassels ir1517, flota the \n",
      "wrought in Brussels ir 1517,frota the  | cartoons of raphael. These copies. Are\n",
      "cartoons of Raphael. These copies. are | of the same size, and include the     \n",
      "of theosame size, and include tbe      | whole of the subjects-comprised in the\n",
      "whole of the subjects- comprised in    | great original work, bat in           \n",
      "the great original work, bat in        | consequence of the wait of room only  \n",
      "consequence of the wait of room only   | s. X of the ii arc hung up for        \n",
      "s.x of theuii arc hung up for          | inspection; and among these are two   \n",
      "inspection; and ameng tbese are two    | the original cartoons of which have   \n",
      "the original cartoons of which have    | unfortunately perished. Tie tapestry  \n",
      "unfortunately perished. Tie            | york appears to be a good imitation of\n",
      "tapesstriyork appears to be a good     | the general outline aid design of     \n",
      "imitation of the general outline aiid  | their toona, but somewhat-defective in\n",
      "design of theiortoona,but somewhat-    | the representation. Of the            \n",
      "defective in the represontatisn. of    | e2pression., of the countenances. It  \n",
      "the e2pression.,of the countenances.   | is, however, singular to find so much \n",
      "It is, iowever, singular to find so    | portrayed by means. V such seen o     \n",
      "much portrayed by means .v luch seent  | imper-felt.-;; ea) ned ul acid xi-ro  \n",
      "o imper-feclt. * - ;;EA)nEDuL AccDZxi  | as eton sce last.-on arthur dn.       \n",
      "-ro %As EToN SceoLAst.-On              | Y-refining a dreadful accident happen \n",
      "rThurgdn.y-reFning a dreadiul accident | sd to the ron. Idles, ono of the      \n",
      "happeiisd to the Ron. Ndles, ono of    | students at eten college, and son of  \n",
      "the students at Eten College, and son  | Lord sondes. This young gentleman,-who\n",
      "of Lord Sondes. This young gentlesman, | is, we understand, beauts15or16years  \n",
      "-who is, we understand, abeauts15or 16 | of age, went, with a number of his    \n",
      "years of age, went, with a nimiber of  | fellow-collegians, to the theatre, and\n",
      "his feldow-collegians, to the theatre, | as they are accustomed, t6! Do-much to\n",
      "and as they are accustomed             | the annoyance of the rest of the      \n",
      ",t6!do-~much to the annoyance of the   | audience commenced playing their dual \n",
      "rest of the audiencecommenced playing  | pranks, one of the most dan. Gerous of\n",
      "their uuatl pranks, one of the most    | which is, to get into the gallery and \n",
      "dan.geroui of which is, to get into    | let themselves down to the boxes anti \n",
      "the gallery and let themselves down to | pit. It is sar rising slut no fatal   \n",
      "the boxes antt pit. It is saryrising   | are dent has ere now occurred to any  \n",
      "tlut no fatal aeeldent has ere now     | et them in trying that-dani rog:      \n",
      "occurred to any et them in trying that | experiment, but it is hoped that the  \n",
      "-danirog: experhnent, but it is hoped  | injury done to-haw sondes soon will   \n",
      "that the injury done rto-              | operate as awning in future: in       \n",
      "hAwdSondes'sson will operate as        | tdeesenehigffroui the gallery a hello.\n",
      "zawning in luture: in                  | K-student lies. His hands, paid       \n",
      "tdeesenehigffroui the gallery a        | winther stood in the boxes underneath,\n",
      "fello'.k-student liel. his hands,      | to guide his feet to-the pili. Rj     \n",
      "*paid iindther stodd in the boxes      | when, unfortunately, the buy in the   \n",
      "underneath, to guide his feet to- the  | gallery it o hi, hands too early,-anl \n",
      "pili.mrj when, unfortunately, the buy  | Mr. 3titles fell luvish great violence\n",
      "in the gallery It '>o hi, hands too    | ntn o the uit. He was in mediately-   \n",
      "early,-anl Mr. 3titlls fell uvirh      | taken ssp and carried to t1egre4lu-   \n",
      "great violence ntn+o tlhe uit. He was  | room, where everything that could be  \n",
      "hnmmdiately -taken Ssp andl ca rried   | thought ef wit's huic by-mtr.-psalms, \n",
      "to t1iegre4lu-roor, wvhere everythaing | the nia niger, who ulvas evil. Len tlv\n",
      "that conld be thoughst ef witvs ahuic  | mae distn, sse. At the car reduce. Sir\n",
      "by -Mtr. -Psaldv, the nianiger, wvho   | john cila plan and3ir. S lee sur s eo \n",
      "uvas evi.lentlv maeb distn,sse.at the  | w, ere send for, and seelily arrival, \n",
      "tcacrreuce. Sir Johnl Cilapmlan and '3 | when: kvas found that the unfortunate \n",
      "ir. S'lle} sur s eo w , ere senu for,  | boy had received a severe f rather iii\n",
      "and spelily arrivel, whei :ikvas found | the hall, ant other injuries of a     \n",
      "tlat the unforttnate boy had receisod  | serious climacter. Th l be Sir: r. Tr \n",
      "a seere f'rathtre iii the hlal, ant    | wits i mediately take; en hv mec in a \n",
      "other injurics of a scrious cliracter. | carriage, and a, tax press sclat to   \n",
      "Th'l'be suir:'r.tr wits i'umediately   | london to Lord fondea, who arrived    \n",
      "takl;en hvomec in a carriage, and a,   | with the. I may nic lichi.; tenant at \n",
      "ttxipress SCllt to London to Lord      | about16clock in thc morn; ns:-w0e uum \n",
      "Sondea,'who arrived dith the '.i maly  | star, d that Mr. Miles, is nov out bf,\n",
      "nic lichi .;ttenjant at about 1        | danger, bitt he sti; lies int a very  \n",
      "6'clock in thc morn;nS: -W0e           | band state.-) wid,.; pres,:. I.?. Apj \n",
      "uumlestar,d that Mr. Miles,is nov out  | it. E, a physician of montpelier,     \n",
      "bf,daiger, bitt he sti; lies int a     | passed a s6. V da agm through loz     \n",
      "very band state.-) Wid,. ;pres,:. i.   | eaux, on his w ae to-paris, who-alber \n",
      "?.aPJ it.e, a physician of             | be ts guin, texas bt footie faculty of\n",
      "MIsontpelier, passed a ' S6.V da       | medicine unde ni-sb? does of theo     \n",
      "agmthsrougn ilozdeaux, on his w ae to- | somnambulic lucidity laith which his  \n",
      "Paris, whi-lber be ts guin,telas       | dau hire ill. Ye rs. Hld, is-gifted.  \n",
      "btoforotie Faculty of *Madicine unde=  | Im. Page aire produced to. Us trio    \n",
      "ni-sb ?soes of theo somnambulic        | original certificate of31. Lordan, 1bv\n",
      "luciditv Lvith which his daughizre     | ician at montpelier l4user ted in the \n",
      "Ill.ye&rs.hld, is -gifted. IM.         | printed account of the sitting of the:\n",
      "Pigeaire produced to. us tlio original | ito il academy of-medicine of the esth\n",
      "certificate of 31. Lordat, 1bvsician   | of march, 183., s p.:. X9, fro. 11v   \n",
      "at Montpeliei' l4userted in the        | which. it appears that Miss page aire \n",
      "printed account of the sitting of the  | repeatedly reid. With tt then imi-;   \n",
      ": itovil Academn of - Medicine of the  | of, her-tvs, in presence of-messrs.;  \n",
      "lSth of Marcb, 183.,S p.:.x 9, fro.11  | vasili Dr.-laforge, routhe; g lot     \n",
      "v hich.iit appears tbat Miss Pigeaire  | itm1,. Qui sac. And; lnr eat,-alf     \n",
      "repeatedly * reidi. withtt thenimi     | medica hg2t e anam dl c (anal-du      \n",
      "-;of',her -tves, in presen'ce of -     | barret.-coir er dc-7-idea1ti)         \n",
      "Messrs. ;Visilidr -Lafoje, Roushe;g    | -o-.-r5;. T c                         \n",
      "lortnitm1, .Quinsac*.and ;Lnreat, -    |                                       \n",
      "alF`MediCahg2t e _anamdL C(Aenal-du    |                                       \n",
      "Barret.-tCoir*er dc -&7-rdeA1ti) -o -  |                                       \n",
      ". -r} 5^; ^ . t c                      |                                       \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "display_record(df.loc[6]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andreas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
