{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import difflib\n",
    "import re\n",
    "import torch\n",
    "import textwrap\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Books_cleaned.csv'\n",
    "folder = './Data_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=folder + '/' + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f59bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Helper Functions for text cleaning ---\n",
    "def pre_clean_for_model(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a hard scrub of the text to remove only the most severe OCR noise\n",
    "    before sending it to a language model.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace('\\n', ' ').replace('<NEWPAGE>', ' ')\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s.,!?-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 2. Recoverable Mapping Function ---\n",
    "def apply_cleaning_in_true_batch(examples, indices, model_pipeline, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Accepts 'indices' to track which rows are being processed.\n",
    "    Returns a special placeholder for batches that fail due to OOM errors.\n",
    "    \"\"\"\n",
    "    print(f\"Processing batch starting with index: {indices[0]}\") # Progress indicator\n",
    "    pre_cleaned_texts = [pre_clean_for_model(text) for text in examples['Full_text']]\n",
    "    all_chunks, chunks_per_review = [], []\n",
    "\n",
    "    for text in pre_cleaned_texts:\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            chunks_per_review.append(0)\n",
    "            continue\n",
    "        review_chunks = []\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = start + chunk_size\n",
    "            review_chunks.append(\" \".join(words[start:end]))\n",
    "            start += chunk_size - overlap\n",
    "        all_chunks.extend(review_chunks)\n",
    "        chunks_per_review.append(len(review_chunks))\n",
    "\n",
    "    if not all_chunks:\n",
    "        return {'Cleaned_text': ['' for _ in examples['Full_text']]}\n",
    "\n",
    "    try:\n",
    "        restored_chunks_results = model_pipeline(all_chunks, max_length=512, batch_size=4)\n",
    "        restored_chunks_text = [result['generated_text'] for result in restored_chunks_results]\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(f\"\\n---!!! WARNING: CUDA OutOfMemoryError caught on batch starting at index {indices[0]}. Marking as failed. !!!---\\n\")\n",
    "        return {'Cleaned_text': [\"__FAILED__\" for _ in examples['Full_text']]}\n",
    "    finally:\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    final_cleaned_texts = []\n",
    "    chunk_idx_start = 0\n",
    "    for num_chunks in chunks_per_review:\n",
    "        if num_chunks == 0:\n",
    "            final_cleaned_texts.append(\"\")\n",
    "            continue\n",
    "        \n",
    "        review_restored_chunks = restored_chunks_text[chunk_idx_start : chunk_idx_start + num_chunks]\n",
    "        full_text_words = []\n",
    "        for i, chunk_text in enumerate(review_restored_chunks):\n",
    "            chunk_words = chunk_text.split()\n",
    "            if i == 0:\n",
    "                full_text_words.extend(chunk_words)\n",
    "            else:\n",
    "                full_text_words.extend(chunk_words[overlap:])\n",
    "        final_cleaned_texts.append(\" \".join(full_text_words))\n",
    "        chunk_idx_start += num_chunks\n",
    "\n",
    "    return {'Cleaned_text': final_cleaned_texts}\n",
    "\n",
    "# --- 3. Load Model and Apply the Mapping ---\n",
    "print(\"Loading the text restoration model...\")\n",
    "restorer = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"pszemraj/flan-t5-large-grammar-synthesis\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# --- 4. Run the Main Pipeline ---\n",
    "print(\"\\n--- Applying the recoverable cleaning pipeline ---\")\n",
    "dataset = dataset.map(\n",
    "    apply_cleaning_in_true_batch,\n",
    "    batched=True,\n",
    "    with_indices=True, \n",
    "    batch_size=16,\n",
    "    fn_kwargs={'model_pipeline': restorer, 'chunk_size': 300, 'overlap': 50}\n",
    ")\n",
    "print(\"Initial processing complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Save the Intermediate Results ---\n",
    "print(\"\\n--- Saving intermediate results (with any failures) ---\")\n",
    "dataset['train'].to_csv(\"cleaned_reviews_with_failures.csv\", index=False)\n",
    "print(\"File saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c35690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. THE RECOVERY PROCESS ---\n",
    "\n",
    "print(\"\\n--- Starting Recovery Process ---\")\n",
    "# Load the dataset that contains the '__FAILED__' markers\n",
    "recovery_dataset = load_dataset(\"csv\", data_files=\"cleaned_reviews_with_failures.csv\")\n",
    "\n",
    "# Filter to get only the rows that failed\n",
    "failed_examples = recovery_dataset['train'].filter(\n",
    "    lambda example: example['Cleaned_text'] == \"__FAILED__\"\n",
    ")\n",
    "\n",
    "if len(failed_examples) > 0:\n",
    "    print(f\"Found {len(failed_examples)} failed examples to re-process.\")\n",
    "\n",
    "    # Re-run the pipeline on the small, failed dataset.\n",
    "    # Use a smaller batch_size or chunk_size for more safety.\n",
    "    reprocessed_failures = failed_examples.map(\n",
    "        apply_cleaning_in_true_batch,\n",
    "        batched=True,\n",
    "        with_indices=True,\n",
    "        batch_size=4, # Smaller batch size for safety\n",
    "        fn_kwargs={'model_pipeline': restorer, 'chunk_size': 200, 'overlap': 30}\n",
    "    )\n",
    "\n",
    "    print(\"Re-processing complete. Now merging results.\")\n",
    "    # You would now merge these corrected results back into your main file.\n",
    "    # The easiest way is often with Pandas.\n",
    "    main_df = pd.read_csv(\"cleaned_reviews_with_failures.csv\")\n",
    "    reprocessed_df = reprocessed_failures.to_pandas()\n",
    "\n",
    "    # Create a dictionary from the reprocessed data for easy mapping\n",
    "    # We need a unique identifier; let's assume 'Full_text' is unique enough for this.\n",
    "    update_map = pd.Series(reprocessed_df['Cleaned_text'].values, index=reprocessed_df['Full_text']).to_dict()\n",
    "\n",
    "    # Update the main DataFrame\n",
    "    main_df['Cleaned_text'] = main_df.apply(\n",
    "        lambda row: update_map.get(row['Full_text'], row['Cleaned_text']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    main_df.to_csv(\"cleaned_reviews_final.csv\", index=False)\n",
    "    print(\"Final, fully cleaned file saved as 'cleaned_reviews_final.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"No failed examples found. Your initial run was successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the dataset with cleaned text into a dataframe ---\n",
    "dataset = load_dataset(\"csv\", data_files=\"cleaned_reviews_final.csv\")\n",
    "\n",
    "# --- 2. Define the SequenceMatcher comparison function ---\n",
    "def calculate_similarity(text_a: str, text_b: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a similarity ratio between two strings using SequenceMatcher.\n",
    "    Returns a float between 0.0 (totally different) and 1.0 (identical).\n",
    "    \"\"\"\n",
    "    # Ensure inputs are strings to avoid errors\n",
    "    if not isinstance(text_a, str) or not isinstance(text_b, str):\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate and return the similarity ratio\n",
    "    return difflib.SequenceMatcher(None, text_a, text_b).ratio()\n",
    "\n",
    "# --- 3. Define the Mapping Function for Batch Processing ---\n",
    "def add_similarity_in_batch(examples):\n",
    "    \"\"\"\n",
    "    This function is designed for .map(batched=True).\n",
    "    'examples' is a dictionary where each value is a LIST of items.\n",
    "    \"\"\"\n",
    "    # Use a list comprehension with zip to efficiently process the batch.\n",
    "    # This pairs up each 'Full_text' with its corresponding 'Cleaned_text'.\n",
    "    similarity_scores = [\n",
    "        calculate_similarity(pre_clean_for_model(original), cleaned)\n",
    "        for original, cleaned in zip(examples['Full_text'], examples['Cleaned_text'])\n",
    "    ]\n",
    "    \n",
    "    # Return a dictionary with the new column. The value must be a list.\n",
    "    return {'similarity_score': similarity_scores}\n",
    "\n",
    "\n",
    "# --- 4. Apply the Mapping ---\n",
    "print(\"--- Applying the similarity calculation using batching ---\")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    add_similarity_in_batch,\n",
    "    batched=True,\n",
    "    batch_size=500  # For CPU tasks, a larger batch size is fine\n",
    ")\n",
    "\n",
    "print(\"\\n--- Dataset after adding the 'similarity_score' column ---\")\n",
    "print(dataset['train'].to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11265c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataset['train']['similarity_score'], bins=50, color='skyblue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd627001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = dataset['train'].to_pandas().sort_values(by='similarity_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the most heavily altered reviews\n",
    "suspicious_reviews = df_sorted[df_sorted['similarity_score'] < 0.01]\n",
    "\n",
    "print(f\"Found {len(suspicious_reviews)} reviews with a similarity score below 0.01 to inspect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665dc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Side-by-Side Pretty Print Function ---\n",
    "def pretty_print_side_by_side(review_series, total_width=120):\n",
    "    \"\"\"\n",
    "    Prints a side-by-side comparison of the pre-cleaned original text and\n",
    "    the final T5-restored text for a single review.\n",
    "\n",
    "    Args:\n",
    "        review_series (pd.Series): A single row from your DataFrame.\n",
    "        total_width (int): The total width of the output in characters.\n",
    "    \"\"\"\n",
    "    # Calculate the width for each text column\n",
    "    divider = \"   |   \"\n",
    "    col_width = (total_width - len(divider)) // 2\n",
    "\n",
    "    # Get the two versions of the text\n",
    "    original_pre_cleaned = pre_clean_for_model(review_series['Full_text'])\n",
    "    t5_cleaned = review_series['Cleaned_text']\n",
    "    \n",
    "    # Wrap the text in each column into lists of lines\n",
    "    original_lines = textwrap.wrap(original_pre_cleaned, width=col_width)\n",
    "    cleaned_lines = textwrap.wrap(t5_cleaned, width=col_width)\n",
    "    \n",
    "    # --- Start Printing ---\n",
    "    print(\"=\" * total_width)\n",
    "    # Use .get() to avoid an error if 'similarity_score' doesn't exist\n",
    "    score = review_series.get('similarity_score', 'N/A')\n",
    "    if isinstance(score, float):\n",
    "        score = f\"{score:.4f}\"\n",
    "    print(f\"Similarity Score: {score}\".center(total_width))\n",
    "    print(\"-\" * total_width)\n",
    "    \n",
    "    # Print headers\n",
    "    header_original = \"Pre-Cleaned Original\".center(col_width)\n",
    "    header_cleaned = \"T5 Restored Text\".center(col_width)\n",
    "    print(f\"{header_original}{divider}{header_cleaned}\")\n",
    "    print(\"-\" * total_width)\n",
    "\n",
    "    # Print the lines side-by-side\n",
    "    max_lines = max(len(original_lines), len(cleaned_lines))\n",
    "    for i in range(max_lines):\n",
    "        # Get the line for each side, or an empty string if one side is shorter\n",
    "        left_line = original_lines[i] if i < len(original_lines) else \"\"\n",
    "        right_line = cleaned_lines[i] if i < len(cleaned_lines) else \"\"\n",
    "        \n",
    "        # Print the formatted line with padding\n",
    "        print(f\"{left_line:<{col_width}}{divider}{right_line:<{col_width}}\")\n",
    "        \n",
    "    print(\"=\" * total_width + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3af0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- This is the dataset we want to save ---\")\n",
    "print(dataset)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Define the output file path ---\n",
    "output_file_path = \"cleaned_reviews.csv\"\n",
    "\n",
    "\n",
    "# --- 3. Save the dataset to a CSV file ---\n",
    "\n",
    "# === METHOD 1: Convert to Pandas and Save (Highly Recommended) ===\n",
    "print(\"--- Saving using the recommended Pandas method ---\")\n",
    "try:\n",
    "    # First, select the 'train' split from the DatasetDict\n",
    "    train_split = dataset['train']\n",
    "\n",
    "    # Convert the split to a Pandas DataFrame\n",
    "    df = train_split.to_pandas()\n",
    "\n",
    "    # Save the DataFrame to CSV.\n",
    "    # `index=False` is crucial to avoid an extra, unnamed column.\n",
    "    # `encoding='utf-8'` is a best practice for text data.\n",
    "    df.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Successfully saved the 'train' split to '{output_file_path}'.\")\n",
    "    print(\"You can now open this file to begin your manual labelling.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with the Pandas method: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_side_by_side(suspicious_reviews.iloc[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
